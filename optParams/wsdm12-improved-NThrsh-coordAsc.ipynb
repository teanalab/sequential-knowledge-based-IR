{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function \n",
    "from bs4 import BeautifulSoup\n",
    "from BeautifulSoup import SoupStrainer as sopstrain\n",
    "import subprocess\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import numpy as np\n",
    "import string\n",
    "import operator\n",
    "import csv\n",
    "import urllib2\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import StringIO\n",
    "import random\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cfgInFileName = /home/fj9124/projects/ir/seq_kb_ir/configs/trec7n8/queryTrec7n8\n",
      "cfgOutFileName = /home/fj9124/projects/ir/seq_kb_ir/configs/trec7n8/wsdmImprCoorAsc/cnet/indriRunQuery.cfg\n",
      "colIndexDir = /scratch/index/indri_5_7/trec7n8\n",
      "knowledgGraph index Dir = /scratch/index/indri_5_7/trec7n8\n",
      "col Qrels File Name = /home/fj9124/projects/ir/seq_kb_ir/qrels/trec7n8/qrels.csv\n",
      "runsFileName = /home/fj9124/projects/ir/seq_kb_ir/runs/wsdmImprCoorAsc/trec7n8/cnet/indriRunQuery.runs\n",
      "evalsFileName = /home/fj9124/projects/ir/seq_kb_ir/evals/wsdmImprCoorAsc/trec7n8/cnet/indriRunQuery.evals\n",
      "dumpindexStatementFilename = /tmp/sbsb/statement_trec7n8_wsdmImprCoorAsc_cnet\n",
      "occuranceCountFilename = /home/fj9124/projects/ir/seq_kb_ir/occuranceCount/occuranceCount\n",
      "countsResultsFile = /home/fj9124/projects/ir/seq_kb_ir/occuranceCount/results/trec7n8.txt\n",
      "conceptnet5RelAllFilename = /scratch/saeid/data/conceptnet5_simp.csv\n",
      "cfgTmpOutFileName = /home/fj9124/projects/ir/seq_kb_ir/configs/trec7n8/wsdmImprCoorAsc/cnet/indriRunQuery.cfg.tmp\n",
      "docIdNameMapFileName = /home/fj9124/projects/ir/seq_kb_ir/occuranceCount/results/trec7n8_docIdNameMap.txt\n"
     ]
    }
   ],
   "source": [
    "knowledgGraph = \"cnet\" # conceptnet5AssocMod, gov, conceptnet5AssocMi, conceptnet5AssocHdl\n",
    "#knowledgGraph_ = ''.join([k.capitalize() if i>0 else k for i, k in enumerate(knowledgGraph)])\n",
    "collection = \"trec7n8\"\n",
    "method = \"wsdmImprCoorAsc\" # hal, mi, assoc, assocMi, assocHal, assoc2\n",
    "simMeasure = \"\" # mi, cnet\n",
    "projectDir = \"/home/fj9124/projects/ir/seq_kb_ir/\" \n",
    "indexDir = \"/scratch/index/indri_5_7/\"\n",
    "colMethodConfigsDir = os.path.join(projectDir, \"configs\", collection, method)\n",
    "cfgInFileName = os.path.join(projectDir, \"configs\", collection, \"query\" + collection.capitalize()) \n",
    "print(\"cfgInFileName =\", cfgInFileName)\n",
    "cfgOutFileName=os.path.join(colMethodConfigsDir, knowledgGraph, \"indriRunQuery.cfg\") \n",
    "print(\"cfgOutFileName =\", cfgOutFileName)\n",
    "colIndexDir = os.path.join(indexDir, collection) \n",
    "print(\"colIndexDir =\", colIndexDir)\n",
    "if knowledgGraph in {\"cnet\"}:\n",
    "    knowledgGraphIndexDir = os.path.join(indexDir, collection)   \n",
    "else:\n",
    "    knowledgGraphIndexDir = os.path.join(indexDir, knowledgGraph)   \n",
    "print(\"knowledgGraph index Dir =\", knowledgGraphIndexDir)\n",
    "graphsDir = os.path.join(projectDir, \"graphs\")\n",
    "#methodGraphsDir = os.path.join(graphsDir, method)\n",
    "methodGraphsFileName = []\n",
    "if method == \"hal\":\n",
    "    methodGraphsFileName = [os.path.join(graphsDir, method, knowledgGraph + \".txt\")]\n",
    "    print(\"methodGraphsFileName =\", methodGraphsFileName)\n",
    "elif method.translate(None, string.digits) in {\"mi\", \"assoc\"}:\n",
    "    methodGraphsFileName = [os.path.join(graphsDir, method.translate(None, string.digits), knowledgGraph, \"graph.txt\")]\n",
    "    print(\"methodGraphsFileName =\", methodGraphsFileName)\n",
    "elif method in {\"assocMi\"}:\n",
    "    if knowledgGraph == \"conceptnet5AssocGov\":\n",
    "        methodGraphsFileName = [os.path.join(graphsDir, \"assoc\", collection, \"conceptnet5AssocMod\" + \".txt\")]\n",
    "        methodGraphsFileName += [os.path.join(graphsDir, \"mi\", collection, \"gov\" + \".txt\")]\n",
    "        print(\"methodGraphsFileName =\", methodGraphsFileName)\n",
    "elif method in {\"assocHal\"}:\n",
    "    if knowledgGraph == \"conceptnet5AssocGov\":\n",
    "        methodGraphsFileName = [os.path.join(graphsDir, \"assoc\", collection, \"conceptnet5AssocMod\" + \".txt\")]\n",
    "        methodGraphsFileName += [os.path.join(graphsDir, \"hal\", \"gov\" + \".txt\")]\n",
    "        print(\"methodGraphsFileName =\", methodGraphsFileName)\n",
    "qrelsDir = os.path.join(projectDir, \"qrels\")\n",
    "colQrelsDir = os.path.join(qrelsDir, collection)\n",
    "colQrelsFileName = os.path.join(colQrelsDir, \"qrels.csv\")\n",
    "print(\"col Qrels File Name =\", colQrelsFileName)\n",
    "runsFileName = os.path.join(projectDir, \"runs\", method, collection, knowledgGraph, \"indriRunQuery.runs\") \n",
    "print(\"runsFileName =\", runsFileName)\n",
    "evalsFileName = os.path.join(projectDir, \"evals\", method, collection, knowledgGraph, \"indriRunQuery.evals\") \n",
    "print(\"evalsFileName =\", evalsFileName)\n",
    "dumpindexStatementFilename = os.path.join(\"/\", \"tmp\", \"sbsb\", \"statement\" + \"_\" + collection + \"_\" + method + \"_\" + knowledgGraph )\n",
    "print(\"dumpindexStatementFilename =\", dumpindexStatementFilename)\n",
    "occuranceCountFilename = os.path.join(projectDir, \"occuranceCount\", \"occuranceCount\")\n",
    "print(\"occuranceCountFilename =\", occuranceCountFilename)\n",
    "countsResultsFile = os.path.join(projectDir,\"occuranceCount\",\"results\",collection+\".txt\")\n",
    "print(\"countsResultsFile =\", countsResultsFile)\n",
    "conceptnet5RelAllFilename =\"/scratch/saeid/data/conceptnet5_simp.csv\"\n",
    "print(\"conceptnet5RelAllFilename =\", conceptnet5RelAllFilename)\n",
    "cfgTmpOutFileName = cfgOutFileName + \".tmp\"\n",
    "print(\"cfgTmpOutFileName =\", cfgTmpOutFileName)\n",
    "docIdNameMapFileName = os.path.join(projectDir, \"occuranceCount\", \"results\", collection + \"_docIdNameMap.txt\")\n",
    "print(\"docIdNameMapFileName =\", docIdNameMapFileName)\n",
    "#dumpindexResFilename = os.path.join(projectDir,\"occuranceCount\",\"results\",collection +\"_dumpindexRes\"+\".txt\")\n",
    "#print(\"dumpindexResFilename =\", dumpindexResFilename)\n",
    "#expansionCount = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# trec_eval -q ../qrels/trec7n8/qrels.csv indriRunQuery.res | grep map | grep -v all | awk '{if ($3 < 0.1) printf $2 \",\"}'\n",
    "if collection == \"gov\":\n",
    "    diffTopics = [4, 6, 7, 8, 9, 10, 12, 18, 19, 21, 22, 24, 26, 27, 30, 31, 32, 33, 36, 37, 40, 45, 46, 47, 48, 50, 52, 53, 54, 57, 58, 59, 63, 65, 66, 67, 70, 73, 78, 79, 80, 82, 84, 86, 92, 93, 94, 95, 96, 98, 99, 101, 102, 103, 104, 105, 106, 107, 109, 110, 111, 112, 113, 115, 116, 117, 118, 120, 123, 125, 126, 127, 129, 132, 133, 134, 135, 136, 137, 140, 142, 143, 147, 148, 149, 152, 153, 154, 155, 156, 157, 158, 160, 161, 162, 163, 165, 166, 167, 168, 169, 171, 175, 176, 177, 178, 179, 180, 182, 183, 185, 186, 188, 189, 191, 192, 194, 195, 196, 197, 200, 201, 202, 203, 204, 207, 208, 211, 212, 214, 215, 218, 219, 221, 224]\n",
    "if collection == \"robust\":\n",
    "    diffTopics = [301, 305, 309, 314, 315, 318, 319, 320, 322, 325, 327, 332, 336, 340, 342, 343, 344, 345, 346, 347, 350, 352, 354, 355, 356, 359, 363, 367, 371, 372, 376, 378, 379, 380, 381, 383, 386, 388, 389, 390, 393, 394, 397, 398, 401, 405, 409, 412, 419, 421, 426, 432, 435, 436, 437, 439, 440, 442, 448, 449, 605, 608, 610, 620, 622, 625, 626, 627, 638, 650, 651, 655, 659, 666, 668, 680, 683, 684, 688, 689, 690]\n",
    "if collection == \"trec7n8\":\n",
    "    diffTopics = [352,354,356,359,363,367,370,371,372,373,376,378,379,381,383,386,388,389,390,393,394,397,398,401,405,408,419,421,422,426,432,435,436,437,439,440,442,443,445,447,448,449]\n",
    "diffTopic = [str(i) for i in diffTopics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generate docIdNameMapFileName\n",
    "if not os.path.isfile(occuranceCountFilename):\n",
    "    cmd = ' '.join([occuranceCountFilename, colIndexDir, \"dm\", \">\", docIdNameMapFileName])\n",
    "    res = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE).stdout.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', 'FR940104-1-00001']\n",
      "LA082389-0020 \t 441223\n",
      "FBIS3-32596 \t 246524\n",
      "LA011890-0198 \t 465144\n",
      "LA011890-0199 \t 465145\n",
      "LA011890-0196 \t 465142\n",
      "LA011890-0197 \t 465143\n",
      "LA011890-0194 \t 465140\n",
      "FT932-3799 \t 88713\n",
      "LA011890-0192 \t 465138\n",
      "LA011890-0193 \t 465139\n",
      "LA011890-0190 \t 465136\n",
      "LA011890-0191 \t 465137\n",
      "FT932-3796 \t 88710\n",
      "FT924-2868 \t 187695\n",
      "FT924-2869 \t 187696\n",
      "FT932-3797 \t 88711\n",
      "FT924-2864 \t 187691\n",
      "FT924-2865 \t 187692\n",
      "FT924-2866 \t 187693\n",
      "FT924-2867 \t 187694\n",
      "FT924-2860 \t 187687\n",
      "FT924-2861 \t 187688\n"
     ]
    }
   ],
   "source": [
    "with open(docIdNameMapFileName, 'r') as f:\n",
    "    reader = list(csv.reader(f, delimiter = ' '))\n",
    "    print(reader[0])\n",
    "    docIdNameMap = {i[1]:i[0] for i in reader}\n",
    "    docNameIdMap = {i[0]:i[1] for i in reader}\n",
    "for i, (k, v) in enumerate(docIdNameMap.iteritems()):\n",
    "    print (k, '\\t', v)\n",
    "    if (i>20):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of count_history = 81446\n",
      "#uw(#4( storm ) #4( be appear in sky )) \t 0\n",
      "#uw(#4( work on sundai )) \t 137\n",
      "#uw(#4( calm time )) \t 23\n",
      "#uw(#4( kill ) #4( prai )) \t 394\n",
      "#uw(#4( bank ) #4( teller offic )) \t 5\n",
      "#uw(#4( water ) #4( hydrogen )) \t 347\n",
      "#uw(#4( drug ) #4( take )) \t 8847\n",
      "#uw(#4( space ) #4( mostli empti )) \t 3\n",
      "#uw(#4( chang ) #4( edg )) \t 3803\n",
      "#uw(#4( happin )) \t 0\n",
      "#uw(#4( trade ) #4( countri )) \t 45838\n",
      "#uw(#4( game ) #4( leisur activ )) \t 23\n",
      "#uw(#4( round in shape )) \t 4\n",
      "#uw(#4( good ) #4( other person )) \t 411\n",
      "#uw(#4( smoke ) #4( fire )) \t 1842\n",
      "#uw(#4( spice up salad )) \t 1\n",
      "#uw(#4( be wound )) \t 410\n",
      "#uw(#4( good ) #4( hear nice new )) \t 0\n",
      "#uw(#4( teacher ) #4( teach student of class )) \t 0\n",
      "#uw(#4( deliveri )) \t 11268\n",
      "#uw(#4( bank ) #4( teller monei )) \t 23\n",
      "#uw(#4( swap share )) \t 106\n"
     ]
    }
   ],
   "source": [
    "count_history = dict()\n",
    "if os.path.isfile(countsResultsFile):\n",
    "    with open(countsResultsFile, 'r') as f:\n",
    "        reader = csv.reader(f, delimiter = \"\\t\")\n",
    "        count_history = {k:int(float(v)) for k,v in list(reader)}\n",
    "    print(\"size of count_history =\", len(count_history))\n",
    "    for i, (k, v) in enumerate(count_history.iteritems()):\n",
    "        print (k, '\\t', v)\n",
    "        if (i>20):\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of conceptnet5RelAll = 2965684\n",
      "tripolitan \t tripoline \t Synonym\n",
      "tripolitan \t tripoli \t RelatedTo\n",
      "age of nemesis \t band \t IsA\n",
      "age of nemesis \t organisation \t InstanceOf\n",
      "age of nemesis \t progressive rock \t dbpedia/genre\n",
      "age of nemesis \t progressive metal \t dbpedia/genre\n",
      "joseph john annabring \t person \t InstanceOf\n",
      "british rail class 438 \t mean of transportation \t InstanceOf\n",
      "british rail class 438 \t train \t InstanceOf\n"
     ]
    }
   ],
   "source": [
    "conceptnet5RelAll = defaultdict(dict)\n",
    "conceptnet5RelAllInv = defaultdict(dict)\n",
    "with open(conceptnet5RelAllFilename, 'r') as f:\n",
    "    reader = csv.reader(f, delimiter = \",\")\n",
    "    for line in list(reader):\n",
    "        if line[0].strip() != \"\" and line[1].strip() != \"\" and line[2].strip() != \"\":\n",
    "            if all(c in string.printable for c in line[0]) and all(c in string.printable for c in line[1]) and all(c in string.printable for c in line[2]):\n",
    "                conceptnet5RelAll[line[1].strip()][line[2].strip()] = line[0].strip()\n",
    "                conceptnet5RelAllInv[line[2].strip()][line[1].strip()] = line[0].strip()\n",
    "print(\"size of conceptnet5RelAll =\", len(conceptnet5RelAll))\n",
    "for i, (k1, k2v) in enumerate(conceptnet5RelAll.iteritems()):\n",
    "    for j, (k2, v) in enumerate(k2v.iteritems()):\n",
    "        if (i>3 or j>3):\n",
    "            break\n",
    "        print (k1, '\\t', k2, '\\t', v)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "347\n"
     ]
    }
   ],
   "source": [
    "print(len(set(conceptnet5RelAll['poach']) | set(conceptnet5RelAll['preserve']) | set( conceptnet5RelAll['wildlife'] ) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'need to eat': 'CapableOf', 'learn how to dance': 'NotDesires', 'sense rain': 'CapableOf', 'blue rock': 'dbpedia/genre', 'provide fur coat': 'UsedFor', 'four': 'RelatedTo', 'any creature': 'RelatedTo', 'sleep': 'Desires', 'kill for food': 'UsedFor', 'trip person': 'CapableOf', 'hibernate in burrow in cold weather': 'CapableOf', 'hate': 'CapableOf', 'near extinction thank to human consumption': 'AtLocation', 'alternative rock': 'dbpedia/genre', 'have cancer': 'CapableOf', 'swan': 'RelatedTo', 'eukaryote': 'InstanceOf', 'young': 'RelatedTo', 'technical death metal': 'dbpedia/genre', 'be hurt': 'NotDesires', 'horse elephant': 'RelatedTo', 'rhythm and blue': 'dbpedia/genre', 'friendly': 'HasProperty', 'sound': 'RelatedTo', 'song': 'InstanceOf', 'very': 'RelatedTo', 'sound like': 'RelatedTo', 'pig cow': 'RelatedTo', 'step into trap': 'CapableOf', 'cool': 'HasProperty', 'multicellular': 'RelatedTo', 'die': 'NotDesires', 'dog elephant': 'RelatedTo', 'race': 'RelatedTo', 'biological': 'RelatedTo', 'cook in fire': 'ReceivesAction', 'meadow': 'AtLocation', 'heavy metal music': 'dbpedia/genre', 'orgasm': 'CapableOf', 'even number of leg': 'HasA', 'fur body': 'RelatedTo', 'movement sentience': 'RelatedTo', 'listen to sound': 'CapableOf', 'bacteria': 'Antonym', 'scientific': 'RelatedTo', 'what': 'RelatedTo', 'indie rock': 'dbpedia/genre', 'opaque': 'HasProperty', 'need shoe': 'NotCapableOf', 'your dog': 'RelatedTo', 'cell': 'RelatedTo', 'zoo thing': 'RelatedTo', 'bird': 'RelatedTo', 'movement': 'RelatedTo', 'body': 'RelatedTo', 'leg': 'RelatedTo', 'certain level of intelligence': 'HasA', 'live underground': 'CapableOf', 'men': 'RelatedTo', 'die only once': 'CapableOf', 'water': 'Desires', 'goat bear': 'RelatedTo', 'carry cargo': 'UsedFor', 'human be': 'RelatedTo', 'mot human': 'RelatedTo', 'warm blood': 'RelatedTo', 'come out of hole': 'IsA', 'in zoo': 'RelatedTo', 'big or small': 'HasProperty', 'cute sometimes': 'RelatedTo', 'keep alive': 'CapableOf', 'video game': 'InstanceOf', 'keen sense': 'HasA', 'wild life': 'RelatedTo', 'love': 'CapableOf', 'classification': 'RelatedTo', 'africa': 'AtLocation', 'prurience': 'DerivedFrom', 'british blue': 'dbpedia/genre', 'dream and do other activity': 'CapableOf', 'feel': 'HasA', 'eye': 'RelatedTo', 'musical work': 'InstanceOf', 'frog': 'RelatedTo', 'four foot': 'RelatedTo', 'live': 'DesireOf', 'major kingdom': 'RelatedTo', 'amusement park': 'RelatedTo', 'live long': 'CapableOf', 'type': 'RelatedTo', 'keep inside castle': 'ReceivesAction', 'progressive house music': 'dbpedia/genre', 'breathe': 'RelatedTo', 'company': 'InstanceOf', 'drown': 'NotDesires', 'mammal reptile': 'RelatedTo', 'rabbit': 'RelatedTo', 'like dog': 'RelatedTo', 'baby': 'HasA', 'generic category': 'RelatedTo', 'cow pig': 'RelatedTo', 'hold': 'RelatedTo', 'anything': 'RelatedTo', 'million of cell': 'HasA', 'science': 'RelatedTo', 'work': 'RelatedTo', 'soul': 'RelatedTo', 'cat': 'RelatedTo', 'reproduce': 'CapableOf', 'anywhere': 'RelatedTo', 'barnyard occupant': 'RelatedTo', 'den': 'AtLocation', 'all creature': 'RelatedTo', 'indie pop': 'dbpedia/genre', 'fauna': 'RelatedTo', 'conduct experiment': 'UsedFor', 'example': 'RelatedTo', 'move from place to place': 'CapableOf', 'general term': 'RelatedTo', 'generic beast': 'RelatedTo', 'lie down': 'CapableOf', 'every breathe thing': 'AtLocation', 'rape': 'CapableOf', 'cat fish': 'RelatedTo', 'no head at all': 'HasA', 'sit': 'IsA', 'plant mineral': 'Antonym', 'many emotion': 'HasA', 'hunt human': 'CapableOf', 'fertilize farm land': 'UsedFor', 'animal by miscellaneous criterion biology topic': 'IsA', 'feel too': 'HasA', 'drummer': 'RelatedTo', 'forest': 'AtLocation', 'pure': 'HasProperty', 'elephant': 'RelatedTo', 'use in sport event': 'HasProperty', 'goat': 'RelatedTo', 'creature': 'RelatedTo', 'grow on tree': 'NotCapableOf', 'wear clothe': 'NotCapableOf', 'plant': 'Antonym', 'pig': 'RelatedTo', 'farm': 'RelatedTo', 'get cold': 'CapableOf', 'organism by taxonomic kingdom biology topic': 'IsA', 'deathgrind': 'dbpedia/genre', 'not plant': 'IsA', 'abuse': 'ReceivesAction', 'blood': 'RelatedTo', 'feel emotion': 'CapableOf', 'scape trap': 'CapableOf', 'ark passenger': 'RelatedTo', 'fungi': 'RelatedTo', 'meat': 'HasA', 'nonhuman': 'RelatedTo', 'generic': 'RelatedTo', 'alive creature': 'RelatedTo', 'like horse': 'RelatedTo', 'travel': 'CapableOf', 'talk': 'NotCapableOf', 'general creature': 'RelatedTo', 'cute': 'RelatedTo', 'masturbate': 'CapableOf', 'barnyard': 'RelatedTo', 'born small and then get big': 'HasProperty', 'move': 'RelatedTo', 'entity': 'RelatedTo', 'brain': 'RelatedTo', 'breathe creature': 'RelatedTo', 'suffer': 'NotDesires', 'subject for porcelain figure': 'IsA', 'before': 'RelatedTo', 'cow horse': 'RelatedTo', 'group': 'RelatedTo', 'attract to their kind': 'ReceivesAction', 'organisation': 'InstanceOf', 'multiple artery': 'HasA', 'animal tissue': 'MadeOf', 'generic critter': 'RelatedTo', 'stand up comedy': 'dbpedia/genre', 'sense your presence': 'CapableOf', 'animal': 'InstanceOf', 'often furry': 'RelatedTo', 'food': 'HasPrerequisite', 'band': 'IsA', 'giraffe': 'RelatedTo', 'life farm': 'RelatedTo', 'foot': 'RelatedTo', 'human and': 'RelatedTo', 'be gay': 'CapableOf', 'term': 'RelatedTo', 'example dog': 'RelatedTo', 'organ': 'RelatedTo', 'poole': 'AtLocation', 'lion bear': 'RelatedTo', 'mammal group': 'RelatedTo', 'lock up in cage': 'ReceivesAction', 'puppet': 'RelatedTo', 'classifier': 'RelatedTo', 'bone': 'CapableOf', 'animalistic': 'Synonym', 'in cage': 'IsA', 'cat dog': 'RelatedTo', 'natural and cute': 'HasProperty', 'fish': 'RelatedTo', 'zebra': 'RelatedTo', 'in wild': 'RelatedTo', 'laboratory': 'AtLocation', 'amusement': 'RelatedTo', 'album': 'InstanceOf', 'bear squirrel': 'RelatedTo', 'hard rock': 'dbpedia/genre', 'be confine': 'NotDesires', 'body part': 'RelatedTo', 'thirst': 'NotDesires', 'fight for his life': 'CapableOf', 'mate': 'DesireOf', 'cigarette smoke': 'CapableOf', 'keep as pet': 'ReceivesAction', 'sovereign entity coexist with human': 'IsA', 'monster': 'Synonym', 'bury dead animal': 'NotCapableOf', 'move life': 'RelatedTo', 'mammal': 'RelatedTo', 'dorset': 'AtLocation', 'person dog': 'RelatedTo', 'sense their environment': 'CapableOf', 'almost no understand': 'HasA', 'star in film': 'CapableOf', 'thing': 'RelatedTo', 'place': 'InstanceOf', 'heartbeat': 'RelatedTo', 'be love': 'Desires', 'mammal creature': 'RelatedTo', 'emotion': 'HasA', 'major': 'RelatedTo', 'very generic': 'RelatedTo', 'play soccer': 'NotCapableOf', 'relate': 'RelatedTo', 'like be in cage': 'NotCapableOf', 'heterotroph': 'IsA', 'sense earthquake': 'CapableOf', 'procreate': 'CapableOf', 'hear it own voice': 'CapableOf', 'elephant mouse': 'RelatedTo', 'talk like human': 'NotCapableOf', 'have new experience': 'CapableOf', 'deathcore': 'dbpedia/genre', 'sheep': 'RelatedTo', 'horse': 'RelatedTo', 'fight each other': 'CapableOf', 'dog pig': 'RelatedTo', 'bite': 'CapableOf', 'breed': 'RelatedTo', 'system': 'RelatedTo', 'name': 'RelatedTo', 'attack': 'CapableOf', 'smart as human': 'NotHasProperty', 'tiger elephant': 'RelatedTo', 'software object': 'IsA', 'work for human be': 'CapableOf', 'murder': 'CapableOf', 'animal or body part': 'IsA', 'park': 'AtLocation', 'artifact': 'NotIsA', 'in forest': 'AtLocation', 'state park': 'AtLocation', 'convert into food': 'ReceivesAction', 'live organism': 'RelatedTo', 'wild mammal': 'RelatedTo', 'you act': 'Antonym', 'critter': 'RelatedTo', 'kind': 'RelatedTo', 'die of cancer': 'CapableOf', 'cuddly pet': 'UsedFor', 'generic cat': 'RelatedTo', 'elephant also': 'RelatedTo', 'feel thought and emotion': 'HasA', 'alternative metal': 'dbpedia/genre', 'be good': 'CapableOf', 'exhibit': 'RelatedTo', 'one head': 'HasA', 'illness': 'NotDesires', 'starve': 'NotDesires', 'defend other animal': 'CapableOf', 'indietronica': 'dbpedia/genre', 'use as pet': 'ReceivesAction', 'generalization': 'RelatedTo', 'before person': 'RelatedTo', 'biological lifeform': 'IsA', 'cell wall': 'RelatedTo', 'any': 'RelatedTo', 'four legged': 'RelatedTo', 'lie': 'CapableOf', 'zoo': 'RelatedTo', 'also': 'RelatedTo', 'puppet drummer': 'RelatedTo', 'build': 'InstanceOf', 'play': 'RelatedTo', 'pain': 'NotDesires', 'fox bear': 'RelatedTo', 'born and then grow large': 'HasProperty', 'object': 'CapableOf', 'into ark': 'RelatedTo', 'kill thing': 'RelatedTo', 'help person': 'CapableOf', 'generic mammal': 'RelatedTo', 'mineral': 'Antonym', 'mobile': 'RelatedTo', 'gain wieght': 'CapableOf', 'sometimes': 'RelatedTo', 'm': 'EtymologicallyDerivedFrom', 'dog': 'RelatedTo', 'general mammal': 'RelatedTo', 'sleep in daylight hour': 'CapableOf', 'feel and emotion': 'HasA', 'man': 'Antonym', 'sensualize': 'DerivedFrom', 'organism': 'IsA', 'hunger': 'NotDesires', 'rabbit duck': 'RelatedTo', 'kill person': 'CapableOf', 'horse cow': 'RelatedTo', 'north america': 'AtLocation', 'plant and': 'RelatedTo', 'kingdom classifier': 'RelatedTo', 'what we': 'RelatedTo', 'visit in zoo': 'UsedFor', 'sheep pig': 'RelatedTo', 'dance rock': 'dbpedia/genre', 'nervous': 'RelatedTo', 'animality': 'DerivedFrom', 'eat': 'DesireOf', 'be burn': 'NotDesires', 'wood': 'RelatedTo', 'science kingdom': 'RelatedTo', 'cow dog': 'RelatedTo', 'beat': 'RelatedTo', 'bear': 'RelatedTo', 'strong sense': 'HasA', 'have sex': 'AtLocation', 'occupant': 'RelatedTo', 'cat general': 'RelatedTo', 'handicap': 'NotDesires', 'relate to': 'RelatedTo', 'be mammal': 'CapableOf', 'furry creature': 'RelatedTo', 'giraffe lion': 'RelatedTo', 'mammal general': 'RelatedTo', 'be attack': 'NotDesires', 'understand human speech': 'NotCapableOf', 'ecosystem': 'PartOf', 'experimental rock': 'dbpedia/genre', 'sense change in weather': 'CapableOf', 'eat fish': 'CapableOf', 'chew on thing': 'Desires', 'sensuality': 'DerivedFrom', 'fair': 'AtLocation', 'animalia': 'MemberOf', 'feel hunger': 'NotDesires', 'lion': 'RelatedTo', 'mouse fish': 'RelatedTo', 'ark': 'RelatedTo', 'fang': 'RelatedTo', 'please': 'HasProperty', 'be male': 'CapableOf', 'outside': 'AtLocation', 'it furry': 'RelatedTo', 'pet store': 'AtLocation', 'we': 'RelatedTo', 'lion frog': 'RelatedTo', 'nature': 'PartOf', 'their love to each others': 'CapableOf', 'live be': 'RelatedTo', 'resist other animal': 'CapableOf', 'dog cat': 'RelatedTo', 'animus': 'RelatedTo', 'bear elephant': 'RelatedTo', 'diverse thing': 'RelatedTo', 'move itself about': 'CapableOf', 'last': 'RelatedTo', 'cow': 'RelatedTo', 'restaurant': 'InstanceOf', 'go to zoo': 'CausesDesire', 'live in fear': 'NotDesires', 'play instrument': 'NotCapableOf', 'die in laboratory': 'CapableOf', 'speak': 'NotCapableOf', 'companionship': 'UsedFor', 'edible': 'HasProperty', 'last question': 'RelatedTo', 'creature general': 'RelatedTo', 'vertebrate': 'RelatedTo', 'eukaryotic organism': 'IsA', 'on farm': 'RelatedTo', 'warmth': 'DesireOf', 'life': 'RelatedTo', 'be eat': 'NotDesires', 'squirrel': 'RelatedTo', 'beastly': 'Synonym', 'zoo exhibit': 'RelatedTo', 'cell molecule and atom': 'MadeOf', 'bear cow': 'RelatedTo', 'live young': 'RelatedTo', 'general': 'RelatedTo', 'spirit': 'RelatedTo', 'no human': 'RelatedTo', 'alive thing': 'RelatedTo', 'fur': 'UsedFor', 'bird and': 'RelatedTo', 'bird mammal': 'RelatedTo', 'move from one place to another': 'CapableOf', 'nature kingdom': 'RelatedTo', 'act': 'Antonym', 'part of natural world': 'IsA', 'be alone': 'CapableOf', 'bird zebra': 'RelatedTo', 'house hold': 'RelatedTo', 'new wave music': 'dbpedia/genre', 'wild': 'Synonym', 'biological kingdom': 'IsA', 'architectural structure': 'InstanceOf', 'bathe': 'CapableOf', 'allow in most church service': 'NotHasProperty', 'all kind': 'RelatedTo', 'mouse deer': 'RelatedTo', 'burrow very fast': 'CapableOf', 'it': 'RelatedTo', 'creature generic': 'RelatedTo', 'feel pleasure': 'CapableOf', 'make fur coat': 'UsedFor', 'horse dog': 'RelatedTo', 'bestial': 'Synonym', 'drive car': 'NotCapableOf', 'mouse': 'RelatedTo', 'get disease': 'CapableOf', 'feed': 'RelatedTo', 'generic term': 'RelatedTo', 'all beast': 'RelatedTo', 'feel pain': 'CapableOf', 'sense their owner feel': 'CapableOf', 'live creature': 'IsA', 'biological machine': 'IsA', 'woodland': 'AtLocation', 'make leather': 'UsedFor', 'synthpop': 'dbpedia/genre', 'strange': 'HasProperty', 'pass water': 'CapableOf', 'order food': 'CapableOf', 'vegetable': 'RelatedTo', 'nervous system': 'RelatedTo', 'generic name': 'RelatedTo', 'house music': 'dbpedia/genre', 'breed to enhance future generation': 'ReceivesAction', 'life form': 'IsA', 'drink': 'CapableOf', 'beast': 'RelatedTo', 'find in zoo': 'ReceivesAction', 'sense fear': 'CapableOf', 'human plant': 'Antonym', 'work play': 'RelatedTo', 'kid': 'RelatedTo', 'thing require too much intelligence': 'NotCapableOf', 'many specie': 'RelatedTo', 'detroit zoo': 'AtLocation', 'keep you company': 'UsedFor', 'person': 'RelatedTo', 'organization': 'IsA', 'muscle': 'HasA', 'musical composition': 'IsA', 'make wine': 'CapableOf', 'livestock': 'RelatedTo', 'consciousness': 'RelatedTo', 'sense fear in human': 'CapableOf', 'live in house': 'NotCapableOf', 'find in forest': 'ReceivesAction', 'competitive': 'HasProperty', 'be carnivore omnivore or vegetarian': 'CapableOf', 'not': 'RelatedTo', 'kill': 'RelatedTo', 'human': 'Antonym', 'part': 'RelatedTo', 'yes': 'RelatedTo', 'not human': 'IsA', 'death': 'NotDesires', \"live body while computer don't\": 'HasA', 'furry': 'RelatedTo', 'post grunge': 'dbpedia/genre', 'book': 'IsA', 'keep in steel pen': 'ReceivesAction', 'transport in aircraft': 'ReceivesAction', 'kingdom': 'RelatedTo', 'write work': 'InstanceOf', 'live specie': 'RelatedTo', 'psychedelic pop': 'dbpedia/genre', 'read': 'NotCapableOf', 'keep in garage': 'ReceivesAction', 'mot': 'RelatedTo', 'duck swan': 'RelatedTo', 'pet wild': 'RelatedTo', 'rock music': 'dbpedia/genre', 'bone inside of them': 'HasA', 'be with more than one not clorofilated eucaryote cell': 'IsA', 'like': 'RelatedTo', 'part hair': 'CapableOf', 'specie': 'InstanceOf', 'all mammal': 'RelatedTo', 'specific': 'Antonym', 'psychedelic rock': 'dbpedia/genre', 'claw': 'RelatedTo', 'adventure game': 'dbpedia/genre', 'lie organism': 'IsA', 'ecology': 'PartOf', 'often': 'RelatedTo', 'habitat': 'RelatedTo', 'duck': 'RelatedTo', 'live entity': 'RelatedTo', 'alive': 'RelatedTo', 'run after it prey': 'CapableOf', 'eat spaghetti': 'CapableOf', 'lion tiger': 'RelatedTo', 'animate': 'RelatedTo', 'progressive rock': 'dbpedia/genre', 'savage': 'Synonym', 'play with lego': 'CapableOf', 'specie cat': 'RelatedTo', 'hip hop music': 'dbpedia/genre', 'mineral vegetable': 'Antonym', 'pet': 'UsedFor', 'delaware': 'AtLocation', 'fox': 'RelatedTo', 'everything': 'RelatedTo', 'pee': 'CapableOf', 'provide companionship': 'UsedFor', 'specific mark': 'HasA', 'victory': 'RelatedTo', 'be': 'RelatedTo', 'live life': 'RelatedTo', 'fictional character': 'InstanceOf', 'listen to each other': 'CapableOf', 'different color': 'HasA', 'carry infection': 'CapableOf', 'untamed': 'Synonym', 'computer game': 'IsA', 'privately hold company': 'InstanceOf', 'freedom': 'Desires', 'transmit disease': 'CapableOf', 'nonhuman creature': 'RelatedTo', 'anima': 'RelatedTo', 'generic dog': 'RelatedTo', 'sense danger': 'CapableOf', 'software': 'InstanceOf', 'reptile': 'RelatedTo', 'film': 'InstanceOf', 'in farm': 'RelatedTo', 'into': 'RelatedTo', 'zoan': 'DerivedFrom', 'pet shop': 'AtLocation', 'legged': 'RelatedTo', 'so natural and cute': 'HasProperty', 'water plant': 'CapableOf', 'gary': 'RelatedTo', 'surface of earth': 'AtLocation', 'your': 'RelatedTo', 'be friend with person': 'CapableOf', 'math rock': 'dbpedia/genre', 'move in photograph': 'NotCapableOf', 'drink wine': 'CapableOf', 'live thing': 'RelatedTo', 'question': 'RelatedTo', 'brute': 'Synonym', 'feel similar to human': 'HasA', 'house': 'RelatedTo', 'chemotrophic': 'IsA', 'note food': 'CapableOf', 'grunge': 'dbpedia/genre', 'body language and can show expression': 'HasA', 'form': 'RelatedTo', 'embody perceptual agent': 'IsA', 'scape zoo': 'CapableOf', 'hear': 'CapableOf', 'save money': 'CapableOf', 'affectionate also': 'IsA', 'non scientific': 'RelatedTo', 'bug': 'Antonym', 'fictitious character': 'IsA', 'dangerous': 'HasProperty', 'sleep in bed': 'CapableOf', 'mammal fish': 'RelatedTo', 'use in sport': 'ReceivesAction', 'anything breathe': 'RelatedTo', 'passenger': 'RelatedTo', 'speak in human language': 'NotCapableOf', 'or mineral': 'RelatedTo', 'worm in their intestine': 'HasA', 'deer': 'RelatedTo', 'vegetable mineral': 'RelatedTo', 'hunt for food': 'CapableOf', 'single': 'InstanceOf', 'diverse': 'RelatedTo', 'bear mouse': 'RelatedTo', 'leave it home': 'CapableOf', 'at zoo': 'AtLocation', 'cafe': 'AtLocation', 'bleed when cut': 'CapableOf', 'act reflexively': 'CapableOf', 'copulate': 'CapableOf', 'music single': 'IsA', 'you': 'RelatedTo', 'be pet': 'CapableOf', 'nice': 'RelatedTo', 'category': 'RelatedTo', 'poop': 'CapableOf', 'four leg': 'RelatedTo', 'fairground': 'AtLocation', 'horse bug': 'RelatedTo', 'insect': 'RelatedTo', 'dance pop': 'dbpedia/genre', 'hear other voice': 'CapableOf', 'special mean of defense and protection': 'HasA', 'mammal bird': 'RelatedTo', 'beastiality': 'UsedFor', 'tiger': 'RelatedTo', 'horse pig': 'RelatedTo', 'sentience': 'RelatedTo', 'warm': 'RelatedTo', 'everywhere': 'RelatedTo'}\n"
     ]
    }
   ],
   "source": [
    "print(conceptnet5RelAll['animal']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indriRunQuery_hist_= dict()\n",
    "dumpindex_hist_ = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def splitStemText(text):\n",
    "    stemmedWords = []\n",
    "    text = re.sub('/|-|\\\"|_',' ', text) # replace - and slash with space\n",
    "    if method in {\"assocRestful\", \"lr\", \"wsdmImpr\", \"wsdmImprCoorAsc\"}:\n",
    "        for text_ in text.split(' '):\n",
    "            text = text_.replace(\" \", \"_\")\n",
    "            url = \"http://conceptnet5.media.mit.edu/data/5.4/uri?language=en&text=\" + text\n",
    "            response = urllib2.urlopen(url)\n",
    "            data = response.read()\n",
    "            data_j = json.loads(data)\n",
    "            text = os.path.basename(data_j[u'uri'])\n",
    "            url = \"http://conceptnet5.media.mit.edu/data/5.4/uri?language=en&text=\" + text\n",
    "            response = urllib2.urlopen(url)\n",
    "            data = response.read()\n",
    "            data_j = json.loads(data)\n",
    "            #print(\"data_j =\", data_j)\n",
    "            #print(\"data_j[u'uri'] =\", data_j[u'uri'])\n",
    "            stemmedWords += [str(os.path.basename(data_j[u'uri']))]\n",
    "        #print(\"stemmedWords =\", stemmedWords)\n",
    "        return (stemmedWords)\n",
    "    else:\n",
    "        words = text.split()\n",
    "        for w in words:\n",
    "            w = re.sub('\\(|\\)|\\'s|,','', w) # remove paranthesis, apostrophe s, comma\n",
    "            w = re.sub('\\'','', w) # remove apostrophe\n",
    "            cmd = \"dumpindex \" + knowledgGraphIndexDir + \" t \" + w + \" | head -n1\"\n",
    "            #print(\"cmd =\", cmd)\n",
    "            stemmedWords.append(subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE).stdout.read().split()[1])\n",
    "        return (stemmedWords)\n",
    "\n",
    "print (\"testing:\", splitStemText(\"going reading\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def analyseQueries():\n",
    "    #    #print(\"intCoeff =\", intCoeff)\n",
    "    with open(cfgInFileName, 'r') as inFile:\n",
    "        reader = inFile.read()\n",
    "        soup = BeautifulSoup(reader, 'lxml')\n",
    "        if collection in {\"aquaint\", \"gov\"}:\n",
    "            tags = soup.find_all(['doc'])\n",
    "        elif collection in {\"robust\", \"trec7n8\", \"gov2\"}:\n",
    "            tags = soup.find_all(['top'])\n",
    "            #print(tags)\n",
    "        \n",
    "        origWordsAll = dict()\n",
    "        for tag in tags:\n",
    "            \n",
    "            if collection in {\"aquaint\", \"gov\"}:\n",
    "                docno = (tag.find('docno').string).strip()\n",
    "            elif collection in {\"robust\", \"trec7n8\", \"gov2\"}:\n",
    "                result = re.search('<num> Number: (.*)\\n', str(tag))                \n",
    "                docno = result.group(1).strip()\n",
    "            \n",
    "            if collection in {\"aquaint\", \"gov\"}:\n",
    "                origWords = splitStemText(tag.find('text').string.strip())\n",
    "            elif collection in {\"robust\", \"trec7n8\", \"gov2\"}:\n",
    "                result = re.search('<title>(.*)\\n', str(tag))                \n",
    "                origWords = splitStemText(result.group(1).strip())\n",
    "            #print(\"origWords =\", origWords)\n",
    "            origWordsAll[docno] = origWords\n",
    "            \n",
    "    return (origWordsAll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cmd = ' '.join([\"dumpindex\", colIndexDir, \"s\", \"|\", \"awk\", \"\\'NR==2{print $2}\\'\"])\n",
    "res = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE).stdout.read()\n",
    "colDocumentCount = int(np.float(res))\n",
    "print(\"colDocumentCount =\", colDocumentCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def genMultSimpQuery(argss):\n",
    "    #    #print(\"intCoeff =\", intCoeff)\n",
    "    soupNew = BeautifulSoup(\"\", 'lxml')\n",
    "    parameters_tag = soupNew.new_tag(\"parameters\")\n",
    "    index_tag = soupNew.new_tag(\"index\")\n",
    "    index_tag.string = colIndexDir\n",
    "    parameters_tag.append(index_tag)\n",
    "        #print(tags)\n",
    "\n",
    "    for args in argss:\n",
    "        #print(\"args =\", args)\n",
    "        (dist, docno, origWords_, relW, intCoeff0, intCoeff1, intCoeff2, dirCoeff, featureWeights, workingset) = args\n",
    "        #print(\"(...) =\", docno, origWords_, intCoeff0, intCoeff1, intCoeff2, dirCoeff, featureWeights, workingset)\n",
    "        doc_tag = soupNew.new_tag(\"query\")\n",
    "\n",
    "        docno_tag = soupNew.new_tag(\"number\")\n",
    "        if relW != \"\":\n",
    "            docno_tag.string = '_'.join([dist, docno, ('_'.join(origWords_)).replace(' ', '_'), relW.replace(' ', '_')])\n",
    "        else:\n",
    "            docno_tag.string = '_'.join([dist, docno, ('_'.join(origWords_)).replace(' ', '_'), relW.replace(' ', '_')])\n",
    "\n",
    "        doc_tag.append(docno_tag)\n",
    "\n",
    "        text_tag = soupNew.new_tag(\"text\")\n",
    "        text_tag.string = \"#weight(\\n\" \n",
    "\n",
    "        text_tag.string += str(intCoeff0) + \" #combine(\" \n",
    "        for ow in set(origWords_):\n",
    "            ow = re.sub(ur\"[^\\w\\d#(),:\\-\\s]+\", ' ', ow)\n",
    "            text_tag.string += ow + \" \"\n",
    "        text_tag.string += \")\\n\"\n",
    "\n",
    "        if len(relW)>0: \n",
    "            relText_string1 = re.sub(ur\"[^\\w\\d#(),:\\-\\s]+\",' ',relW)\n",
    "            text_tag.string += str(intCoeff1) + \" #combine(\" \n",
    "            #print(\"relText_string1 =\", relText_string1)\n",
    "            text_tag.string += relText_string1.encode('utf-8')\n",
    "            text_tag.string += \")\\n\"\n",
    "\n",
    "        text_tag.string += \") \"\n",
    "\n",
    "        doc_tag.append(text_tag)\n",
    "        \n",
    "        if workingset[0] != \"all\":\n",
    "            for d in workingset:\n",
    "                workingSetDocno_tag = soupNew.new_tag(\"workingSetDocno\")\n",
    "                workingSetDocno_tag.string = d\n",
    "                doc_tag.append(workingSetDocno_tag)\n",
    "\n",
    "        parameters_tag.append(doc_tag)\n",
    "        #print(doc_tag)\n",
    "\n",
    "    rule_tag = soupNew.new_tag(\"rule\")\n",
    "    rule_tag.string = \"method:dir,mu:\" + str(dirCoeff)\n",
    "    #rule_tag.string = \"method:two\"\n",
    "    parameters_tag.append(rule_tag)\n",
    "\n",
    "    threads_tag = soupNew.new_tag(\"threads\")\n",
    "    threads_tag.string = \"32\"\n",
    "    parameters_tag.append(threads_tag)\n",
    "\n",
    "    count_tag = soupNew.new_tag(\"count\")\n",
    "    count_tag.string = \"10\"\n",
    "    parameters_tag.append(count_tag)\n",
    "\n",
    "    trecFormat_tag = soupNew.new_tag(\"trecFormat\")\n",
    "    trecFormat_tag.string = \"true\"\n",
    "    parameters_tag.append(trecFormat_tag)\n",
    "\n",
    "    soupNew.append(parameters_tag)\n",
    "    #print(soupNew.prettify())\n",
    "    #print(\"cfgTmpOutFileName =\", cfgTmpOutFileName)\n",
    "    with open( cfgTmpOutFileName, 'a+') as outFile:\n",
    "        soupNewStr = str(soupNew)\n",
    "        soupNewStr = soupNewStr.replace(\"</text>\", \"\\n</text>\\n\").replace(\"query>\", \"query>\\n\").replace(\"<text>\", \"\\n<text>\\n\").replace(\"</index>\", \"</index>\\n\").replace(\"\\n<index>\", \"<index>\")\n",
    "\n",
    "        outFile.write(soupNewStr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def genCommandsinHist_i(dist, docno, origWords_, relW, intCoeff0, intCoeff1, intCoeff2, dirCoeff, featureWeights):\n",
    "    \n",
    "    indriRunQuery_hist = [(dist, docno, origWords_, relW, intCoeff0, intCoeff1, intCoeff2, dirCoeff, cfgTmpOutFileName, ['all'])]\n",
    "    return indriRunQuery_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def runCommandsinHist_i(cfgTmpOutFileName, indriRunQuery_hist, indriRunQuery_hist_, featureWeights):\n",
    "    with open( cfgTmpOutFileName, 'w+') as outFile:\n",
    "        outFile.write(\"\")\n",
    "    args = []\n",
    "    #print(\"indriRunQuery_hist =\", indriRunQuery_hist, \"\\nlen(indriRunQuery_hist) =\",len(indriRunQuery_hist))\n",
    "    for (dist, docno, origWords_, relW, intCoeff0, intCoeff1, intCoeff2, dirCoeff, cfgTmpOutFileName, workingset) in indriRunQuery_hist:\n",
    "        #print(\"origWords_, relW (before if) =\", origWords_, \"---\", relW)\n",
    "        if '_'.join([dist, docno, ('_'.join(origWords_)).replace(' ', '_'), relW.replace(' ', '_')]) not in indriRunQuery_hist_:\n",
    "            #print(\"origWords_, relW (after if) =\", origWords_, \"---\", relW)\n",
    "            args += [(dist, docno, origWords_, relW, intCoeff0, intCoeff1, intCoeff2, dirCoeff, featureWeights, workingset)]\n",
    "    #print(\"args =\", args)\n",
    "    if len(args)>0:\n",
    "        genMultSimpQuery(args)\n",
    "    cmd = ' '.join([\"IndriRunQuery\", cfgTmpOutFileName])\n",
    "    #print(\"cmd =\", cmd)\n",
    "    indriRun = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE).stdout.read()\n",
    "    #print(\"indriRun =\", indriRun)\n",
    "    indriRunQuery_hist_tmp = [[j for c, j in enumerate(i.split(' ')) if c in {0, 2, 4}] for i in indriRun.split('\\n') if len(i)>0]\n",
    "    #indriRunQuery_hist_ = dict()\n",
    "    for i in indriRunQuery_hist_tmp:\n",
    "        if i[0] in indriRunQuery_hist_:\n",
    "            indriRunQuery_hist_[i[0]] += [[i[1], i[2]]]\n",
    "        else:\n",
    "            indriRunQuery_hist_[i[0]] = [[i[1], i[2]]]\n",
    "    #indriRunQuery_hist_ = list(set(indriRunQuery_hist_))\n",
    "    return indriRunQuery_hist_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def genCommandsinHist_d(indriRunQuery_hist_, dumpindex_hist, dist, docno, origWords_, ow, relW, intCoeff0, intCoeff1, intCoeff2, dirCoeff, featureWeights):\n",
    "\n",
    "    expr = '_'.join([dist, docno, '_'.join(origWords_), \"\"]).replace(' ', '_')\n",
    "    #print(\"expr =\", expr)\n",
    "    indriRunQuery_hist_1 = indriRunQuery_hist_[expr]\n",
    "    #print(\"indriRunQuery_hist_1 =\", indriRunQuery_hist_1)\n",
    "    topTDocs = [i[0] for i in indriRunQuery_hist_1]\n",
    "    \n",
    "    expr = \"#4( \" + regex.sub(' ', relW) + \" )\"\n",
    "    #expr = '_'.join([dist, docno, '_'.join(origWords_), \"\"]).replace(' ', '_')\n",
    "    #print(\"expr =\", expr)\n",
    "    #cmd2 = [(dist, docno, [expr], \"\", intCoeff0, intCoeff1, intCoeff2, dirCoeff, cfgTmpOutFileName, topTDocs)]\n",
    "    #if cmd2 not in indriRunQuery_phCounter_hist:\n",
    "    #    indriRunQuery_phCounter_hist += cmd2\n",
    "    cmd2 = (\"efb\", expr + \":\" + ','.join(topTDocs))\n",
    "    if cmd2 not in dumpindex_hist:\n",
    "        dumpindex_hist.add(cmd2)\n",
    "    \n",
    "    #print(\"relW =\", relW)\n",
    "    expr = \"#4( \" + regex.sub(' ', relW) + \" )\"\n",
    "    cmd2 = (\"fx\", expr)\n",
    "    if cmd2 not in dumpindex_hist:\n",
    "        dumpindex_hist.add(cmd2)\n",
    "    #print(\"dumpindex_hist =\", dumpindex_hist)\n",
    "    \n",
    "    for documentName in topTDocs:\n",
    "        cmd2 = (\"dcf\", docIdNameMap[documentName])\n",
    "        if cmd2 not in dumpindex_hist:\n",
    "            dumpindex_hist.add(cmd2)\n",
    "\n",
    "    for ow in origWords_:\n",
    "        phrase = [\"#uw(#4( \" + regex.sub(' ', ow) + \" ) #4( \" + regex.sub(' ', relW) + \" ))\"]\n",
    "        #cmd2 = (\"efb\", \"#uw(#4( \" + regex.sub(' ', ow) + \" ) #4( \" + regex.sub(' ', relW) + \" )):\" + ','.join(topTDocs))\n",
    "        #cmd2 = [(dist, docno, phrase, \"\", intCoeff0, intCoeff1, intCoeff2, dirCoeff, cfgTmpOutFileName, topTDocs)]\n",
    "        #if cmd2 not in indriRunQuery_phCounter_hist:\n",
    "        #    indriRunQuery_phCounter_hist += cmd2        \n",
    "        cmd2 = (\"fx\", phrase[0])\n",
    "        if cmd2 not in dumpindex_hist:\n",
    "            dumpindex_hist.add(cmd2)\n",
    "        \n",
    "        cmd2 = (\"efb\", phrase[0] + \":\" + ','.join(topTDocs))\n",
    "        if cmd2 not in dumpindex_hist:\n",
    "            dumpindex_hist.add(cmd2)\n",
    "            \n",
    "    for j1, ow1 in enumerate(origWords_):\n",
    "        for j2, ow2 in enumerate(origWords_):\n",
    "            if j1>j2:\n",
    "                phrase = [\"#uw(#4( \" + regex.sub(' ', ow1) + \" ) #4( \" + regex.sub(' ', ow2) + \" ) #4( \" + regex.sub(' ', relW) + \" ))\"]\n",
    "                #cmd2 = [(dist, docno, phrase, \"\", intCoeff0, intCoeff1, intCoeff2, dirCoeff, cfgTmpOutFileName, topTDocs)]\n",
    "                #if cmd2 not in indriRunQuery_phCounter_hist:\n",
    "                #    indriRunQuery_phCounter_hist += cmd2        \n",
    "                #cmd2 = (\"fx\", phrase[0])\n",
    "                cmd2 = (\"fx\", phrase[0])\n",
    "                if cmd2 not in dumpindex_hist:\n",
    "                    dumpindex_hist.add(cmd2)\n",
    "                cmd2 = (\"efb\", phrase[0] + \":\" + ','.join(topTDocs))\n",
    "                if cmd2 not in dumpindex_hist:\n",
    "                    dumpindex_hist.add(cmd2)\n",
    "    #print(\"len(dumpindex_hist) =\", len(dumpindex_hist))\n",
    "    return dumpindex_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "def runCommandsinHist_d(dist, docno, dumpindex_hist, dumpindex_hist_):\n",
    "    args = []\n",
    "    dumpindex_hist_dict = dict()\n",
    "    tmp = set()\n",
    "    #print(\"dumpindex_hist =\", dumpindex_hist)\n",
    "    for i in dumpindex_hist:\n",
    "        expr = '_'.join([dist, docno, '_'.join(i[0]), \"\"]).replace(' ', '_')\n",
    "        if i[1] not in dumpindex_hist_.get(expr, dict()):\n",
    "#        if i[1] not in dumpindex_hist_.get(dist + \"_\" + docno + \"_\" + i[0], dict()):\n",
    "            if i[1] in tmp:\n",
    "                continue\n",
    "            #print(\"i =\", i)\n",
    "            tmp.add(i[1])\n",
    "            if i[0] in dumpindex_hist_dict:\n",
    "                dumpindex_hist_dict[i[0]] += [[re.sub(ur\"[^\\w\\d#(),:\\-\\s]+\",' ',j) for j in i[1:]]]\n",
    "            else:\n",
    "                dumpindex_hist_dict[i[0]] =  [[re.sub(ur\"[^\\w\\d#(),:\\-\\s]+\",' ',j) for j in i[1:]]]\n",
    "                \n",
    "    for k, v in dumpindex_hist_dict.iteritems():\n",
    "        fileName = dumpindexStatementFilename + \"_\" + k + \".tmp\"\n",
    "        with open(fileName, 'w') as f:\n",
    "            for i in v:\n",
    "                f.write(' '.join(i) + '\\n')\n",
    "                #print(\"i =\", i)\n",
    "        cmd = ' '.join([occuranceCountFilename, colIndexDir, k, fileName])\n",
    "        #print(\"cmd =\", cmd)\n",
    "        dumpindex_hist_tmp = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE).stdout.read()\n",
    "        #print(\"dumpindex_hist_tmp =\", dumpindex_hist_tmp)\n",
    "        dumpindex_hist_tmp = dict([tuple(j for c2, j in enumerate(i.split(':')) if c2 in {0,1} ) for i in dumpindex_hist_tmp.split(\"\\n\") if len(i)>0])\n",
    "        #print(\"dumpindex_hist_tmp.get(\\\"#od4(travel)\\\",\\\"\\\") =\", dumpindex_hist_tmp.get(\"#od4(travel)\", dict()) )\n",
    "        dumpindex_hist_tmp = {k1:v1.split(',') for k1,v1 in dumpindex_hist_tmp.iteritems()}\n",
    "        dumpindex_hist_tmp = {dist + \"_\" + docno + \"_\" + k1:[i for i in v1 if len(i)>0] for k1,v1 in dumpindex_hist_tmp.iteritems()}\n",
    "        #dumpindex_hist_tmp = {re.sub(ur\"[^\\w\\d#()\\s]+\",' ',k1):[i for i in v1 if len(i)>0] for k1,v1 in dumpindex_hist_tmp.iteritems()}\n",
    "        #dumpindex_tmp = [i for i in dumpindex_tmp.split(\"\\n\")]\n",
    "        #print(\"dumpindex_hist_tmp =\", dumpindex_hist_tmp)\n",
    "        #dumpindex_hist_[k] = dumpindex_hist_tmp\n",
    "        #expr = '_'.join([dist, docno, '_'.join(k), \"\"]).replace(' ', '_')    \n",
    "        dumpindex_hist_[k] = dict(dumpindex_hist_.get(k, dict()).items() + dumpindex_hist_tmp.items())\n",
    "        #dumpindex_hist_[dist + \"_\" + docno + \"_\" + k] = dict(dumpindex_hist_.get(dist + \"_\" + docno + \"_\" + k, dict()).items() + dumpindex_hist_tmp.items())\n",
    "    #print(\"dumpindex_hist_['efb'].get('#od4(travel)', dict())]\", dumpindex_hist_['efb'].get(\"#od4(travel)\", dict()))\n",
    "        #print(\"dumpindex_hist_[k].keys() =\", dumpindex_hist_[k].keys())\n",
    "    #print(\"dumpindex_hist_ =\", dumpindex_hist_)\n",
    "        \n",
    "    return dumpindex_hist_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# keys of rWords are important\n",
    "# values of rWords1 are important\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "def getRelWords_train(args, indriRunQuery_hist_, dumpindex_hist_):\n",
    "    #print(\"origWords =\", origWords)\n",
    "    text_string = \"\"\n",
    "    \n",
    "    count_history, rWords, dist, docno, origWords, expansionCount, intCoeff0, intCoeff1, intCoeff2, dirCoeff, featureWeights = args\n",
    "        \n",
    "    relWords = dict()\n",
    "    #if method == \"relAll\":\n",
    "            \n",
    "    if method in {\"lr\", \"wsdmImpr\", \"wsdmImprCoorAsc\"}:\n",
    "        origWords_ = [w.replace(\" \", \"_\") for w in origWords]\n",
    "        #relWords_l = []\n",
    "        dumpindex_hist = set()\n",
    "\n",
    "        indriRunQuery_hist = []\n",
    "        indriRunQuery_hist += genCommandsinHist_i(dist, docno, origWords_, \"\", intCoeff0, intCoeff1, intCoeff2, dirCoeff, featureWeights)\n",
    "        #for relW in conceptnet5RelAll[ow].keys():\n",
    "        #    indriRunQuery_hist += genCommandsinHist_i(dist, docno, origWords_, ow, relW, intCoeff0, intCoeff1, intCoeff2, dirCoeff, featureWeights)\n",
    "\n",
    "        indriRunQuery_hist_ = runCommandsinHist_i(cfgTmpOutFileName, indriRunQuery_hist, indriRunQuery_hist_, featureWeights)\n",
    "        #print(\"indriRunQuery_hist =\", indriRunQuery_hist)\n",
    "        for ow in origWords_:\n",
    "            #print(\"ow =\", ow)\n",
    "            #relWords_l += list(conceptnet5RelAll[ow])\n",
    "            #print(ow, \"---\", conceptnet5RelAll[ow])\n",
    "\n",
    "            \n",
    "            \n",
    "            for relW in conceptnet5RelAll[ow].keys():\n",
    "                dumpindex_hist |= genCommandsinHist_d(indriRunQuery_hist_, dumpindex_hist, dist, docno, origWords_, ow, relW, intCoeff0, intCoeff1, intCoeff2, dirCoeff, featureWeights)\n",
    "                #dumpindex_hist |= genCommandsinHist_id(indriRunQuery_hist_, dumpindex_hist, indriRunQuery_phCounter_hist, docno, origWords_, ow, relW, intCoeff0, intCoeff1, intCoeff2, dirCoeff, featureWeights)\n",
    "                #print(\"indriRunQuery_phCounter_hist =\", indriRunQuery_phCounter_hist)\n",
    "                #indriRunQuery_phCounter_hist = [list(x) for x in set(tuple(x) for x in indriRunQuery_phCounter_hist)]\n",
    "            \n",
    "        dumpindex_hist_ = runCommandsinHist_d(dist, docno, dumpindex_hist, dumpindex_hist_)\n",
    "        \n",
    "        #indriRunQuery_hist_ = runCommandsinHist_i(cfgTmpOutFileName, indriRunQuery_phCounter_hist, indriRunQuery_hist_, featureWeights)\n",
    "\n",
    "    return(indriRunQuery_hist_, dumpindex_hist_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "def documentLength(dist, docno, documentName, dumpindex_hist_):\n",
    "    #documentLength_ = int(dumpindex_hist_['dcf'][documentName][0])\n",
    "    documentLength_ = int(dumpindex_hist_['dcf'][dist + \"_\" + docno + \"_\" + documentName][0])\n",
    "    return documentLength_\n",
    "\n",
    "def weightRelConcept(dist, docno, origWords_, ow, relW, intCoeff0, intCoeff1, intCoeff2, dirCoeff, count_history, featureWeights, indriRunQuery_hist_, dumpindex_hist_):\n",
    "    \n",
    "    features = dict()\n",
    "    \n",
    "    expr_i = '_'.join([dist, docno, '_'.join(origWords_), \"\"]).replace(' ', '_')\n",
    "    #print(\"expr_i =\", expr_i)\n",
    "    indriRunQuery_hist_1 = indriRunQuery_hist_[expr_i]\n",
    "    #print(\"indriRunQuery_hist_1 =\", indriRunQuery_hist_1)\n",
    "    topTDocs = [i[0] for i in indriRunQuery_hist_1]\n",
    "    \n",
    "    topTDocScores_d = {i[0]:np.float(i[1]) for i in indriRunQuery_hist_1}\n",
    "    \n",
    "    \n",
    "    expTDocScore = np.float(indriRunQuery_hist_1[0][1])\n",
    "    features[\"expTDocScore\"] = expTDocScore\n",
    "    \n",
    "    expr = \"#4( \" + regex.sub(' ', relW) + \" )\"\n",
    "    #expr = expr.replace(' ', '_')\n",
    "    #print(\"expr =\", expr)\n",
    "    #expr_i = '_'.join([dist, docno, '_'.join([expr.replace(' ', '_')]), \"\"]).replace(' ', '_')\n",
    "    #indriRunQuery_hist_1 = indriRunQuery_hist_[expr_i]\n",
    "    #print(\"indriRunQuery_hist_1 =\", indriRunQuery_hist_1)\n",
    "    relWTopDocuments = dumpindex_hist_['efb'][dist + \"_\" + docno + \"_\" + expr][1:]\n",
    "    #relWTopDocuments = list([i[0] for i in indriRunQuery_hist_1]) #deep copy\n",
    "    \n",
    "    numerator = len(relWTopDocuments) # relW term count in the collection\n",
    "    denumerator = np.sum([documentLength(dist, docno, i, dumpindex_hist_) for i in topTDocs]) # number of terms in topDocs\n",
    "    topTermFrac = numerator / np.float(denumerator)\n",
    "    features[\"topTermFrac\"] = topTermFrac\n",
    "    \n",
    "    numCanDocs = len(set([i[0] for i in relWTopDocuments]))\n",
    "    features[\"numCanDocs\"] = numCanDocs\n",
    "    \n",
    "    if numCanDocs > 0:\n",
    "        avgCDocScore = np.sum([ topTDocScores_d[i] for i in set([i for i in relWTopDocuments])])/np.float(numCanDocs)\n",
    "    else:\n",
    "        avgCDocScore = 0\n",
    "    features[\"avgCDocScore\"] = avgCDocScore\n",
    "\n",
    "    l = [ topTDocScores_d[i] for i in set(relWTopDocuments)]\n",
    "    if len(l)>0:\n",
    "        maxCDocScore = np.max(l)\n",
    "    else:\n",
    "        maxCDocScore = 0\n",
    "    features[\"maxCDocScore\"] = maxCDocScore\n",
    "    \n",
    "    colDocFreq = np.float(dumpindex_hist_['fx'][dist + \"_\" + docno + \"_\" + expr][0])\n",
    "    if colDocFreq>0:\n",
    "        conIdf = np.log(colDocumentCount / colDocFreq)\n",
    "    else:\n",
    "        conIdf = np.log(colDocumentCount / 1)\n",
    "    features[\"conIdf\"] = conIdf\n",
    "    \n",
    "    coocurDocumentsTop = dict()\n",
    "    coocurDocumentsCount = dict()\n",
    "    for ow in origWords_:\n",
    "        expr = \"#uw(#4( \" + regex.sub(' ', ow) + \" ) #4( \" + regex.sub(' ', relW) + \" ))\"\n",
    "        #expr = expr.replace(' ', '_')\n",
    "        #expr_i = '_'.join([dist, docno, '_'.join([expr.replace(' ', '_')]), \"\"]).replace(' ', '_')\n",
    "        #indriRunQuery_hist_1 = indriRunQuery_hist_[expr_i]\n",
    "        coocurDocumentsTop[ow] = dumpindex_hist_['efb'][dist + \"_\" + docno + \"_\" + expr][1:]\n",
    "        #coocurDocumentsTop[ow] = list([i[0] for i in indriRunQuery_hist_1])\n",
    "        coocurDocumentsCount[ow] = np.float(dumpindex_hist_['fx'][dist + \"_\" + docno + \"_\" + expr][0])\n",
    "\n",
    "    avgColCor = np.sum(coocurDocumentsCount.values())/np.float(len(coocurDocumentsCount.values())) \n",
    "    features[\"avgColCor\"] = avgColCor\n",
    "    \n",
    "    maxColCor = np.max(coocurDocumentsCount.values())\n",
    "    features[\"maxColCor\"] = maxColCor\n",
    "    \n",
    "    avgTopColCor = np.sum([len(set(i)) for i in coocurDocumentsTop.values()])/np.float(len(coocurDocumentsTop.values())) \n",
    "    features[\"avgTopColCor\"] = avgTopColCor\n",
    "    \n",
    "    l = [len(set(i)) for i in coocurDocumentsTop.values()]\n",
    "    if len(l)>0:\n",
    "        maxTopColCor = np.max(l)\n",
    "    else:\n",
    "        maxTopColCor = 0\n",
    "    features[\"maxTopColCor\"] = maxTopColCor\n",
    "\n",
    "    coocurDocumentsTop = dict()\n",
    "    coocurDocumentsCount = dict()\n",
    "    for j1, ow1 in enumerate(origWords_):\n",
    "        for j2, ow2 in enumerate(origWords_):\n",
    "            if j1>j2:\n",
    "                expr = \"#uw(#4( \" + regex.sub(' ', ow1) + \" ) #4( \" + regex.sub(' ', ow2) + \" ) #4( \" + regex.sub(' ', relW) + \" ))\"\n",
    "                #expr = expr.replace(' ', '_')\n",
    "                #expr_i = '_'.join([dist, docno, '_'.join([expr.replace(' ', '_')]), \"\"]).replace(' ', '_')\n",
    "                #indriRunQuery_hist_1 = indriRunQuery_hist_[expr_i]\n",
    "                #indriRunQuery_hist_1 = indriRunQuery_hist_['_'.join([dist, docno, expr]).replace(' ', '_')]\n",
    "                coocurDocumentsTop[ow1+ow2] = dumpindex_hist_['efb'][dist + \"_\" + docno + \"_\" + expr][1:]\n",
    "                #coocurDocumentsTop[ow1+ow2] = list([i[0] for i in indriRunQuery_hist_1])\n",
    "                coocurDocumentsCount[ow] = np.float(dumpindex_hist_['fx'][dist + \"_\" + docno + \"_\" + expr][0])\n",
    "\n",
    "    avgColPCor = np.sum(coocurDocumentsCount.values())/np.float(len(coocurDocumentsCount.values()))  \n",
    "    features[\"avgColPCor\"] = avgColPCor\n",
    "\n",
    "    if len(coocurDocumentsCount.values())>0:\n",
    "        maxColPCor = np.max(coocurDocumentsCount.values())\n",
    "    else:\n",
    "        maxColPCor = 0\n",
    "    features[\"maxColPCor\"] = maxColPCor\n",
    "    \n",
    "    avgTopColPCor = np.sum([len(set(i)) for i in coocurDocumentsTop.values()])/np.float(len(coocurDocumentsTop.values())) \n",
    "    features[\"avgTopColPCor\"] = avgTopColPCor\n",
    "\n",
    "    l = [len(set(i)) for i in coocurDocumentsTop.values()] \n",
    "    if len(l)>0:\n",
    "        maxTopColPCor = np.max(l)\n",
    "    else:\n",
    "        maxTopColPCor = 0\n",
    "    features[\"maxTopColPCor\"] = maxTopColPCor\n",
    "    \n",
    "    \"\"\"\n",
    "    relWg = 0\n",
    "    for feature, weight in featureWeights.iteritems():\n",
    "        relWg += features[feature] * weight\n",
    "    \"\"\"\n",
    "    #print(\"origWords_ =\", origWords_)\n",
    "    #print(\"relW =\", relW)\n",
    "    #print(\"features =\", features)\n",
    "    #print(\"relWg =\", relWg)\n",
    "\n",
    "    #return relWg\n",
    "    features = {k:np.abs(np.nan_to_num(v)) for k,v in features.iteritems()}\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def normDictOfDicts(_dict_):\n",
    "    dict_ = dict(_dict_)\n",
    "    dSum = dict()\n",
    "    #dMax = dict()\n",
    "    #dMin = dict()\n",
    "    for k, v in dict_.iteritems():\n",
    "        \"\"\"\n",
    "        for k1, v1 in v.iteritems():\n",
    "            dMax[k1] = max(dMax.get(k1, -(np.inf)), v1)\n",
    "            dMin[k1] = min(dMin.get(k1,  (np.inf)), v1)\n",
    "        \"\"\"\n",
    "        #print (\"type(v) = \", type(v))\n",
    "        dSum = { k1:dSum.get(k1,0)+v1 for k1, v1 in v.items()}\n",
    "        #dMax = { k1:max(dMax.get(k1, -(np.inf)), v1) for k1, v1 in v.items()}\n",
    "        #dMin = { k1:min(dMin.get(k1,  np.inf), v1) for k1, v1 in v.items()}\n",
    "    #print(\"dict_ =\", dict_)\n",
    "    dict_r = dict()\n",
    "    for k, v in dict_.items():\n",
    "        dict_r[k] = { k1:v1/dSum[k1] if dSum[k1] != 0 else 0 for k1, v1 in v.items()}\n",
    "        #dict_[k] = { k1:dict_[k][k1]/dSum[k1] if dSum[k1] != 0 else dict_[k][k1] for k1, v1 in v.items()}\n",
    "        \"\"\"\n",
    "        #dict_r[k] = { k1:(dict_[k][k1]-dMin[k1])/(dMax[k1]-dMin[k1]) if (dMax[k1]-dMin[k1]) != 0 else dict_[k][k1] for k1, v1 in v.items()}\n",
    "        dict_r1 = dict()\n",
    "        for k1, v1 in v.items():\n",
    "            if (dMax[k1]-dMin[k1]) != 0: \n",
    "                dict_r1[k1] = (v1-dMin[k1])/(dMax[k1]-dMin[k1])\n",
    "            elif dict_r1[k1]!=0: \n",
    "                dict_r1[k1] = 1\n",
    "            else: \n",
    "                dict_r1[k1] = 0\n",
    "        dict_r[k] = dict_r1\n",
    "        \"\"\"\n",
    "    return dict_r\n",
    "\n",
    "d_tst = {1:{2:3, 4:5}, 3:{2:3.2, 4:5.4}, 5:{2:3.4, 4:5.5}}\n",
    "print(\"dict of dicts before normalization =\", d_tst)\n",
    "d_tst = normDictOfDicts(d_tst)\n",
    "print(\"dict of dicts after normalization =\", d_tst)\n",
    "\n",
    "def weightedCombineDicts(ddict_, dict_):\n",
    "    dict_cmb = dict()\n",
    "    for k, v in ddict_.items():\n",
    "        cmb = 0\n",
    "        for k1, v1 in v.items():\n",
    "            cmb += dict_[k1] * v1\n",
    "        dict_cmb[k] = cmb\n",
    "    return dict_cmb\n",
    "\n",
    "cmb_tst = { 2:0.4, 4:0.6}\n",
    "print(\"cmb_tst =\", cmb_tst)\n",
    "dCombtst = weightedCombineDicts(d_tst, cmb_tst)\n",
    "print(\"dCombtst =\", dCombtst)\n",
    "\n",
    "# keys of rWords are important\n",
    "# values of rWords1 are important\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "def getRelWords(args, indriRunQuery_hist_, dumpindex_hist_):\n",
    "    #print(\"origWords =\", origWords)\n",
    "    text_string = \"\"\n",
    "    \n",
    "    count_history, rWords, dist, docno, origWords, expansionCount, intCoeff0, intCoeff1, intCoeff2, dirCoeff, featureWeights = args\n",
    "        \n",
    "    relWords = dict()\n",
    "    #if method == \"relAll\":\n",
    "        \n",
    "    if method == \"assocRestful\":\n",
    "        origWords_ = [w.replace(\" \", \"_\") for w in origWords]\n",
    "        url = \"http://conceptnet5.media.mit.edu/data/5.4/assoc/list/en/\"+','.join(origWords_)\n",
    "        response = urllib2.urlopen(url)\n",
    "        data = response.read()\n",
    "        data_j = json.loads(data)\n",
    "        relWords = {str(d[0].encode('utf-8')).replace('/c/en/', '').replace('-', ' '):np.float(d[1]) for d in data_j[u'similar'] if '/c/en/' in str(d[0].encode('utf-8'))}\n",
    "    \n",
    "    elif method in {\"lr\", \"wsdmImpr\", \"wsdmImprCoorAsc\"}:\n",
    "        origWords_ = [w.replace(\" \", \"_\") for w in origWords]\n",
    "        #relWords_l = []\n",
    "\n",
    "        for ow in origWords_:\n",
    "            #relWords_l += list(conceptnet5RelAll[ow])\n",
    "            #print(ow, \"---\", conceptnet5RelAll[ow])\n",
    "            \"\"\"\n",
    "            for relW in conceptnet5RelAll[ow].keys():\n",
    "                relWg = weightRelConcept(dist, docno, origWords_, ow, relW, intCoeff0, intCoeff1, intCoeff2, dirCoeff, count_history, featureWeights, indriRunQuery_hist_, dumpindex_hist_)\n",
    "                if relW in relWords:\n",
    "                    relWords[relW] += relWg\n",
    "                else:\n",
    "                    relWords[relW] = relWg  \n",
    "            \"\"\"\n",
    "            relWords_unNorm = dict()\n",
    "            for relW in conceptnet5RelAll[ow].keys():\n",
    "                feature_ = weightRelConcept(dist, docno, origWords_, ow, relW, intCoeff0, intCoeff1, intCoeff2, dirCoeff, count_history, featureWeights, indriRunQuery_hist_, dumpindex_hist_)\n",
    "                if relW in relWords_unNorm:\n",
    "                    relWords_unNorm[relW] += feature_\n",
    "                else:\n",
    "                    relWords_unNorm[relW] = feature_\n",
    "            #print(\"relWords before =\", relWords_unNorm)\n",
    "            relWordsNorm = normDictOfDicts(relWords_unNorm)\n",
    "            #print(\"relWordsNorm =\", relWordsNorm)\n",
    "            relWords = weightedCombineDicts(relWordsNorm, featureWeights)\n",
    "            #print(\"relWords after comb =\", relWords)\n",
    "            \n",
    "            \n",
    "    else:\n",
    "        count_history = countExpr(count_history, origWords, rWords)\n",
    "        expressions = set()\n",
    "        for ow in origWords:\n",
    "            rWords_ow = rWords.get(ow, [])\n",
    "            expressions.add(\"#4( \" + ow.translate(None, string.punctuation) + \" )\")\n",
    "            if ow.strip() == \"\":\n",
    "                continue\n",
    "            if simMeasure == \"mi\":\n",
    "                N_w = countExpr_get_1(count_history, ow)\n",
    "            for rw_wv in rWords_ow:\n",
    "                (rw_w, rw_v) = rw_wv.items()[0]\n",
    "                if rw_w.strip() == \"\":\n",
    "                    continue           \n",
    "                if simMeasure == \"mi\":\n",
    "                    N_v = countExpr_get_1(count_history, rw_w)\n",
    "                    N_wv = countExpr_get_2(count_history, ow, rw_w)\n",
    "                    rw_v_ = mi_(N_w, N_v, N_wv)\n",
    "                elif simMeasure == \"cnet\":\n",
    "                    rw_v_ = rw_v\n",
    "                relWords[rw_w] = relWords.get(rw_w, 0) + rw_v_\n",
    "    \n",
    "    relWords_sorted = sorted(relWords.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    #print (\"origWords =\", origWords)\n",
    "    #print (\"relWords_sorted =\", relWords_sorted)\n",
    "    \n",
    "    #counter = 0\n",
    "    relWords_sel = []\n",
    "    for counter, (rw_w, rw_v) in enumerate(relWords_sorted):\n",
    "        #print (\"rw_w, rw_v =\", rw_w, rw_v)\n",
    "        #print (\"expansionCount =\", expansionCount)\n",
    "        if (counter >= expansionCount):\n",
    "        #if (rw_v < expansionCount):\n",
    "            break\n",
    "        if  all(c in string.printable for c in rw_w):\n",
    "            if rw_w not in origWords:\n",
    "                rw_w = regex.sub(' ', rw_w)\n",
    "                text_string += rw_w + \" \"\n",
    "                relWords_sel += [rw_w]\n",
    "        #1/0\n",
    "        #counter += 1\n",
    "    #print(\"counter =\", counter)\n",
    "    #rint(\"relWords_sel =\", relWords_sel)\n",
    "#    return(relWords_sel)\n",
    "    return(count_history, text_string, relWords_sel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getAllRelWords_train(dist, count_history, rWords, origWordsAll, expansionCount1, N1, expansionCount2, intCoeff0, intCoeff1, intCoeff2, dirCoeff, featureWeights, indriRunQuery_hist_, dumpindex_hist_):\n",
    "    relText_stringAll = []\n",
    "    relText_string = dict()\n",
    "    relWords_sel = dict()\n",
    "    for c1, (docno, origWords) in enumerate(origWordsAll.iteritems()):\n",
    "        print(\"c1 =\", c1, \"docno =\", docno, \"--- origWords =\", origWords)\n",
    "        args = (count_history, rWords, dist, docno, origWords, expansionCount1, intCoeff0, intCoeff1, intCoeff2, dirCoeff, featureWeights)\n",
    "        indriRunQuery_hist_, dumpindex_hist_ = getRelWords_train(args, indriRunQuery_hist_, dumpindex_hist_)\n",
    "        #print(\"c1 =\", c1, \"- len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) =\", len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())))\n",
    "    \n",
    "    #print(\"relText_stringAll =\", relText_stringAll)\n",
    "    return (indriRunQuery_hist_, dumpindex_hist_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getAllRelWords(dist, count_history, rWords, origWordsAll, expansionCount1, N1, expansionCount2, intCoeff0, intCoeff1, intCoeff2, dirCoeff, featureWeights, indriRunQuery_hist_, dumpindex_hist_):\n",
    "    relText_stringAll = []\n",
    "    relText_string = dict()\n",
    "    relWords_sel = dict()\n",
    "    for c1, (docno, origWords) in enumerate(origWordsAll.iteritems()):\n",
    "        #print(str(c1), \"---\", docno, \"---\", origWords)\n",
    "        args = (count_history, rWords, dist, docno, origWords, expansionCount1, intCoeff0, intCoeff1, intCoeff2, dirCoeff, featureWeights)\n",
    "        count_history, relText_string_, relWords_sel_ = getRelWords(args, indriRunQuery_hist_, dumpindex_hist_)\n",
    "        #print(\"len(indriRunQuery_hist_), len(dumpindex_hist_['dcf']), len(dumpindex_hist_['ef']) =\", len(indriRunQuery_hist_), len(dumpindex_hist_['dcf']), len(dumpindex_hist_['ef']))\n",
    "        relText_string[docno] = relText_string_\n",
    "        relWords_sel[docno] = relWords_sel_[0:N1]\n",
    "        #print(docno, \"---\", origWords, \"---\", relWords_sel_)\n",
    "    relText_stringAll += [relText_string]\n",
    "    \"\"\"\n",
    "    relText_string = dict()\n",
    "    relWords_sel = dict()\n",
    "    for docno, origWords_new in relWords_sel.iteritems():\n",
    "        args = (count_history, rWords, docno, origWords, expansionCount1, intCoeff0, intCoeff1, intCoeff2, dirCoeff, featureWeights)\n",
    "        count_history, relText_string_, relWords_sel_ = getRelWords(args, indriRunQuery_hist_, dumpindex_hist_)\n",
    "        relText_string[docno] = relText_string_\n",
    "        relWords_sel[docno] = relWords_sel_\n",
    "    relText_stringAll += [relText_string]\n",
    "    \"\"\"\n",
    "    #print(\"relText_stringAll =\", relText_stringAll)\n",
    "    return (count_history, relText_stringAll, relWords_sel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "def genQueries(cfgOutFileName_, origWordsAll, relText_stringAll, intCoeff0, intCoeff1, intCoeff2, dirCoeff):\n",
    "    #    #print(\"intCoeff =\", intCoeff)\n",
    "    with open(cfgInFileName, 'r') as inFile:\n",
    "        reader = inFile.read()\n",
    "        soupNew = BeautifulSoup(\"\", 'lxml')\n",
    "        parameters_tag = soupNew.new_tag(\"parameters\")\n",
    "        index_tag = soupNew.new_tag(\"index\")\n",
    "        index_tag.string = colIndexDir\n",
    "        parameters_tag.append(index_tag)\n",
    "            #print(tags)\n",
    "\n",
    "        for docno, origWords in origWordsAll.iteritems():\n",
    "            doc_tag = soupNew.new_tag(\"query\")\n",
    "\n",
    "            docno_tag = soupNew.new_tag(\"number\")\n",
    "            docno_tag.string = docno\n",
    "            doc_tag.append(docno_tag)\n",
    "\n",
    "            text_tag = soupNew.new_tag(\"text\")\n",
    "            text_tag.string = \"#weight(\\n\" \n",
    "                    \n",
    "            text_tag.string += str(intCoeff0) + \" #combine(\" \n",
    "            for ow in set(origWords):\n",
    "                ow = regex.sub(' ', ow)\n",
    "                text_tag.string += ow + \" \"\n",
    "            text_tag.string += \")\\n\"\n",
    "\n",
    "            if len(relText_stringAll[0])>0:\n",
    "                if len(relText_stringAll[0][docno]) > 3: \n",
    "                    relText_string1 = regex.sub(' ', relText_stringAll[0][docno])\n",
    "                    text_tag.string += str(intCoeff1) + \" #combine(\" \n",
    "                    #print(\"relText_string1 =\", relText_string1)\n",
    "                    text_tag.string += relText_string1.encode('utf-8')\n",
    "                    text_tag.string += \")\\n\"\n",
    "            \n",
    "            \"\"\"\n",
    "            #print(len(relText_stringAll[1]))\n",
    "            if len(relText_stringAll[1])>0:\n",
    "                if len(relText_stringAll[1][docno]) > 3: \n",
    "                    relText_string2 = regex.sub(' ', relText_stringAll[1][docno])\n",
    "                    text_tag.string += str(intCoeff2) + \" #combine(\" \n",
    "                    #print(\"relText_string2 =\", relText_string2)\n",
    "                    text_tag.string += relText_string2.encode('utf-8')\n",
    "                    text_tag.string += \")\\n\"\n",
    "            \"\"\"\n",
    "            #print(\"origWords =\", origWords)\n",
    "            #print(\"relWords_sel1 =\", relWords_sel1)\n",
    "            #print(\"relWords_sel2 =\", relWords_sel2)\n",
    "            \n",
    "            \n",
    "            text_tag.string += \") \"\n",
    "\n",
    "            doc_tag.append(text_tag)\n",
    "\n",
    "            parameters_tag.append(doc_tag)\n",
    "            #print(doc_tag)\n",
    "\n",
    "        rule_tag = soupNew.new_tag(\"rule\")\n",
    "        rule_tag.string = \"method:dir,mu:\" + str(dirCoeff)\n",
    "        #rule_tag.string = \"method:two\"\n",
    "        parameters_tag.append(rule_tag)\n",
    "\n",
    "        #intCoeff_tag = soupNew.new_tag(\"intCoeff\")\n",
    "        #intCoeff_tag.string = \"0.8\"\n",
    "        #parameters_tag.append(intCoeff_tag)\n",
    "\n",
    "        threads_tag = soupNew.new_tag(\"threads\")\n",
    "        threads_tag.string = \"32\"\n",
    "        parameters_tag.append(threads_tag)\n",
    "\n",
    "        count_tag = soupNew.new_tag(\"count\")\n",
    "        count_tag.string = \"1000\"\n",
    "        parameters_tag.append(count_tag)\n",
    "\n",
    "        trecFormat_tag = soupNew.new_tag(\"trecFormat\")\n",
    "        trecFormat_tag.string = \"true\"\n",
    "        parameters_tag.append(trecFormat_tag)\n",
    "\n",
    "        soupNew.append(parameters_tag)\n",
    "        #print(soupNew.prettify())\n",
    "    #print(\"outFileName =\", outFileName)\n",
    "    with open( cfgOutFileName_, 'w') as outFile:\n",
    "        soupNewStr = str(soupNew)\n",
    "        soupNewStr = soupNewStr.replace(\"</text>\", \"\\n</text>\\n\").replace(\"query>\", \"query>\\n\").replace(\"<text>\", \"\\n<text>\\n\").replace(\"</index>\", \"</index>\\n\").replace(\"\\n<index>\", \"<index>\")\n",
    "\n",
    "        outFile.write(soupNewStr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def precisionCompute_train(dist, origWordsAll, count_history, intCoeffs0, intCoeffs1, intCoeffs2, expansionCounts1, expansionCounts2, dirCoeffs, N1s, featureWeights_, indriRunQuery_hist_, dumpindex_hist_):\n",
    "    #randNum = random.randint(1,1e9)\n",
    "    indriRunQuery_hist_, dumpindex_hist_ = getAllRelWords_train(dist, count_history, rWords, origWordsAll, expansionCounts1[0], N1s[0], expansionCounts2[0], intCoeffs0[0], intCoeffs1[0], intCoeffs2[0], dirCoeffs[0], featureWeights_[0], indriRunQuery_hist_, dumpindex_hist_)\n",
    "                                    \n",
    "    return count_history, indriRunQuery_hist_, dumpindex_hist_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#training\n",
    "dist = str(1)\n",
    "intCoeffs0 = [ 0.7 ]\n",
    "intCoeffs1 = [ 0.3 ]\n",
    "#intCoeffs2 = [ 0.18 ]\n",
    "intCoeffs2 = [ 0.5 ]\n",
    "#intCoeffs1 = np.arange(0.1, 1, 0.1)\n",
    "expansionCounts1 = [ 85 ]\n",
    "#expansionCounts2 = [ 145 ]\n",
    "expansionCounts2 = [ 0 ]\n",
    "#expansionCounts2 = range(15, 220, 5)\n",
    "dirCoeffs = [ 1600 ]\n",
    "#dirCoeffs = range(200, 4000, 200)\n",
    "N1s = [0]\n",
    "rWords = dict()\n",
    "featureWeights_1 = {'maxTopColCor': 0.1, 'expTDocScore': 0.1, 'maxColPCor': 0.1, 'avgCDocScore': 0.1, 'topTermFrac': 0.1, 'avgColCor': 0.1, 'avgTopColCor': 0.1, 'numCanDocs': 0.1, 'maxTopColPCor': 0.1, 'maxCDocScore': 0.1, 'avgColPCor': 0.1, 'conIdf': 0.1, 'avgTopColPCor': 0.1, 'maxColCor': 0.1}\n",
    "featureWeights_l=[featureWeights_1]\n",
    "origWordsAll = analyseQueries()\n",
    "\n",
    "count_history, indriRunQuery_hist_, dumpindex_hist_ = precisionCompute_train(dist, origWordsAll, count_history, intCoeffs0, intCoeffs1, intCoeffs2, expansionCounts1, expansionCounts2, dirCoeffs, N1s, featureWeights_l, indriRunQuery_hist_, dumpindex_hist_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def precisionCompute(dist, origWordsAll, count_history, intCoeffs0, intCoeffs1, intCoeffs2, expansionCounts1, expansionCounts2, dirCoeffs, N1s, featureWeights_, indriRunQuery_hist_, dumpindex_hist_):\n",
    "    mapPrecs = []\n",
    "    for featureWeights in featureWeights_:\n",
    "        for N1 in N1s:\n",
    "            for intCoeff0 in intCoeffs0:\n",
    "                for intCoeff1 in intCoeffs1:\n",
    "                    for intCoeff2 in intCoeffs2:\n",
    "                        for expansionCount1 in expansionCounts1:\n",
    "                            for expansionCount2 in expansionCounts2:\n",
    "                                for dirCoeff in dirCoeffs:\n",
    "                                    \n",
    "                                    \n",
    "                                    #res = getAllRelWords_p(count_history, rWords, origWordsAll, expansionCount1, N1, expansionCount2, intCoeff0, intCoeff1, intCoeff2, dirCoeff, featureWeights)\n",
    "                                    \n",
    "                                    count_history, relText_stringAll, relWords_sel = getAllRelWords(dist, count_history, rWords, origWordsAll, expansionCount1, N1, expansionCount2, intCoeff0, intCoeff1, intCoeff2, dirCoeff, featureWeights, indriRunQuery_hist_, dumpindex_hist_)\n",
    "                                    \n",
    "                                    #print(\"len(relText_stringAll[1]) =\", len(relText_stringAll[1]))\n",
    "                                    ##########\n",
    "                                    #cfgOutFileName = os.path.join(\"tmp\",\"sbsb\",str(randNum)+\".cfg\")\n",
    "                                    #runsFileName = os.path.join(\"tmp\",\"sbsb\",str(randNum)+\".run\")\n",
    "                                    #evalsFileName = os.path.join(\"tmp\",\"sbsb\",str(randNum)+\".eval\")\n",
    "                                    ##########\n",
    "                                    genQueries(cfgOutFileName, origWordsAll, relText_stringAll, intCoeff0, intCoeff1, intCoeff2, dirCoeff)\n",
    "                                    subprocess.Popen(\"IndriRunQuery \" + cfgOutFileName + \" > \" + runsFileName, shell=True, stdout=subprocess.PIPE).stdout.read()\n",
    "                                    cmd = \"trec_eval -q \" + colQrelsFileName + \" \" + runsFileName + \" > \" + evalsFileName\n",
    "                                    subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE).stdout.read()\n",
    "                                    cmd = \"cat \" + evalsFileName + \" | grep map | grep all | grep -v gm | awk '{print $3}' \"\n",
    "                                    #print (\"cmd = \", cmd)\n",
    "                                    mapPrec = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE).stdout.read()\n",
    "                                    mapPrecs += [np.float(mapPrec)]\n",
    "                                    print(\"featureWeights =\", featureWeights)\n",
    "                                    print(\"intCoeff0, intCoeff1, intCoeff2, expansionCount1, expansionCount2, dirCoeff, map precision, N1 =\", intCoeff0, intCoeff1, intCoeff2, expansionCount1, expansionCount2, dirCoeff, N1, mapPrec)\n",
    "                                    #mapPrecs[intCoeff] = mapPrec\n",
    "    return relText_stringAll, relWords_sel, mapPrecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dist = str(1)\n",
    "intCoeffs0 = [ 0.7 ]\n",
    "intCoeffs1 = [ 0.5 ]\n",
    "#intCoeffs2 = [ 0.18 ]\n",
    "intCoeffs2 = [ 0.5 ]\n",
    "#intCoeffs1 = np.arange(0.1, 1, 0.1)\n",
    "#expansionCounts1 = [ 0.00050 ]\n",
    "expansionCounts1 = [ 20 ]\n",
    "#expansionCounts2 = [ 145 ]\n",
    "expansionCounts2 = [ 0 ]\n",
    "expansionCounts1 = range(0, 220, 5)\n",
    "dirCoeffs = [ 1600 ]\n",
    "#dirCoeffs = range(200, 4000, 200)\n",
    "N1s_max = [100]\n",
    "#N1s = range(1, 30, 1)\n",
    "#mapPrecs = \n",
    "rWords = dict()\n",
    "maxIteration = 1000\n",
    "#featureWeights_l=[]\n",
    "#featureWeights_1 = {'maxTopColCor': 0.1, 'expTDocScore': 0.1, 'maxColPCor': 0.1, 'avgCDocScore': 0.1, 'topTermFrac': 0.1, 'avgColCor': 0.1, 'avgTopColCor': 0.1, 'numCanDocs': 0.1, 'maxTopColPCor': 0.1, 'maxCDocScore': 0.1, 'avgColPCor': 0.1, 'conIdf': 0.1, 'avgTopColPCor': 0.1, 'maxColCor': 0.1}\n",
    "\"\"\"\n",
    "for conIdf in [0.1] :\n",
    "#for conIdf in [-5000, -500, -50, -5, -0.5, -0.05, 0, 0.05, 0.5, 5, 50, 500, 5000] :\n",
    "#for conIdf in np.arange(-1,1,0.2):\n",
    "    featureWeights_1[\"conIdf\"] = conIdf\n",
    "    featureWeights_l += [dict(featureWeights_1)]\n",
    "relText_stringAll, relWords_sel1, mapPrecs = precisionCompute(dist, origWordsAll, count_history, intCoeffs0, intCoeffs1, intCoeffs2, expansionCounts1, expansionCounts2, dirCoeffs, N1s_max, featureWeights_l, indriRunQuery_hist_, dumpindex_hist_)\n",
    "\"\"\"\n",
    "#for ft, wg in featureWeights_1.iteritems():\n",
    "# initialize:\n",
    "#featureWeights_1_new = { k:0.0 for k in featureWeights_1.keys()}\n",
    "#featureWeights_1_new = {'maxTopColCor': 0.0, 'maxColPCor': 1.0, 'expTDocScore': 0.0, 'topTermFrac':1.0, 'maxCDocScore' : -1.0, 'avgColCor': 0.0, 'avgTopColCor': -1.0, 'numCanDocs': 0.0, 'maxTopColPCor': 0.0, 'avgCDocScore': 1.0, 'avgColPCor': 1.0, 'conIdf': 0.0, 'avgTopColPCor': 1.0, 'maxColCor': -1.0}\n",
    "featureWeights_1_new = {'maxColPCor': 0.59999999999999998, 'avgCDocScore': 0.59999999999999998, 'avgTopColCor': -0.79999999999999993, 'numCanDocs': 0.0, 'expTDocScore': 0.0, 'avgColPCor': 0.59999999999999998, 'maxCDocScore': -1.6000000000000001, 'avgTopColPCor': 1.0, 'maxTopColCor': 0.0, 'topTermFrac': 1.0, 'avgColCor': 0.0, 'maxTopColPCor': 0.0, 'conIdf': -0.40000000000000002, 'maxColCor': -1.2}\n",
    "# direction:\n",
    "#featureWeights_1_dir = {'maxTopColCor': 0.0, 'maxColPCor': 1.0, 'expTDocScore': 0.0, 'topTermFrac':1.0, 'maxCDocScore' : -1.0, 'avgColCor': 0.0, 'avgTopColCor': -1.0, 'numCanDocs': 0.0, 'maxTopColPCor': 0.0, 'avgCDocScore': 1.0, 'avgColPCor': 1.0, 'conIdf': 0.0, 'avgTopColPCor': 1.0, 'maxColCor': -1.0}\n",
    "#featureWeights_1_new = {'maxTopColCor': 0.0, 'expTDocScore': 0.0, 'maxColPCor': 0.0, 'avgCDocScore': 0.0, 'topTermFrac': 0.0, 'avgColCor': 0.0, 'avgTopColCor': 0.0, 'numCanDocs': 0.0, 'maxTopColPCor': 0.0, 'maxCDocScore': 0.0, 'avgColPCor': 0.0, 'conIdf': 0.1, 'avgTopColPCor': 0.1, 'maxColCor': 0.1}\n",
    "#for wg_new in np.arange(-1.0,1.1,1.0):\n",
    "featureWeights_l = [dict(featureWeights_1_new)]\n",
    "relText_stringAll, relWords_sel1, mapPrecs = precisionCompute(dist, origWordsAll, count_history, intCoeffs0, intCoeffs1, intCoeffs2, expansionCounts1, expansionCounts2, dirCoeffs, N1s_max, featureWeights_l, indriRunQuery_hist_, dumpindex_hist_)\n",
    "\"\"\"\n",
    "for iter1 in range(0, maxIteration,1):\n",
    "    for ft, wg in featureWeights_1_new.iteritems():\n",
    "        # initialize:\n",
    "        #featureWeights_1_new = { k:0.0 for k in featureWeights_1.keys()}\n",
    "        #featureWeights_1_new = {'maxTopColCor': 0.0, 'expTDocScore': 0.0, 'maxColPCor': 0.0, 'avgCDocScore': 0.0, 'topTermFrac': 0.0, 'avgColCor': 0.0, 'avgTopColCor': 0.0, 'numCanDocs': 0.0, 'maxTopColPCor': 0.0, 'maxCDocScore': 0.0, 'avgColPCor': 0.0, 'conIdf': 0.1, 'avgTopColPCor': 0.1, 'maxColCor': 0.1}\n",
    "        #if featureWeights_1_dir[ft] == 0:\n",
    "        np_arange = wg + np.arange(-0.4, 0.41, 0.2)\n",
    "        #elif featureWeights_1_dir[ft] > 0:\n",
    "        #    np_arange = wg + np.arange(0.0,2.1,0.4)\n",
    "        #elif featureWeights_1_dir[ft] < 0:\n",
    "        #    np_arange = wg + np.arange(-2.0,0.1,0.4)\n",
    "        mapPrecs = dict()\n",
    "        for wg_new in np_arange:\n",
    "            print(\"feature =\", ft, \" orig weight =\", wg, \" new weight =\", wg_new)\n",
    "            #featureWeights_l=[]\n",
    "            featureWeights_1_new[ft] = wg_new\n",
    "            featureWeights_l = [dict(featureWeights_1_new)]\n",
    "            relText_stringAll, relWords_sel1, mapPrec_ = precisionCompute(dist, origWordsAll, count_history, intCoeffs0, intCoeffs1, intCoeffs2, expansionCounts1, expansionCounts2, dirCoeffs, N1s_max, featureWeights_l, indriRunQuery_hist_, dumpindex_hist_)\n",
    "            mapPrecs[wg_new] = mapPrec_\n",
    "        wg_max = max(mapPrecs.iteritems(), key=operator.itemgetter(1))[0]\n",
    "        #print (\"mapPrecs =\", mapPrecs)\n",
    "        mapPrecs = {k:v[0] for k,v in mapPrecs.iteritems()}\n",
    "        print (\"mapPrecs =\", mapPrecs)\n",
    "        featureWeights_1_new[ft] = wg_max\n",
    "        print (\"featureWeights_1_new =\", featureWeights_1_new)\n",
    "        print (\"iter1 =\", iter1, \" ft =\", ft, \" wg_max =\", wg_max, \" prec_max =\", mapPrecs[wg_max])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "dist = str(2)\n",
    "intCoeffs0 = [ 0.7 ]\n",
    "intCoeffs1 = [ 0.5 ]\n",
    "#intCoeffs2 = [ 0.18 ]\n",
    "intCoeffs2 = [ 0.5 ]\n",
    "#intCoeffs1 = np.arange(0.1, 1, 0.1)\n",
    "expansionCounts1 = [ 10 ]\n",
    "#expansionCounts2 = [ 145 ]\n",
    "expansionCounts2 = [ 0 ]\n",
    "#expansionCounts2 = range(15, 220, 5)\n",
    "dirCoeffs = [ 1600 ]\n",
    "#dirCoeffs = range(200, 4000, 200)\n",
    "N1s = 2\n",
    "N2s_max = [100]\n",
    "rWords = dict()\n",
    "featureWeights_1 = {'maxTopColCor': 0.1, 'expTDocScore': 0.1, 'maxColPCor': 0.1, 'avgCDocScore': 0.1, 'topTermFrac': 0.1, 'avgColCor': 0.1, 'avgTopColCor': 0.1, 'numCanDocs': 0.1, 'maxTopColPCor': 0.1, 'maxCDocScore': 0.1, 'avgColPCor': 0.1, 'conIdf': 0.1, 'avgTopColPCor': 0.1, 'maxColCor': 0.1}\n",
    "featureWeights_l=[featureWeights_1]\n",
    "relWords_sel1 = {k:v[0:N1s] for k,v in relWords_sel.iteritems()}\n",
    "indriRunQuery_hist_, dumpindex_hist_ = precisionCompute_train(dist, relWords_sel1, count_history, intCoeffs0, intCoeffs1, intCoeffs2, expansionCounts1, expansionCounts2, dirCoeffs, N2s_max, featureWeights_l, indriRunQuery_hist_, dumpindex_hist_)\n",
    "\"\"\"\n",
    "#training\n",
    "dist = str(1)\n",
    "intCoeffs0 = [ 0.7 ]\n",
    "intCoeffs1 = [ 0.3 ]\n",
    "#intCoeffs2 = [ 0.18 ]\n",
    "intCoeffs2 = [ 0.5 ]\n",
    "#intCoeffs1 = np.arange(0.1, 1, 0.1)\n",
    "expansionCounts1 = [ 85 ]\n",
    "#expansionCounts2 = [ 145 ]\n",
    "expansionCounts2 = [ 0 ]\n",
    "#expansionCounts2 = range(15, 220, 5)\n",
    "dirCoeffs = [ 1600 ]\n",
    "#dirCoeffs = range(200, 4000, 200)\n",
    "N2s_max = [0]\n",
    "N1s = 2\n",
    "rWords = dict()\n",
    "featureWeights_1 = {'maxTopColCor': 0.1, 'expTDocScore': 0.1, 'maxColPCor': 0.1, 'avgCDocScore': 0.1, 'topTermFrac': 0.1, 'avgColCor': 0.1, 'avgTopColCor': 0.1, 'numCanDocs': 0.1, 'maxTopColPCor': 0.1, 'maxCDocScore': 0.1, 'avgColPCor': 0.1, 'conIdf': 0.1, 'avgTopColPCor': 0.1, 'maxColCor': 0.1}\n",
    "featureWeights_l=[featureWeights_1]\n",
    "relWords_sel1 = {k:v[0:N1s] for k,v in relWords_sel1.iteritems()}\n",
    "count_history, indriRunQuery_hist_, dumpindex_hist_ = precisionCompute_train(dist, origWordsAll, count_history, intCoeffs0, intCoeffs1, intCoeffs2, expansionCounts1, expansionCounts2, dirCoeffs, N2s_max, featureWeights_l, indriRunQuery_hist_, dumpindex_hist_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(dist)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
