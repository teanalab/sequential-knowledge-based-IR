{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import sys\n",
    "import json\n",
    "import os\n",
    "import subprocess\n",
    "import re\n",
    "import string\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib2\n",
    "import csv\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "annotateProg = /home/fj9124/projects/ir/seq_kb_ir/optParams/annotate.py\n",
      "colInConfigsFile = /home/fj9124/projects/ir/seq_kb_ir/configs/gov2/06.topics.701-850.txt\n",
      "colOutConfigsFile = /home/fj9124/projects/ir/seq_kb_ir/configs/gov2/indriRunQuery.cfg\n",
      "collection index Dir = /scratch/index/indri_5_7/gov2\n",
      "colRunsFile = /home/fj9124/projects/ir/seq_kb_ir/runs/gov2/indriRunQuery.run\n",
      "colQrelsFile = /home/fj9124/projects/ir/seq_kb_ir/qrels/gov2/qrels.tb06.top50\n",
      "colEvalsFile = /home/fj9124/projects/ir/seq_kb_ir/evals/gov2/indriRunQuery.evals\n"
     ]
    }
   ],
   "source": [
    "collection = \"gov2\"\n",
    "\n",
    "projectDir = \"/home/fj9124/projects/ir/seq_kb_ir/\"\n",
    "optParamsDir = os.path.join(projectDir, \"optParams\")\n",
    "annotateProg = os.path.join(optParamsDir, \"annotate.py\")\n",
    "print(\"annotateProg =\", annotateProg)\n",
    "\n",
    "configsDir = os.path.join(projectDir, \"configs\")\n",
    "colConfigsDir = os.path.join(configsDir, collection)\n",
    "if collection == \"gov2\":\n",
    "    colInConfigsFile = os.path.join(colConfigsDir, \"06.topics.701-850.txt\")\n",
    "print (\"colInConfigsFile =\", colInConfigsFile)\n",
    "colOutConfigsFile = os.path.join(colConfigsDir, \"indriRunQuery.cfg\")\n",
    "print (\"colOutConfigsFile =\", colOutConfigsFile)\n",
    "\n",
    "indexDir = \"/scratch/index/indri_5_7/\"\n",
    "colIndexDir = os.path.join(indexDir, collection) \n",
    "print(\"collection index Dir =\", colIndexDir)\n",
    "\n",
    "colRunsFile = os.path.join(projectDir, \"runs\", collection, \"indriRunQuery.run\")\n",
    "print(\"colRunsFile =\", colRunsFile)\n",
    "\n",
    "if collection == \"gov2\":\n",
    "    colQrelsFile = os.path.join(projectDir, \"qrels\", collection, \"qrels.tb06.top50\")\n",
    "print(\"colQrelsFile =\", colQrelsFile)\n",
    "\n",
    "colEvalsFile = os.path.join(projectDir, \"evals\", collection, \"indriRunQuery.evals\")\n",
    "print(\"colEvalsFile =\", colEvalsFile)\n",
    "\n",
    "stopwordsFile = \"/backup/ir/conf/stopwords.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set(['lest', 'all', 'just', 'less', 'being', 'indeed', 'over', 'anyway', 'hath', 'own', 'through', 'wherewith', 'hereto', 'whereinto', 'where', 'still', 'thyself', 'its', 'before', 'one', 'whereabouts', 'whereon', 'yourselves', 'halves', 'selves', 'herself', 'whoever', 'had', 'behind', 'hereabouts', 'should', 'meantime', 'to', 'must', 'choose', 'whatsoever', 'under', 'inwards', 'of', 'has', 'might', 'thereafter', 'latterly', 'outside', 'do', 'hindmost', 'his', 'somebody', 'around', 'thereby', 'get', 'very', 'thee', 'she', 'none', 'underneath', 'whichever', 'cannot', 'every', 'whatever', 'they', 'front', 'during', 'thus', 'now', 'him', 'nor', 'like', 'well', 'several', 'hereafter', 'whilst', 'did', 'ever', 'seldom', 'whither', 'via', 'this', 'someone', 'either', 'though', 'always', 'become', 'thereupon', 'sometime', 'whomever', 'upward', 'therein', 'canst', 'because', 'often', 'our', 'doing', 'howsoever', 'yourself', 'some', 'somehow', 'up', 'namely', 'towards', 'are', 'notwithstanding', 'et', 'beyond', 'ourselves', 'whosoever', 'out', 'even', 'what', 'vs', 'for', 'whoa', 'wheresoever', 'furthermore', 'since', 'while', 'per', 'whole', 'henceforth', 'everything', 'enough', 'does', 'above', 'between', 'it', 'neither', 'thereabouts', 'forth', 'across', 'ought', 'be', 'we', 'never', 'whose', 'each', 'however', 'here', 'quite', 'whereof', 'how', 'were', 'worse', 'let', 'nowhere', 'although', 'others', 'alone', 'along', 'by', 'both', 'about', 'last', 'would', 'anything', 'thou', 'many', 'could', 'thence', 'wherever', 'according', 'against', 'etc', 'whence', 'became', 'whereto', 'inward', 'hence', 'whereupon', 'who', 'onto', 'or', 'whereafter', 'otherwise', 'contrariwise', 'among', 'hither', 'already', 'afterwards', 'thereon', 'into', 'within', 'thereabout', 'thereof', 'hitherto', 'down', 'hast', 'everyone', 'done', 'another', 'wherefore', 'moreover', 'throughout', 'nowadays', 'anyhow', 'whereunto', 'everybody', \"doesn't\", 'from', 'nope', 'her', 'whom', 'whichsoever', 'there', 'only', 'been', 'next', 'anyone', 'their', 'inasmuch', 'much', 'therefore', 'worst', 'forward', 'themselves', 'thru', 'hardly', 'thrice', 'more', 'till', 'himself', 'elsewhere', 'mostly', 'on', 'with', 'am', 'becoming', 'hereby', 'amongst', 'else', 'everywhere', 'too', 'somewhat', 'ours', 'ok', 'hers', 'than', 'those', 'he', 'me', 'albeit', 'myself', 'whensoever', 'thenceforth', 'unlike', 'these', 'was', 'inside', 'yet', 'thereto', 'us', 'until', 'besides', 'nevertheless', 'below', 'anywhere', 'can', 'ms', 'mr', 'your', 'toward', 'my', 'something', 'and', 'sometimes', 'whenever', 'want', 'then', 'almost', 'certain', 'is', 'in', 'beforehand', 'herein', 'an', 'as', 'itself', 'at', 'have', 'further', 'need', 'ie', 'any', 'wherefrom', 'if', 'again', 'no', 'not', 'latter', 'meanwhile', 'when', 'whereat', 'same', 'wherein', 'beside', 'also', 'that', 'other', 'whew', 'which', 'becomes', 'instead', 'you', 'really', 'week', 'nobody', 'unless', 'somewhere', 'may', 'after', 'upon', 'them', 'insomuch', 'most', 'hereupon', 'whether', 'but', 'nothing', 'such', 'yours', 'why', 'upwards', 'a', 'furthest', 'off', 'whereby', 'thy', 'i', 'maybe', 'nonetheless', 'rather', 'anybody', 'together', 'mrs', 'perhaps', 'without', 'so', 'whomsoever', 'the', 'whereas', 'once'])\n"
     ]
    }
   ],
   "source": [
    "with open(stopwordsFile, 'r') as f:\n",
    "    stopwords = list(csv.reader(f, delimiter='\\n'))\n",
    "    stopwords = {s[0] for s in stopwords}\n",
    "    print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conceptNetText(expr):\n",
    "    if expr == \"\":\n",
    "        return(\"\")\n",
    "    expr = expr.replace(' ', '_')\n",
    "    textUrl = \"http://conceptnet5.media.mit.edu/data/5.4/uri?language=en&text=\" + expr\n",
    "    #print(\"textUrl =\", textUrl)\n",
    "    html_doc = urllib2.urlopen(textUrl)\n",
    "    soup = BeautifulSoup(html_doc, \"lxml\")\n",
    "    jsonContent = soup.find('body').find('p').contents[0]\n",
    "    jsonContent = json.loads(jsonContent)\n",
    "    conceptText = jsonContent[u'uri']\n",
    "    \n",
    "    conceptTextB = str(os.path.basename(conceptText))#.replace('_', ' ')\n",
    "    conceptTextD = str(os.path.dirname(conceptText))\n",
    "    if conceptTextD == \"/c/en\":\n",
    "    #if numFound != 0:\n",
    "        return(conceptTextB)\n",
    "    else:\n",
    "        return(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conceptNetLookup(conceptText):\n",
    "    lookupUrl = \"http://conceptnet5.media.mit.edu/data/5.4\" + conceptText\n",
    "    #print(\"lookupUrl =\", lookupUrl)\n",
    "    html_doc = urllib2.urlopen(lookupUrl)\n",
    "    soup = BeautifulSoup(html_doc, \"lxml\")\n",
    "    #print(soup.prettify())\n",
    "    jsonContent = soup.find('body').find('p').contents[0]\n",
    "    jsonContent = json.loads(jsonContent)\n",
    "    numFound = int(jsonContent[u'numFound'])\n",
    "    if numFound != 0:\n",
    "        return(conceptText)\n",
    "    else:\n",
    "        return(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def genBigrams(unigrams):\n",
    "    a = copy.deepcopy(unigrams)\n",
    "    b = copy.deepcopy(unigrams)\n",
    "    del a[0]\n",
    "    del b[-1]\n",
    "    s = [b[i] + \" \" + j for i, j in enumerate(a) if j not in stopwords and b[i] not in stopwords ]\n",
    "    return(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def entityRecognition(text):\n",
    "    # check if a derivation of expression exists in conceptnet and what is it? \n",
    "    concepts = []\n",
    "    #origText = \"Relevant documents will include those on historical exploration and drilling as well as history of regulatory bodies. Relevant are history of the oil industry in various states, even if drilling began in 1950 or later\"\n",
    "    #unigrams = origText.split()\n",
    "    unigrams = text.split()\n",
    "    bigrams = genBigrams(unigrams)\n",
    "    #print (\"bigrams =\", bigrams)\n",
    "    unigramConcepts = set()\n",
    "    for unigram in unigrams:\n",
    "        if unigram in stopwords:\n",
    "            continue\n",
    "        #print(\"unigram =\", unigram)\n",
    "        uri = \"/c/en/\" + conceptNetText(conceptNetText(unigram))\n",
    "        unigramConcept = conceptNetLookup(uri).replace(\"/c/en/\", \"\")\n",
    "        #print (\"unigramConcept =\", unigramConcept)\n",
    "        if unigramConcept != \"\":\n",
    "            unigramConcepts.add(unigramConcept)\n",
    "    #print(\"unigramConcepts =\", unigramConcepts)\n",
    "    bigramConcepts = set()\n",
    "    for bigram in bigrams:\n",
    "        bigram = bigram.replace(' ', '_')\n",
    "        uri = \"/c/en/\" + conceptNetText(conceptNetText(bigram))\n",
    "        bigramConcept = conceptNetLookup(uri).replace(\"/c/en/\", \"\")\n",
    "        #print(\"bigramConcept =\", bigramConcept)\n",
    "        if bigramConcept != \"\":\n",
    "            bigramConcepts.add(bigramConcept)\n",
    "    #print(\"bigramConcepts =\", bigramConcepts)\n",
    "    return(unigramConcepts, bigramConcepts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set(['body', 'late', 'begin', 'relevant', 'drill', 'industry', 'will', 'state', 'oil', 'exploration', '1950', 'historical', 'regulatory', 'various', 'include', 'document', 'history']) set(['oil_industry', 'relevant_document', 'regulatory_body'])\n"
     ]
    }
   ],
   "source": [
    "# testing entityRecognition\n",
    "text = \"Relevant documents will include those on historical exploration and drilling as well as history of regulatory bodies. Relevant are history of the oil industry in various states, even if drilling began in 1950 or later\"\n",
    "unigramConcepts, bigramConcepts = entityRecognition(text)\n",
    "print(unigramConcepts, bigramConcepts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#example: origText = \"Relevant documents will include those on historical exploration and drilling as well as history of regulatory bodies. Relevant are history of the oil industry in various states, even if drilling began in 1950 or later\"\n",
    "def annotateTextAlchemyApi(origText):\n",
    "    #print(\"origText =\", origText)\n",
    "    cmd = ' '.join([annotateProg, \"\\\"\" + origText + \"\\\"\"])\n",
    "    #print(\"cmd =\", cmd)\n",
    "    annotateResults = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE).stdout.read()\n",
    "    #print(\"annotateResults =\", annotateResults)\n",
    "    #annotateResults = subprocess.check_output([annotateProg, \"\\\"\" + origText + \"\\\"\"])\n",
    "    print(\"annotateResults =\", annotateResults[0:500], \"...\")\n",
    "    annotateResultsJson = re.split('## Response Object ##|## Keywords ##',annotateResults)[1].strip()\n",
    "    annotateResultsJsonParsed = json.loads(annotateResultsJson)\n",
    "    freebaseEntities = [ j[u'freebase'] for j in annotateResultsJsonParsed[\"concepts\"] if u'freebase' in j]\n",
    "    #print (freebaseEntities)\n",
    "    entityTexts = [ str(j['text'].encode('utf8')) for j in annotateResultsJsonParsed[\"concepts\"]]\n",
    "    #print (entityTexts)\n",
    "    return(entityTexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origText = Relevant documents will include those on historical exploration and drilling as well as history of regulatory bodies. Relevant are history of the oil industry in various states, even if drilling began in 1950 or later\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Petroleum', 'Renaissance', 'History', 'Petroleum industry']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing annotation:\n",
    "origText = \"Relevant documents will include those on historical exploration and drilling as well as history of regulatory bodies. Relevant are history of the oil industry in various states, even if drilling began in 1950 or later\"\n",
    "annotateTextAlchemyApi(origText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting keys from /wikipedia/en namespace of freespace\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'freebaseEntities' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-90bc42d8cc7a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"extracting keys from /wikipedia/en namespace of freespace\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"freebase url =\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfreebaseEntities\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mhtml_doc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0murllib2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfreebaseEntities\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhtml_doc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"lxml\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'freebaseEntities' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"extracting keys from /wikipedia/en namespace of freespace\")\n",
    "print (\"freebase url =\", freebaseEntities[0])\n",
    "html_doc = urllib2.urlopen(freebaseEntities[0])\n",
    "soup = BeautifulSoup(html_doc, \"lxml\")\n",
    "\n",
    "freebaseContent = soup.text.split('\\n')\n",
    "\n",
    "#  \"the key [that] can be used to formulate a URL to the corresponding English article on Wikipedia\"\n",
    "keyWikipediaEn = \"key:wikipedia.en \"\n",
    "\n",
    "wikipediaEn = [re.sub(keyWikipediaEn, '', str(i)).strip() for i in freebaseContent if keyWikipediaEn in i]\n",
    "wikipediaEn = [re.sub('[^a-zA-Z]+', ' ', i) for i in wikipediaEn]\n",
    "wikipediaEn = [re.sub( '\\s+', ' ', i ).strip() for i in wikipediaEn]\n",
    "print(\"wikipediaEn =\", wikipediaEn)\n",
    "\n",
    "\n",
    "#keyWikipediaObjectKeyEn = \"ns:type.object.key    ns:wikipedia.en.\"\n",
    "\n",
    "#wikipediaObjectKeyEn = [re.sub(keyWikipediaObjectKeyEn, '', str(i)).strip() for i in freebaseContent if keyWikipediaObjectKeyEn in i]\n",
    "#wikipediaObjectKeyEn = [re.sub('[^a-zA-Z]+', ' ', i) for i in wikipediaObjectKeyEn]\n",
    "#wikipediaObjectKeyEn = [re.sub( '\\s+', ' ', i ).strip() for i in wikipediaObjectKeyEn]\n",
    "#print(\"wikipediaObjectKeyEn =\", wikipediaObjectKeyEn)\n",
    "\n",
    "#print(set(wikipediaObjectKeyEn) - set(wikipediaEn))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parse queries file\n",
      "queries = {801: 'Describe the origin nature extent of spread and means of controlling kudzu ', 802: 'What is the impact of volcano eruptions on global temperature ', 803: 'Give the history of this holiday and the various ways of celebrating May Day ', 804: 'Describe resolutions proposed and legislation passed to ban the cloning of humans and the rationale for the bans ', 805: 'Describe the Identify Theft Passport issued to identity theft victims to show to creditors and law enforcement officers questioning their credit worthiness or innocence ', 806: 'What is Doctors Without Borders Medecins Sans Frontieres and what do they do ', 807: 'Describe the nature and history of sugar tariff rate quotas in the United States ', 808: 'What information is available on the involvement of the North Korean Government in counterfeiting of US currency ', 809: 'Identify wastewater treatment projects that involve constructed or natural wetlands ', 810: 'Provide information regarding timeshare resales ', 811: 'What is the state of recognizing handwritten inputs to computers ', 812: 'What conditions lead doctors to recommend total knee replacement surgery and what complications can result from such surgery ', 813: 'What is the Atlantic Intracoastal Waterway ', 814: 'Provide information about the Johnstown Flood in Johnstown Pennsylvania ', 815: 'Find accounts of actual Coast Guard rescues ', 816: 'Describe efforts made by USAID to protect the biodiversity in the Galapagos Islands in Ecuador ', 817: 'How are naming rights to sports stadiums acquired ', 818: 'What is known about the culture and history of the Chaco people from features of the Chaco Culture National Historic Park ', 819: 'What is known about the 1890 U S Census ', 820: 'What are imported fire ants and how can they be controlled ', 821: 'Describe the work at home scams that are promoted over the Internet ', 822: 'Give the history of the Battle of the Little Big Horn June 25 and 26 1876 also referred to as Custer s Last Stand ', 823: 'What features and services are provided by continuing care retirement communities CCRC s ', 824: 'What is the current role of the Civil Air Patrol and what training do participants receive ', 825: 'Describe the deployment of National Guard units to Iraq ', 826: 'What is the relationship between the U S and the Seminole Indians of Florida ', 827: 'Give a definition of and or a description of an application for the Hidden Markov Modeling algorithm ', 828: 'What companies or organizations use secret or mystery shoppers ', 829: 'Provide information on all kinds of material international support provided to either side in the Spanish Civil War ', 830: 'Locate past or present model railroad layouts ', 831: 'Describe the security measures at Dulles International Airport in Virginia ', 832: 'What activity involving U S labor unions has taken place since 1980 ', 833: 'Provide information about the government of Iceland ', 834: 'How is the global positioning system GPS used for research and monitoring of earthquakes ', 835: 'Why is Boston s Central Artery project also known as The Big Dig characterized as pork ', 836: 'What level of wages are paid to illegal immigrants ', 837: 'Provide information on the pre 1500 history of the Eskimo Inuit people ', 838: 'How have humans responded and how should they respond to the appearance of coyotes in urban and suburban areas ', 839: 'Explain various techniques used in dyeing textiles and identify their advantages or disadvantages ', 840: 'Give the definition locations or characteristics of geysers ', 841: 'Provide information on camels in North America in both prehistoric and modern times ', 842: 'Give information about David McCullough author his life works and or awards ', 843: 'Who was Pol Pot and what did he do ', 844: 'Give information about segmental duplications in genomes ', 845: 'Provide information about tomato farming and production in New Jersey ', 846: 'Describe evidence that heredity does or does not play a role in obesity ', 847: 'What was the role of Portugal in World War II ', 848: 'Identify radio stations by their call letters and location or ownership ', 849: 'What is scalable vector graphics ', 850: 'How frequently does the Mississippi River flood its banks '}\n"
     ]
    }
   ],
   "source": [
    "print(\"parse queries file\")\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "queries = dict()\n",
    "with open(colInConfigsFile, 'r') as infile:\n",
    "    xmlDoc = infile.read()\n",
    "    soup = BeautifulSoup(xmlDoc, \"lxml\")\n",
    "    topTags = soup.findAll('top')\n",
    "    #print(soup.prettify())\n",
    "    for topTag in topTags:\n",
    "        numTag = topTag.findAll('num')[0]\n",
    "        qNo = int(str(numTag.contents[0]).replace(\"Number: \", \"\"))\n",
    "        if qNo < 801:\n",
    "            continue\n",
    "        #narrTag = numTag.findAll('title')[0].findAll('desc')[0].findAll('narr')[0]\n",
    "        #qText = str(numTag.findAll('title')[0].contents[0])\n",
    "        qText = str(numTag.findAll('title')[0].findAll('desc')[0].contents[0])\n",
    "        #print(qTextTag)\n",
    "        qText = qText.replace(\"Description:\",\"\").replace(\"\\n\", \" \").strip()\n",
    "        #narr = narrTag.string.replace(\"Narrative:\",\"\").replace(\"\\n\", \" \").strip()\n",
    "        qText = regex.sub(' ', qText)\n",
    "        qText = re.sub( '\\s+', ' ', qText)\n",
    "        queries[qNo] = qText\n",
    "print (\"queries =\", queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mrfQueryGen(conceptText, odW, uwW):\n",
    "    mrfText = \"#weight( \"\n",
    "    mrfText += str(1 - odW - uwW) + \" #combine( \" + conceptText + \" ) \"\n",
    "    conceptText_ = conceptText.split(\" \")\n",
    "    #if len(conceptText_) > 2:\n",
    "    odText = \"\"\n",
    "    uwText = \"\"\n",
    "    for i in range(1, len(conceptText_)):\n",
    "        odText += \" #od4( \" + ' '.join(conceptText_[i-1:i+1]) + \" ) \"\n",
    "        uwText += \" #uw17( \" + ' '.join(conceptText_[i-1:i+1]) + \" ) \"\n",
    "    if len(odText) > 10: # if it is not just tags\n",
    "        mrfText += str(odW) + \" #combine( \" + odText + \" ) \"\n",
    "        mrfText += str(uwW) + \" #combine( \" + uwText + \" ) \"\n",
    "    mrfText += \" ) \"\n",
    "    return(mrfText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generateQuery(dirCoeff, intCoeff, odW, uwW):\n",
    "    soupNew = BeautifulSoup(\"\", 'lxml')\n",
    "    parameters_tag = soupNew.new_tag(\"parameters\")\n",
    "    index_tag = soupNew.new_tag(\"index\")\n",
    "    index_tag.string = colIndexDir\n",
    "    parameters_tag.append(index_tag)\n",
    "    for qNo, qText in queries.iteritems():\n",
    "        doc_tag = soupNew.new_tag(\"query\")\n",
    "\n",
    "        docno_tag = soupNew.new_tag(\"number\")\n",
    "        docno_tag.string = str(qNo)\n",
    "        doc_tag.append(docno_tag)\n",
    "        \n",
    "        text_tag = soupNew.new_tag(\"text\")\n",
    "        \n",
    "        text_tag.string = \"#weight( \"\n",
    "        text_tag.string += str(1- intCoeff) + \" #combine( \" + qText + \")\"\n",
    "        \n",
    "        concepts = annotateTextAlchemyApi(qText)\n",
    "        \n",
    "        text_tag.string += str(intCoeff) + \" #combine(\"\n",
    "        for concept in concepts:\n",
    "            print(\"concept =\", concept.decode('utf-8'))\n",
    "            text_tag.string += mrfQueryGen(concept.decode('utf-8'), odW, uwW)\n",
    "        text_tag.string += \" )\"\n",
    "\n",
    "        text_tag.string += \" )\"\n",
    "        \n",
    "        doc_tag.append(text_tag)\n",
    "\n",
    "        parameters_tag.append(doc_tag)\n",
    "        #print(doc_tag)\n",
    "\n",
    "    rule_tag = soupNew.new_tag(\"rule\")\n",
    "    rule_tag.string = \"method:dir,mu:\" + str(dirCoeff)\n",
    "    #rule_tag.string = \"method:two\"\n",
    "    parameters_tag.append(rule_tag)\n",
    "\n",
    "    #intCoeff_tag = soupNew.new_tag(\"intCoeff\")\n",
    "    #intCoeff_tag.string = \"0.8\"\n",
    "    #parameters_tag.append(intCoeff_tag)\n",
    "\n",
    "    threads_tag = soupNew.new_tag(\"threads\")\n",
    "    threads_tag.string = \"32\"\n",
    "    parameters_tag.append(threads_tag)\n",
    "\n",
    "    count_tag = soupNew.new_tag(\"count\")\n",
    "    count_tag.string = \"1000\"\n",
    "    parameters_tag.append(count_tag)\n",
    "\n",
    "    trecFormat_tag = soupNew.new_tag(\"trecFormat\")\n",
    "    trecFormat_tag.string = \"true\"\n",
    "    parameters_tag.append(trecFormat_tag)\n",
    "\n",
    "    soupNew.append(parameters_tag)\n",
    "    print(soupNew.prettify())\n",
    "    #print(\"outFileName =\", outFileName)\n",
    "\n",
    "    with open(colOutConfigsFile, 'w') as outfile:\n",
    "        outfile.write(str(soupNew))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "annotateResults = text = Describe the origin nature extent of spread and means of controlling kudzu \n",
      "Error in combined call:  daily-transaction-limit-exceeded\n",
      " ...\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-857bfd29b10a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0modW\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.15\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0muwW\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.15\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mgenerateQuery\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdirrCoeff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mintCoeff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0modW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muwW\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-39-b08882228062>\u001b[0m in \u001b[0;36mgenerateQuery\u001b[1;34m(dirCoeff, intCoeff, odW, uwW)\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mtext_tag\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m \u001b[0mintCoeff\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" #combine( \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mqText\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\")\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mconcepts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mannotateText\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mqText\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mtext_tag\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mintCoeff\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" #combine(\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-44-40c0bf6fe54e>\u001b[0m in \u001b[0;36mannotateText\u001b[1;34m(origText)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;31m#annotateResults = subprocess.check_output([annotateProg, \"\\\"\" + origText + \"\\\"\"])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"annotateResults =\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mannotateResults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mannotateResultsJson\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'## Response Object ##|## Keywords ##'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mannotateResults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mannotateResultsJsonParsed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mannotateResultsJson\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mfreebaseEntities\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34mu'freebase'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mannotateResultsJsonParsed\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"concepts\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;34mu'freebase'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "dirrCoeff= 2600\n",
    "intCoeff = 0.8\n",
    "odW = 0.15\n",
    "uwW = 0.15\n",
    "generateQuery(dirrCoeff, intCoeff, odW, uwW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cmd = ' '.join([\"IndriRunQuery\", colOutConfigsFile, \">\", colRunsFile])\n",
    "print(\"cmd =\", cmd)\n",
    "subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE).stdout.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cmd = ' '.join([\"trec_eval\", \"-q\", colQrelsFile, colRunsFile, \">\", colEvalsFile])\n",
    "print(\"cmd =\", cmd)\n",
    "subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE).stdout.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(colEvalsFile, \"r\") as infile:\n",
    "    reader = csv.reader(infile, delimiter='\\t')\n",
    "    precTable = list(reader)\n",
    "    #print(precTable)\n",
    "    precTable = {i[0].strip(): float(i[2].strip()) for i in precTable if i[1].strip()==\"all\" and i[0].strip() != \"runid\"}\n",
    "    print(precTable['map'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
