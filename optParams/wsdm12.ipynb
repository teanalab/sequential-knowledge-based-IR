{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function \n",
    "from bs4 import BeautifulSoup\n",
    "from BeautifulSoup import SoupStrainer as sopstrain\n",
    "import subprocess\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import string\n",
    "import operator\n",
    "import csv\n",
    "import urllib2\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cfg infile Name = /home/fj9124/projects/ir/seq_kb_ir/configs/trec7n8/queryTrec7n8\n",
      "cfg outfile Name = /home/fj9124/projects/ir/seq_kb_ir/configs/trec7n8/lr/cnet/indriRunQuery.cfg\n",
      "colIndexDir = /scratch/index/indri_5_7/trec7n8\n",
      "knowledgGraph index Dir = /scratch/index/indri_5_7/trec7n8\n",
      "col Qrels File Name = /home/fj9124/projects/ir/seq_kb_ir/qrels/trec7n8/qrels.csv\n",
      "runsFileName = /home/fj9124/projects/ir/seq_kb_ir/runs/lr/trec7n8/cnet/indriRunQuery.runs\n",
      "evalsFileName = /home/fj9124/projects/ir/seq_kb_ir/evals/lr/trec7n8/cnet/indriRunQuery.evals\n",
      "countsResultsFile = /home/fj9124/projects/ir/seq_kb_ir/occuranceCount/results/trec7n8.txt\n",
      "conceptnet5RelAllFilename = /scratch/saeid/data/conceptnet5_simp.csv\n",
      "cfgTmpOutFileName = /home/fj9124/projects/ir/seq_kb_ir/configs/trec7n8/lr/cnet/indriRunQuery.cfg.tmp\n",
      "docIdNameMapFileName = /home/fj9124/projects/ir/seq_kb_ir/occuranceCount/results/trec7n8_docIdNameMap.txt\n"
     ]
    }
   ],
   "source": [
    "knowledgGraph = \"cnet\" # conceptnet5AssocMod, gov, conceptnet5AssocMi, conceptnet5AssocHdl\n",
    "#knowledgGraph_ = ''.join([k.capitalize() if i>0 else k for i, k in enumerate(knowledgGraph)])\n",
    "collection = \"trec7n8\"\n",
    "method = \"lr\" # hal, mi, assoc, assocMi, assocHal, assoc2\n",
    "simMeasure = \"\" # mi, cnet\n",
    "projectDir = \"/home/fj9124/projects/ir/seq_kb_ir/\" \n",
    "indexDir = \"/scratch/index/indri_5_7/\"\n",
    "colMethodConfigsDir = os.path.join(projectDir, \"configs\", collection, method)\n",
    "cfgInFileName = os.path.join(projectDir, \"configs\", collection, \"query\" + collection.capitalize()) \n",
    "print(\"cfg infile Name =\", cfgInFileName)\n",
    "cfgOutFileName=os.path.join(colMethodConfigsDir, knowledgGraph, \"indriRunQuery.cfg\") \n",
    "print(\"cfg outfile Name =\", cfgOutFileName)\n",
    "colIndexDir = os.path.join(indexDir, collection) \n",
    "print(\"colIndexDir =\", colIndexDir)\n",
    "if knowledgGraph in {\"cnet\"}:\n",
    "    knowledgGraphIndexDir = os.path.join(indexDir, collection)   \n",
    "else:\n",
    "    knowledgGraphIndexDir = os.path.join(indexDir, knowledgGraph)   \n",
    "print(\"knowledgGraph index Dir =\", knowledgGraphIndexDir)\n",
    "graphsDir = os.path.join(projectDir, \"graphs\")\n",
    "#methodGraphsDir = os.path.join(graphsDir, method)\n",
    "methodGraphsFileName = []\n",
    "if method == \"hal\":\n",
    "    methodGraphsFileName = [os.path.join(graphsDir, method, knowledgGraph + \".txt\")]\n",
    "    print(\"methodGraphsFileName =\", methodGraphsFileName)\n",
    "elif method.translate(None, string.digits) in {\"mi\", \"assoc\"}:\n",
    "    methodGraphsFileName = [os.path.join(graphsDir, method.translate(None, string.digits), knowledgGraph, \"graph.txt\")]\n",
    "    print(\"methodGraphsFileName =\", methodGraphsFileName)\n",
    "elif method in {\"assocMi\"}:\n",
    "    if knowledgGraph == \"conceptnet5AssocGov\":\n",
    "        methodGraphsFileName = [os.path.join(graphsDir, \"assoc\", collection, \"conceptnet5AssocMod\" + \".txt\")]\n",
    "        methodGraphsFileName += [os.path.join(graphsDir, \"mi\", collection, \"gov\" + \".txt\")]\n",
    "        print(\"methodGraphsFileName =\", methodGraphsFileName)\n",
    "elif method in {\"assocHal\"}:\n",
    "    if knowledgGraph == \"conceptnet5AssocGov\":\n",
    "        methodGraphsFileName = [os.path.join(graphsDir, \"assoc\", collection, \"conceptnet5AssocMod\" + \".txt\")]\n",
    "        methodGraphsFileName += [os.path.join(graphsDir, \"hal\", \"gov\" + \".txt\")]\n",
    "        print(\"methodGraphsFileName =\", methodGraphsFileName)\n",
    "qrelsDir = os.path.join(projectDir, \"qrels\")\n",
    "colQrelsDir = os.path.join(qrelsDir, collection)\n",
    "colQrelsFileName = os.path.join(colQrelsDir, \"qrels.csv\")\n",
    "print(\"col Qrels File Name =\", colQrelsFileName)\n",
    "runsFileName = os.path.join(projectDir, \"runs\", method, collection, knowledgGraph, \"indriRunQuery.runs\") \n",
    "print(\"runsFileName =\", runsFileName)\n",
    "evalsFileName = os.path.join(projectDir, \"evals\", method, collection, knowledgGraph, \"indriRunQuery.evals\") \n",
    "print(\"evalsFileName =\", evalsFileName)\n",
    "countsResultsFile = os.path.join(projectDir,\"occuranceCount\",\"results\",collection+\".txt\")\n",
    "print(\"countsResultsFile =\", countsResultsFile)\n",
    "conceptnet5RelAllFilename =\"/scratch/saeid/data/conceptnet5_simp.csv\"\n",
    "print(\"conceptnet5RelAllFilename =\", conceptnet5RelAllFilename)\n",
    "cfgTmpOutFileName = cfgOutFileName + \".tmp\"\n",
    "print(\"cfgTmpOutFileName =\", cfgTmpOutFileName)\n",
    "docIdNameMapFileName = os.path.join(projectDir, \"occuranceCount\", \"results\", collection + \"_docIdNameMap.txt\")\n",
    "print(\"docIdNameMapFileName =\", docIdNameMapFileName)\n",
    "#expansionCount = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', 'FR940104-1-00001']\n",
      "LA082389-0020 \t 441223\n",
      "FBIS3-32596 \t 246524\n",
      "LA011890-0198 \t 465144\n",
      "LA011890-0199 \t 465145\n",
      "LA011890-0196 \t 465142\n",
      "LA011890-0197 \t 465143\n",
      "LA011890-0194 \t 465140\n",
      "FT932-3799 \t 88713\n",
      "LA011890-0192 \t 465138\n",
      "LA011890-0193 \t 465139\n",
      "LA011890-0190 \t 465136\n",
      "LA011890-0191 \t 465137\n",
      "FT932-3796 \t 88710\n",
      "FT924-2868 \t 187695\n",
      "FT924-2869 \t 187696\n",
      "FT932-3797 \t 88711\n",
      "FT924-2864 \t 187691\n",
      "FT924-2865 \t 187692\n",
      "FT924-2866 \t 187693\n",
      "FT924-2867 \t 187694\n",
      "FT924-2860 \t 187687\n",
      "FT924-2861 \t 187688\n"
     ]
    }
   ],
   "source": [
    "with open(docIdNameMapFileName, 'r') as f:\n",
    "    reader = list(csv.reader(f, delimiter = ' '))\n",
    "    print(reader[0])\n",
    "    docIdNameMap = {i[1]:i[0] for i in reader}\n",
    "    docNameIdMap = {i[0]:i[1] for i in reader}\n",
    "for i, (k, v) in enumerate(docIdNameMap.iteritems()):\n",
    "    print (k, '\\t', v)\n",
    "    if (i>20):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of count_history = 81446\n",
      "#uw(#4( storm ) #4( be appear in sky )) \t 0\n",
      "#uw(#4( work on sundai )) \t 137\n",
      "#uw(#4( calm time )) \t 23\n",
      "#uw(#4( kill ) #4( prai )) \t 394\n",
      "#uw(#4( bank ) #4( teller offic )) \t 5\n",
      "#uw(#4( water ) #4( hydrogen )) \t 347\n",
      "#uw(#4( drug ) #4( take )) \t 8847\n",
      "#uw(#4( space ) #4( mostli empti )) \t 3\n",
      "#uw(#4( chang ) #4( edg )) \t 3803\n",
      "#uw(#4( happin )) \t 0\n",
      "#uw(#4( trade ) #4( countri )) \t 45838\n",
      "#uw(#4( game ) #4( leisur activ )) \t 23\n",
      "#uw(#4( round in shape )) \t 4\n",
      "#uw(#4( good ) #4( other person )) \t 411\n",
      "#uw(#4( smoke ) #4( fire )) \t 1842\n",
      "#uw(#4( spice up salad )) \t 1\n",
      "#uw(#4( be wound )) \t 410\n",
      "#uw(#4( good ) #4( hear nice new )) \t 0\n",
      "#uw(#4( teacher ) #4( teach student of class )) \t 0\n",
      "#uw(#4( deliveri )) \t 11268\n",
      "#uw(#4( bank ) #4( teller monei )) \t 23\n",
      "#uw(#4( swap share )) \t 106\n"
     ]
    }
   ],
   "source": [
    "count_history = dict()\n",
    "with open(countsResultsFile, 'r') as f:\n",
    "    reader = csv.reader(f, delimiter = \"\\t\")\n",
    "    count_history = {k:int(float(v)) for k,v in list(reader)}\n",
    "print(\"size of count_history =\", len(count_history))\n",
    "for i, (k, v) in enumerate(count_history.iteritems()):\n",
    "    print (k, '\\t', v)\n",
    "    if (i>20):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of conceptnet5RelAll = 3207471\n",
      "tripolitan \t tripoline \t Synonym\n",
      "tripolitan \t tripoli \t RelatedTo\n",
      "age of nemesis \t band \t IsA\n",
      "age of nemesis \t organisation \t InstanceOf\n",
      "age of nemesis \t progressive rock \t dbpedia/genre\n",
      "age of nemesis \t progressive metal \t dbpedia/genre\n",
      "joseph john annabring \t person \t InstanceOf\n",
      "british rail class 438 \t mean of transportation \t InstanceOf\n",
      "british rail class 438 \t train \t InstanceOf\n"
     ]
    }
   ],
   "source": [
    "conceptnet5RelAll = defaultdict(dict)\n",
    "conceptnet5RelAllInv = defaultdict(dict)\n",
    "with open(conceptnet5RelAllFilename, 'r') as f:\n",
    "    reader = csv.reader(f, delimiter = \",\")\n",
    "    for line in list(reader):\n",
    "        if line[0].strip() != \"\" and line[1].strip() != \"\" and line[2].strip() != \"\":\n",
    "            conceptnet5RelAll[line[1].strip()][line[2].strip()] = line[0].strip()\n",
    "            conceptnet5RelAllInv[line[2].strip()][line[1].strip()] = line[0].strip()\n",
    "print(\"size of conceptnet5RelAll =\", len(conceptnet5RelAll))\n",
    "for i, (k1, k2v) in enumerate(conceptnet5RelAll.iteritems()):\n",
    "    for j, (k2, v) in enumerate(k2v.iteritems()):\n",
    "        if (i>3 or j>3):\n",
    "            break\n",
    "        print (k1, '\\t', k2, '\\t', v)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing: ['go', 'read']\n"
     ]
    }
   ],
   "source": [
    "def splitStemText(text):\n",
    "    stemmedWords = []\n",
    "    text = re.sub('/|-|\\\"|_',' ', text) # replace - and slash with space\n",
    "    if method in {\"assocRestful\", \"lr\"}:\n",
    "        for text_ in text.split(' '):\n",
    "            text = text_.replace(\" \", \"_\")\n",
    "            url = \"http://conceptnet5.media.mit.edu/data/5.4/uri?language=en&text=\" + text\n",
    "            response = urllib2.urlopen(url)\n",
    "            data = response.read()\n",
    "            data_j = json.loads(data)\n",
    "            text = os.path.basename(data_j[u'uri'])\n",
    "            url = \"http://conceptnet5.media.mit.edu/data/5.4/uri?language=en&text=\" + text\n",
    "            response = urllib2.urlopen(url)\n",
    "            data = response.read()\n",
    "            data_j = json.loads(data)\n",
    "            #print(\"data_j =\", data_j)\n",
    "            #print(\"data_j[u'uri'] =\", data_j[u'uri'])\n",
    "            stemmedWords += [str(os.path.basename(data_j[u'uri']))]\n",
    "        #print(\"stemmedWords =\", stemmedWords)\n",
    "        return (stemmedWords)\n",
    "    else:\n",
    "        words = text.split()\n",
    "        for w in words:\n",
    "            w = re.sub('\\(|\\)|\\'s|,','', w) # remove paranthesis, apostrophe s, comma\n",
    "            w = re.sub('\\'','', w) # remove apostrophe\n",
    "            cmd = \"dumpindex \" + knowledgGraphIndexDir + \" t \" + w + \" | head -n1\"\n",
    "            #print(\"cmd =\", cmd)\n",
    "            stemmedWords.append(subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE).stdout.read().split()[1])\n",
    "        return (stemmedWords)\n",
    "\n",
    "print (\"testing:\", splitStemText(\"going reading\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def analyseQueries():\n",
    "    #    #print(\"intCoeff =\", intCoeff)\n",
    "    with open(cfgInFileName, 'r') as inFile:\n",
    "        reader = inFile.read()\n",
    "        soup = BeautifulSoup(reader, 'lxml')\n",
    "        if collection in {\"aquaint\", \"gov\"}:\n",
    "            tags = soup.find_all(['doc'])\n",
    "        elif collection in {\"robust\", \"trec7n8\"}:\n",
    "            tags = soup.find_all(['top'])\n",
    "            #print(tags)\n",
    "        \n",
    "        origWordsAll = dict()\n",
    "        for tag in tags:\n",
    "            \n",
    "            if collection in {\"aquaint\", \"gov\"}:\n",
    "                docno = (tag.find('docno').string).strip()\n",
    "            elif collection in {\"robust\", \"trec7n8\"}:\n",
    "                result = re.search('<num> Number: (.*)\\n', str(tag))                \n",
    "                docno = result.group(1).strip()\n",
    "            \n",
    "            if collection in {\"aquaint\", \"gov\"}:\n",
    "                origWords = splitStemText(tag.find('text').string.strip())\n",
    "            elif collection in {\"robust\", \"trec7n8\"}:\n",
    "                result = re.search('<title>(.*)\\n', str(tag))                \n",
    "                origWords = splitStemText(result.group(1).strip())\n",
    "            #print(\"origWords =\", origWords)\n",
    "            origWordsAll[docno] = origWords\n",
    "            \n",
    "    return (origWordsAll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def countExpr_get_2(count_history, w, v):\n",
    "    if w.translate(None, string.punctuation).strip() == \"\" or v.translate(None, string.punctuation).strip() == \"\":\n",
    "        return 0;\n",
    "    return count_history[\"#uw(#4( \" + w.translate(None, string.punctuation) + \" ) #4( \" + v.translate(None, string.punctuation) + \" ))\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docIdNameMap[\"FT944-15973\"] =  6961\n",
      "documentLength(\"FT944-15973\") = 287\n"
     ]
    }
   ],
   "source": [
    "def documentLength(documentName):\n",
    "    documentLength_ = subprocess.Popen(' '.join([\"dumpindex\", colIndexDir, \"dv\", docIdNameMap[documentName], \"|\", \"awk\", \"\\'END{print $1}\\'\"]), shell=True, stdout=subprocess.PIPE).stdout.read()\n",
    "    return int(documentLength_)\n",
    "\n",
    "print(\"docIdNameMap[\\\"FT944-15973\\\"] = \", docIdNameMap[\"FT944-15973\"])\n",
    "print(\"documentLength(\\\"FT944-15973\\\") =\", documentLength(\"FT944-15973\"))\n",
    "\n",
    "def weightRelConcept(docno, origWords_, ow, relW, intCoeff0, intCoeff1, intCoeff2, dirCoeff, count_history, featureWeights):\n",
    "    \n",
    "    features = dict()\n",
    "    \n",
    "    genOneSimpQuery(docno, origWords_, relW, intCoeff0, intCoeff1, dirCoeff, cfgTmpOutFileName)\n",
    "    \n",
    "    indriRun = subprocess.Popen(\"IndriRunQuery \" + cfgTmpOutFileName, shell=True, stdout=subprocess.PIPE).stdout.read()\n",
    "    indriRun = [i.split(' ') for i in indriRun.split('\\n')]\n",
    "    indriRun = [i for i in indriRun if len(i)>1] # remove empty arrays\n",
    "    topTDocScores_d = {i[2]:np.float(i[4]) for i in indriRun}\n",
    "    \n",
    "    #scores = [ np.float(i[4]) for i in indriRun]\n",
    "    topTDocs = [i[2] for i in indriRun]\n",
    "    \n",
    "    topTDocScore = np.sum([ np.float(i[4]) for i in indriRun])\n",
    "    features[\"topTDocScore\"] = topTDocScore\n",
    "    \n",
    "    relWDocuments = subprocess.Popen(' '.join([\"dumpindex\", colIndexDir, \"e\", \"#od4(\" + relW + \")\"]), shell=True, stdout=subprocess.PIPE).stdout.read()\n",
    "    relWDocuments = [i.split(' ') for i in relWDocuments.split('\\n')]\n",
    "    relWDocuments = [[docNameIdMap[i[0]], i[1], i[2], i[3]] for j, i in enumerate(relWDocuments) if j>0] # remove empty arrays\n",
    "    #print(\"relWDocuments =\", relWDocuments)\n",
    "    \n",
    "    relWTopDocuments = [i for i in relWDocuments if i[0] in topTDocs]\n",
    "    #print(\"relWTopDocuments =\", relWTopDocuments)\n",
    "    #numerator = np.sum([int(i[1]) for i in relWTopDocuments]) # term count in the collection\n",
    "    numerator = len(relWTopDocuments) # term count in the collection\n",
    "    denumerator = np.sum([documentLength(i) for i in topTDocs])\n",
    "    topTermFrac = numerator / np.float(denumerator)\n",
    "    #print(\"numerator, denumerator, topTermFrac =\", numerator, denumerator, topTermFrac)\n",
    "    features[\"topTermFrac\"] = topTermFrac\n",
    "    \n",
    "    numCanDocs = len(set([i[0] for i in relWTopDocuments]))\n",
    "    features[\"numCanDocs\"] = numCanDocs\n",
    "\n",
    "    avgCDocScore = np.sum([ topTDocScores_d[i[0]] for i in relWTopDocuments])/np.float(numCanDocs)\n",
    "    features[\"avgCDocScore\"] = avgCDocScore\n",
    "    \n",
    "    maxCDocScore = np.max([ topTDocScores_d[i[0]] for i in relWTopDocuments])\n",
    "    features[\"maxCDocScore\"] = maxCDocScore\n",
    "    \n",
    "    conIdf = len(relWDocuments)\n",
    "    features[\"conIdf\"] = conIdf\n",
    "    \n",
    "    coocurDocuments = dict()\n",
    "    coocurDocumentsTop = dict()\n",
    "    for ow in origWords_:\n",
    "        coocurDocuments_ = subprocess.Popen(' '.join([\"dumpindex\", colIndexDir, \"e\", \"#uw(#4( \" + ow + \" ) #4( \" + relW + \" ))\"]), shell=True, stdout=subprocess.PIPE).stdout.read()\n",
    "        coocurDocuments_ = [i.split(' ') for i in coocurDocuments_.split('\\n')]\n",
    "        coocurDocuments[ow] = [[docNameIdMap[i[0]], i[1], i[2], i[3]] for j, i in enumerate(coocurDocuments_) if j>0] # remove empty arrays\n",
    "        coocurDocumentsTop[ow] = [[i[0], i[1], i[2], i[3]] for i in coocurDocuments[ow] if i[0] in topTDocs ] # remove empty arrays\n",
    "\n",
    "        \n",
    "    avgColCor = np.sum([len(i) for i in coocurDocuments.values()])/np.float(len(coocurDocuments)) \n",
    "    features[\"avgColCor\"] = avgColCor\n",
    "\n",
    "    maxColCor = np.max([len(i) for i in coocurDocuments.values()])\n",
    "    features[\"maxColCor\"] = maxColCor\n",
    "    \n",
    "    avgTopColCor = np.sum([len(i) for i in coocurDocumentsTop.values()])/np.float(len(coocurDocumentsTop)) \n",
    "    features[\"avgTopColCor\"] = avgTopColCor\n",
    "\n",
    "    maxTopColCor = np.max([len(i) for i in coocurDocumentsTop.values()])\n",
    "    features[\"maxTopColCor\"] = maxTopColCor\n",
    "\n",
    "    for j1, ow1 in enumerate(origWords_):\n",
    "        for j2, ow2 in enumerate(origWords_):\n",
    "            if j1>j2:\n",
    "                coocurDocuments_ = subprocess.Popen(' '.join([\"dumpindex\", colIndexDir, \"e\", \"#uw(#4( \" + ow1 + \" ) #4( \" + ow2 + \" ) #4( \" + relW + \" ))\"]), shell=True, stdout=subprocess.PIPE).stdout.read()\n",
    "                coocurDocuments_ = [i.split(' ') for i in coocurDocuments_.split('\\n')]\n",
    "                coocurDocuments[ow1+ow2] = [[docNameIdMap[i[0]], i[1], i[2], i[3]] for j, i in enumerate(coocurDocuments_) if j>0] # remove empty arrays\n",
    "                coocurDocumentsTop[ow1+ow2] = [[i[0], i[1], i[2], i[3]] for i in coocurDocuments[ow1+ow2] if i[0] in topTDocs ] # remove empty arrays\n",
    "\n",
    "    avgColPCor = np.sum([len(i) for i in coocurDocuments.values()])/np.float(len(coocurDocuments)) \n",
    "    features[\"avgColPCor\"] = avgColPCor\n",
    "\n",
    "    maxColPCor = np.max([len(i) for i in coocurDocuments.values()])\n",
    "    features[\"maxColPCor\"] = maxColPCor\n",
    "    \n",
    "    avgTopColPCor = np.sum([len(i) for i in coocurDocumentsTop.values()])/np.float(len(coocurDocumentsTop)) \n",
    "    features[\"avgTopColPCor\"] = avgTopColPCor\n",
    "\n",
    "    maxTopColPCor = np.max([len(i) for i in coocurDocumentsTop.values()])\n",
    "    features[\"maxTopColPCor\"] = maxTopColPCor\n",
    "    \n",
    "    relWg = 0\n",
    "    for feature, weight in featureWeights.iteritems():\n",
    "        relWg += features[feature] * weight\n",
    "        \n",
    "    return relWg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def genOneSimpQuery(docno, origWords_, relW, intCoeff0, intCoeff1, dirCoeff, cfgTmpOutFileName):\n",
    "    #    #print(\"intCoeff =\", intCoeff)\n",
    "    soupNew = BeautifulSoup(\"\", 'lxml')\n",
    "    parameters_tag = soupNew.new_tag(\"parameters\")\n",
    "    index_tag = soupNew.new_tag(\"index\")\n",
    "    index_tag.string = colIndexDir\n",
    "    parameters_tag.append(index_tag)\n",
    "        #print(tags)\n",
    "\n",
    "    doc_tag = soupNew.new_tag(\"query\")\n",
    "\n",
    "    docno_tag = soupNew.new_tag(\"number\")\n",
    "    docno_tag.string = docno\n",
    "    doc_tag.append(docno_tag)\n",
    "\n",
    "    text_tag = soupNew.new_tag(\"text\")\n",
    "    text_tag.string = \"#weight(\\n\" \n",
    "\n",
    "    text_tag.string += str(intCoeff0) + \" #combine(\" \n",
    "    for ow in set(origWords_):\n",
    "        ow = regex.sub(' ', ow)\n",
    "        text_tag.string += ow + \" \"\n",
    "    text_tag.string += \")\\n\"\n",
    "\n",
    "    if len(relW)>0: \n",
    "        relText_string1 = relW\n",
    "        text_tag.string += str(intCoeff1) + \" #combine(\" \n",
    "        #print(\"relText_string1 =\", relText_string1)\n",
    "        text_tag.string += relText_string1.encode('utf-8')\n",
    "        text_tag.string += \")\\n\"\n",
    "\n",
    "    text_tag.string += \") \"\n",
    "\n",
    "    doc_tag.append(text_tag)\n",
    "\n",
    "    parameters_tag.append(doc_tag)\n",
    "    #print(doc_tag)\n",
    "\n",
    "    rule_tag = soupNew.new_tag(\"rule\")\n",
    "    rule_tag.string = \"method:dir,mu:\" + str(dirCoeff)\n",
    "    #rule_tag.string = \"method:two\"\n",
    "    parameters_tag.append(rule_tag)\n",
    "\n",
    "    #intCoeff_tag = soupNew.new_tag(\"intCoeff\")\n",
    "    #intCoeff_tag.string = \"0.8\"\n",
    "    #parameters_tag.append(intCoeff_tag)\n",
    "\n",
    "    threads_tag = soupNew.new_tag(\"threads\")\n",
    "    threads_tag.string = \"32\"\n",
    "    parameters_tag.append(threads_tag)\n",
    "\n",
    "    count_tag = soupNew.new_tag(\"count\")\n",
    "    count_tag.string = \"10\"\n",
    "    parameters_tag.append(count_tag)\n",
    "\n",
    "    trecFormat_tag = soupNew.new_tag(\"trecFormat\")\n",
    "    trecFormat_tag.string = \"true\"\n",
    "    parameters_tag.append(trecFormat_tag)\n",
    "\n",
    "    soupNew.append(parameters_tag)\n",
    "    #print(soupNew.prettify())\n",
    "    #print(\"outFileName =\", outFileName)\n",
    "    with open( cfgTmpOutFileName, 'w') as outFile:\n",
    "        soupNewStr = str(soupNew)\n",
    "        soupNewStr = soupNewStr.replace(\"</text>\", \"\\n</text>\\n\").replace(\"query>\", \"query>\\n\").replace(\"<text>\", \"\\n<text>\\n\").replace(\"</index>\", \"</index>\\n\").replace(\"\\n<index>\", \"<index>\")\n",
    "\n",
    "        outFile.write(soupNewStr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# keys of rWords are important\n",
    "# values of rWords1 are important\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "def getRelWords(count_history, rWords, docno, origWords, expansionCount, intCoeff0, intCoeff1, intCoeff2, dirCoeff):\n",
    "    #print(\"origWords =\", origWords)\n",
    "    text_string = \"\"\n",
    "    \n",
    "        \n",
    "    relWords = dict()\n",
    "    #if method == \"relAll\":\n",
    "        \n",
    "    if method == \"assocRestful\":\n",
    "        origWords_ = [w.replace(\" \", \"_\") for w in origWords]\n",
    "        url = \"http://conceptnet5.media.mit.edu/data/5.4/assoc/list/en/\"+','.join(origWords_)\n",
    "        response = urllib2.urlopen(url)\n",
    "        data = response.read()\n",
    "        data_j = json.loads(data)\n",
    "        relWords = {str(d[0].encode('utf-8')).replace('/c/en/', '').replace('-', ' '):np.float(d[1]) for d in data_j[u'similar'] if '/c/en/' in str(d[0].encode('utf-8'))}\n",
    "    \n",
    "    elif method == \"lr\":\n",
    "        origWords_ = [w.replace(\" \", \"_\") for w in origWords]\n",
    "        #relWords_l = []\n",
    "        for ow in origWords_:\n",
    "            #relWords_l += list(conceptnet5RelAll[ow])\n",
    "            #print(ow, \"---\", conceptnet5RelAll[ow])\n",
    "            for relW in conceptnet5RelAll[ow].keys():\n",
    "                relWg = weightRelConcept(docno, origWords_, ow, relW, intCoeff0, intCoeff1, intCoeff2, dirCoeff)\n",
    "                if relW in relWords:\n",
    "                    relWords[relW] += relWg\n",
    "                else:\n",
    "                    relWords[relW] = relWg   \n",
    "    \n",
    "    else:\n",
    "        count_history = countExpr(count_history, origWords, rWords)\n",
    "        expressions = set()\n",
    "        for ow in origWords:\n",
    "            rWords_ow = rWords.get(ow, [])\n",
    "            expressions.add(\"#4( \" + ow.translate(None, string.punctuation) + \" )\")\n",
    "            if ow.strip() == \"\":\n",
    "                continue\n",
    "            if simMeasure == \"mi\":\n",
    "                N_w = countExpr_get_1(count_history, ow)\n",
    "            for rw_wv in rWords_ow:\n",
    "                (rw_w, rw_v) = rw_wv.items()[0]\n",
    "                if rw_w.strip() == \"\":\n",
    "                    continue           \n",
    "                if simMeasure == \"mi\":\n",
    "                    N_v = countExpr_get_1(count_history, rw_w)\n",
    "                    N_wv = countExpr_get_2(count_history, ow, rw_w)\n",
    "                    rw_v_ = mi_(N_w, N_v, N_wv)\n",
    "                elif simMeasure == \"cnet\":\n",
    "                    rw_v_ = rw_v\n",
    "                relWords[rw_w] = relWords.get(rw_w, 0) + rw_v_\n",
    "    \n",
    "    relWords_sorted = sorted(relWords.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    #print (\"origWords =\", origWords)\n",
    "    #print (\"relWords_sorted =\", relWords_sorted)\n",
    "    #counter = 0\n",
    "    relWords_sel = []\n",
    "    for counter, (rw_w, rw_v) in enumerate(relWords_sorted):\n",
    "        if (counter >= expansionCount):\n",
    "            break\n",
    "        if  all(c in string.printable for c in rw_w):\n",
    "            if rw_w not in origWords:\n",
    "                rw_w = regex.sub(' ', rw_w)\n",
    "                text_string += rw_w + \" \"\n",
    "                relWords_sel += [rw_w]\n",
    "        #counter += 1\n",
    "    #print(\"counter =\", counter)\n",
    "    return(count_history, text_string, relWords_sel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getAllRelWords(count_history, rWords, origWordsAll, expansionCount1, N1, expansionCount2, intCoeff0, intCoeff1, intCoeff2, dirCoeff):\n",
    "    relText_stringAll = []\n",
    "    relText_string = dict()\n",
    "    relWords_sel = dict()\n",
    "    for docno, origWords in origWordsAll.iteritems():\n",
    "        count_history, relText_string_, relWords_sel_ = getRelWords(count_history, rWords, docno, origWords, expansionCount1, intCoeff0, intCoeff1, intCoeff2, dirCoeff)\n",
    "        relText_string[docno] = relText_string_\n",
    "        relWords_sel[docno] = relWords_sel_[0:N1]\n",
    "        #print(docno, \"---\", origWords, \"---\", relWords_sel_)\n",
    "    relText_stringAll += [relText_string]\n",
    "    \n",
    "    relText_string = dict()\n",
    "    relWords_sel = dict()\n",
    "    for docno, origWords_new in relWords_sel.iteritems():\n",
    "        count_history, relText_string_, relWords_sel_ = getRelWords(count_history, rWords, docno, origWords_new, expansionCount2, intCoeff0, intCoeff1, intCoeff2, dirCoeff)\n",
    "        relText_string[docno] = relText_string_\n",
    "        relWords_sel[docno] = relWords_sel_\n",
    "    relText_stringAll += [relText_string]\n",
    "    \n",
    "    #print(\"relText_stringAll =\", relText_stringAll)\n",
    "    return (count_history, relText_stringAll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "def genQueries(origWordsAll, relText_stringAll, intCoeff0, intCoeff1, intCoeff2, dirCoeff):\n",
    "    #    #print(\"intCoeff =\", intCoeff)\n",
    "    with open(cfgInFileName, 'r') as inFile:\n",
    "        reader = inFile.read()\n",
    "        soupNew = BeautifulSoup(\"\", 'lxml')\n",
    "        parameters_tag = soupNew.new_tag(\"parameters\")\n",
    "        index_tag = soupNew.new_tag(\"index\")\n",
    "        index_tag.string = colIndexDir\n",
    "        parameters_tag.append(index_tag)\n",
    "            #print(tags)\n",
    "\n",
    "        for docno, origWords in origWordsAll.iteritems():\n",
    "            doc_tag = soupNew.new_tag(\"query\")\n",
    "\n",
    "            docno_tag = soupNew.new_tag(\"number\")\n",
    "            docno_tag.string = docno\n",
    "            doc_tag.append(docno_tag)\n",
    "\n",
    "            text_tag = soupNew.new_tag(\"text\")\n",
    "            text_tag.string = \"#weight(\\n\" \n",
    "                    \n",
    "            text_tag.string += str(intCoeff0) + \" #combine(\" \n",
    "            for ow in set(origWords):\n",
    "                ow = regex.sub(' ', ow)\n",
    "                text_tag.string += ow + \" \"\n",
    "            text_tag.string += \")\\n\"\n",
    "\n",
    "            if len(relText_stringAll[0])>0:\n",
    "                if len(relText_stringAll[0][docno]) > 3: \n",
    "                    relText_string1 = relText_stringAll[0][docno]\n",
    "                    text_tag.string += str(intCoeff1) + \" #combine(\" \n",
    "                    #print(\"relText_string1 =\", relText_string1)\n",
    "                    text_tag.string += relText_string1.encode('utf-8')\n",
    "                    text_tag.string += \")\\n\"\n",
    "            \n",
    "            #print(len(relText_stringAll[1]))\n",
    "            if len(relText_stringAll[1])>0:\n",
    "                if len(relText_stringAll[1][docno]) > 3: \n",
    "                    relText_string2 = relText_stringAll[1][docno]\n",
    "                    text_tag.string += str(intCoeff2) + \" #combine(\" \n",
    "                    #print(\"relText_string2 =\", relText_string2)\n",
    "                    text_tag.string += relText_string2.encode('utf-8')\n",
    "                    text_tag.string += \")\\n\"\n",
    "            \n",
    "            #print(\"origWords =\", origWords)\n",
    "            #print(\"relWords_sel1 =\", relWords_sel1)\n",
    "            #print(\"relWords_sel2 =\", relWords_sel2)\n",
    "            \n",
    "            \n",
    "            text_tag.string += \") \"\n",
    "\n",
    "            doc_tag.append(text_tag)\n",
    "\n",
    "            parameters_tag.append(doc_tag)\n",
    "            #print(doc_tag)\n",
    "\n",
    "        rule_tag = soupNew.new_tag(\"rule\")\n",
    "        rule_tag.string = \"method:dir,mu:\" + str(dirCoeff)\n",
    "        #rule_tag.string = \"method:two\"\n",
    "        parameters_tag.append(rule_tag)\n",
    "\n",
    "        #intCoeff_tag = soupNew.new_tag(\"intCoeff\")\n",
    "        #intCoeff_tag.string = \"0.8\"\n",
    "        #parameters_tag.append(intCoeff_tag)\n",
    "\n",
    "        threads_tag = soupNew.new_tag(\"threads\")\n",
    "        threads_tag.string = \"32\"\n",
    "        parameters_tag.append(threads_tag)\n",
    "\n",
    "        count_tag = soupNew.new_tag(\"count\")\n",
    "        count_tag.string = \"1000\"\n",
    "        parameters_tag.append(count_tag)\n",
    "\n",
    "        trecFormat_tag = soupNew.new_tag(\"trecFormat\")\n",
    "        trecFormat_tag.string = \"true\"\n",
    "        parameters_tag.append(trecFormat_tag)\n",
    "\n",
    "        soupNew.append(parameters_tag)\n",
    "        #print(soupNew.prettify())\n",
    "    #print(\"outFileName =\", outFileName)\n",
    "    with open( cfgOutFileName, 'w') as outFile:\n",
    "        soupNewStr = str(soupNew)\n",
    "        soupNewStr = soupNewStr.replace(\"</text>\", \"\\n</text>\\n\").replace(\"query>\", \"query>\\n\").replace(\"<text>\", \"\\n<text>\\n\").replace(\"</index>\", \"</index>\\n\").replace(\"\\n<index>\", \"<index>\")\n",
    "\n",
    "        outFile.write(soupNewStr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def precisionCompute(count_history, intCoeffs0, intCoeffs1, intCoeffs2, expansionCounts1, expansionCounts2, dirCoeffs, N1s):\n",
    "    mapPrecs = dict()\n",
    "    for N1 in N1s:\n",
    "        for intCoeff0 in intCoeffs0:\n",
    "            for intCoeff1 in intCoeffs1:\n",
    "                for intCoeff2 in intCoeffs2:\n",
    "                    for expansionCount1 in expansionCounts1:\n",
    "                        for expansionCount2 in expansionCounts2:\n",
    "                            for dirCoeff in dirCoeffs:\n",
    "                                origWordsAll = analyseQueries()\n",
    "                                count_history, relText_stringAll = getAllRelWords(count_history, rWords, origWordsAll, expansionCount1, N1, expansionCount2, intCoeff0, intCoeff1, intCoeff2, dirCoeff)\n",
    "                                print(\"len(relText_stringAll[1]) =\", len(relText_stringAll[1]))\n",
    "                                genQueries(origWordsAll, relText_stringAll, intCoeff0, intCoeff1, intCoeff2, dirCoeff)\n",
    "                                subprocess.Popen(\"IndriRunQuery \" + cfgOutFileName + \" > \" + runsFileName, shell=True, stdout=subprocess.PIPE).stdout.read()\n",
    "                                cmd = \"trec_eval -q \" + colQrelsFileName + \" \" + runsFileName + \" > \" + evalsFileName\n",
    "                                subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE).stdout.read()\n",
    "                                cmd = \"cat \" + evalsFileName + \" | grep map | grep all | grep -v gm | awk '{print $3}' \"\n",
    "                                #print (\"cmd = \", cmd)\n",
    "                                mapPrec = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE).stdout.read()\n",
    "                                print(\"intCoeff0, intCoeff1, intCoeff2, expansionCount1, expansionCount2, dirCoeff, map precision, N1 =\", intCoeff0, intCoeff1, intCoeff2, expansionCount1, expansionCount2, dirCoeff, N1, mapPrec)\n",
    "                                #mapPrecs[intCoeff] = mapPrec\n",
    "    return count_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relWTopDocuments = [['FT941-5453', '1', '411'], ['FT921-6003', '2', '1252'], ['FT933-9176', '3', '826'], ['FBIS4-67534', '1', '400'], ['FBIS3-60235', '1', '237'], ['LA112890-0007', '1', '1143'], ['LA102090-0048', '5', '1879'], ['LA100189-0051', '3', '686'], ['LA021190-0107', '1', '710']]\n",
      "numerator, denumerator, topTermFrac = 18 8747 0.00205784840517\n",
      "relWTopDocuments = []\n",
      "numerator, denumerator, topTermFrac = 0.0 11361 0.0\n",
      "relWTopDocuments = [['FT941-15416', '4', '409'], ['FT941-5453', '3', '411'], ['FT921-6003', '7', '1252'], ['FT933-7330', '3', '1540'], ['FT933-9176', '4', '826'], ['FBIS3-41533', '8', '476'], ['FBIS4-67534', '2', '400'], ['LA112890-0007', '2', '1143'], ['LA021190-0107', '2', '710'], ['LA050889-0063', '11', '1213']]\n",
      "numerator, denumerator, topTermFrac = 46 8370 0.00549581839904\n",
      "relWTopDocuments = [['LA082189-0001', '1', '2833'], ['LA082390-0149', '1', '431'], ['LA122790-0033', '1', '900'], ['LA111190-0042', '1', '1928'], ['LA101890-0141', '1', '725']]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-97-6ac7dc282bfb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m#mapPrecs =\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mrWords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mcount_history\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprecisionCompute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcount_history\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mintCoeffs0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mintCoeffs1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mintCoeffs2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpansionCounts1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpansionCounts2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdirCoeffs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN1s\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-92-c786b48fcaeb>\u001b[0m in \u001b[0;36mprecisionCompute\u001b[1;34m(count_history, intCoeffs0, intCoeffs1, intCoeffs2, expansionCounts1, expansionCounts2, dirCoeffs, N1s)\u001b[0m\n\u001b[0;32m      9\u001b[0m                             \u001b[1;32mfor\u001b[0m \u001b[0mdirCoeff\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdirCoeffs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m                                 \u001b[0morigWordsAll\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0manalyseQueries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m                                 \u001b[0mcount_history\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrelText_stringAll\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetAllRelWords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcount_history\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrWords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morigWordsAll\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpansionCount1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpansionCount2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mintCoeff0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mintCoeff1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mintCoeff2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdirCoeff\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m                                 \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"len(relText_stringAll[1]) =\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrelText_stringAll\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m                                 \u001b[0mgenQueries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morigWordsAll\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrelText_stringAll\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mintCoeff0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mintCoeff1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mintCoeff2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdirCoeff\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-90-c371c0317940>\u001b[0m in \u001b[0;36mgetAllRelWords\u001b[1;34m(count_history, rWords, origWordsAll, expansionCount1, N1, expansionCount2, intCoeff0, intCoeff1, intCoeff2, dirCoeff)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mrelWords_sel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mdocno\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morigWords\u001b[0m \u001b[1;32min\u001b[0m \u001b[0morigWordsAll\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mcount_history\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrelText_string_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrelWords_sel_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetRelWords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcount_history\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrWords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdocno\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morigWords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpansionCount1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mintCoeff0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mintCoeff1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mintCoeff2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdirCoeff\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0mrelText_string\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdocno\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrelText_string_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mrelWords_sel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdocno\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrelWords_sel_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mN1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-89-d0876a52a2c5>\u001b[0m in \u001b[0;36mgetRelWords\u001b[1;34m(count_history, rWords, docno, origWords, expansionCount, intCoeff0, intCoeff1, intCoeff2, dirCoeff)\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[1;31m#print(ow, \"---\", conceptnet5RelAll[ow])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mrelW\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mconceptnet5RelAll\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mow\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m                 \u001b[0mrelWg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweightRelConcept\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocno\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morigWords_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrelW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mintCoeff0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mintCoeff1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mintCoeff2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdirCoeff\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mrelW\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrelWords\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m                     \u001b[0mrelWords\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrelW\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mrelWg\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-96-6d47b0b51945>\u001b[0m in \u001b[0;36mweightRelConcept\u001b[1;34m(docno, origWords_, ow, relW, intCoeff0, intCoeff1, intCoeff2, dirCoeff)\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"relWTopDocuments =\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrelWTopDocuments\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0mnumerator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrelWTopDocuments\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[0mdenumerator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdocumentLength\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtopTDocs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m     \u001b[0mtopTermFrac\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumerator\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdenumerator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"numerator, denumerator, topTermFrac =\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumerator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdenumerator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopTermFrac\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-96-6d47b0b51945>\u001b[0m in \u001b[0;36mdocumentLength\u001b[1;34m(documentName)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdocumentLength\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocumentName\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mdocumentLength_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"dumpindex\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolIndexDir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"dv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdocIdNameMap\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdocumentName\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"|\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"awk\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\\'END{print $1}\\'\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshell\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocumentLength_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"docIdNameMap[\\\"FT944-15973\\\"] = \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdocIdNameMap\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"FT944-15973\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/fj9124/anaconda2/lib/python2.7/subprocess.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags)\u001b[0m\n\u001b[0;32m    708\u001b[0m                                 \u001b[0mp2cread\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp2cwrite\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    709\u001b[0m                                 \u001b[0mc2pread\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc2pwrite\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 710\u001b[1;33m                                 errread, errwrite)\n\u001b[0m\u001b[0;32m    711\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    712\u001b[0m             \u001b[1;31m# Preserve original exception in case os.close raises.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/fj9124/anaconda2/lib/python2.7/subprocess.pyc\u001b[0m in \u001b[0;36m_execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, cwd, env, universal_newlines, startupinfo, creationflags, shell, to_close, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite)\u001b[0m\n\u001b[0;32m   1229\u001b[0m                     \u001b[0mgc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1230\u001b[0m                     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1231\u001b[1;33m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1232\u001b[0m                     \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1233\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[0mgc_was_enabled\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "intCoeffs0 = [ 0.7 ]\n",
    "intCoeffs1 = [ 0.3 ]\n",
    "#intCoeffs2 = [ 0.18 ]\n",
    "intCoeffs2 = [ 0.5 ]\n",
    "#intCoeffs1 = np.arange(0.1, 1, 0.1)\n",
    "expansionCounts1 = [ 1000 ]\n",
    "#expansionCounts2 = [ 145 ]\n",
    "expansionCounts2 = [ 0 ]\n",
    "#expansionCounts2 = range(15, 220, 5)\n",
    "dirCoeffs = [ 1600 ]\n",
    "#dirCoeffs = range(200, 4000, 200)\n",
    "N1s = [0]\n",
    "#N1s = range(1, 30, 1)\n",
    "#mapPrecs = \n",
    "rWords = dict()\n",
    "count_history = precisionCompute(count_history, intCoeffs0, intCoeffs1, intCoeffs2, expansionCounts1, expansionCounts2, dirCoeffs, N1s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
