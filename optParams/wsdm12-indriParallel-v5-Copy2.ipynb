{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function \n",
    "from bs4 import BeautifulSoup\n",
    "from BeautifulSoup import SoupStrainer as sopstrain\n",
    "import subprocess\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import numpy as np\n",
    "import string\n",
    "import operator\n",
    "import csv\n",
    "import urllib2\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import StringIO\n",
    "import random\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cfgInFileName = /home/fj9124/projects/ir/seq_kb_ir/configs/robust/queryRobust\n",
      "cfgOutFileName = /home/fj9124/projects/ir/seq_kb_ir/configs/robust/lr/cnet/indriRunQuery.cfg\n",
      "colIndexDir = /scratch/index/indri_5_7/robust\n",
      "knowledgGraph index Dir = /scratch/index/indri_5_7/robust\n",
      "col Qrels File Name = /home/fj9124/projects/ir/seq_kb_ir/qrels/robust/qrels.csv\n",
      "runsFileName = /home/fj9124/projects/ir/seq_kb_ir/runs/lr/robust/cnet/indriRunQuery.runs\n",
      "evalsFileName = /home/fj9124/projects/ir/seq_kb_ir/evals/lr/robust/cnet/indriRunQuery.evals\n",
      "dumpindexStatementFilename = /tmp/sbsb/statement_robust_lr_cnet\n",
      "occuranceCountFilename = /home/fj9124/projects/ir/seq_kb_ir/occuranceCount/occuranceCount\n",
      "countsResultsFile = /home/fj9124/projects/ir/seq_kb_ir/occuranceCount/results/robust.txt\n",
      "conceptnet5RelAllFilename = /scratch/saeid/data/conceptnet5_simp.csv\n",
      "cfgTmpOutFileName = /home/fj9124/projects/ir/seq_kb_ir/configs/robust/lr/cnet/indriRunQuery.cfg.tmp\n",
      "docIdNameMapFileName = /home/fj9124/projects/ir/seq_kb_ir/occuranceCount/results/robust_docIdNameMap.txt\n"
     ]
    }
   ],
   "source": [
    "knowledgGraph = \"cnet\" # conceptnet5AssocMod, gov, conceptnet5AssocMi, conceptnet5AssocHdl\n",
    "#knowledgGraph_ = ''.join([k.capitalize() if i>0 else k for i, k in enumerate(knowledgGraph)])\n",
    "collection = \"robust\"\n",
    "method = \"lr\" # hal, mi, assoc, assocMi, assocHal, assoc2\n",
    "simMeasure = \"\" # mi, cnet\n",
    "projectDir = \"/home/fj9124/projects/ir/seq_kb_ir/\" \n",
    "indexDir = \"/scratch/index/indri_5_7/\"\n",
    "colMethodConfigsDir = os.path.join(projectDir, \"configs\", collection, method)\n",
    "cfgInFileName = os.path.join(projectDir, \"configs\", collection, \"query\" + collection.capitalize()) \n",
    "print(\"cfgInFileName =\", cfgInFileName)\n",
    "cfgOutFileName=os.path.join(colMethodConfigsDir, knowledgGraph, \"indriRunQuery.cfg\") \n",
    "print(\"cfgOutFileName =\", cfgOutFileName)\n",
    "colIndexDir = os.path.join(indexDir, collection) \n",
    "print(\"colIndexDir =\", colIndexDir)\n",
    "if knowledgGraph in {\"cnet\"}:\n",
    "    knowledgGraphIndexDir = os.path.join(indexDir, collection)   \n",
    "else:\n",
    "    knowledgGraphIndexDir = os.path.join(indexDir, knowledgGraph)   \n",
    "print(\"knowledgGraph index Dir =\", knowledgGraphIndexDir)\n",
    "graphsDir = os.path.join(projectDir, \"graphs\")\n",
    "#methodGraphsDir = os.path.join(graphsDir, method)\n",
    "methodGraphsFileName = []\n",
    "if method == \"hal\":\n",
    "    methodGraphsFileName = [os.path.join(graphsDir, method, knowledgGraph + \".txt\")]\n",
    "    print(\"methodGraphsFileName =\", methodGraphsFileName)\n",
    "elif method.translate(None, string.digits) in {\"mi\", \"assoc\"}:\n",
    "    methodGraphsFileName = [os.path.join(graphsDir, method.translate(None, string.digits), knowledgGraph, \"graph.txt\")]\n",
    "    print(\"methodGraphsFileName =\", methodGraphsFileName)\n",
    "elif method in {\"assocMi\"}:\n",
    "    if knowledgGraph == \"conceptnet5AssocGov\":\n",
    "        methodGraphsFileName = [os.path.join(graphsDir, \"assoc\", collection, \"conceptnet5AssocMod\" + \".txt\")]\n",
    "        methodGraphsFileName += [os.path.join(graphsDir, \"mi\", collection, \"gov\" + \".txt\")]\n",
    "        print(\"methodGraphsFileName =\", methodGraphsFileName)\n",
    "elif method in {\"assocHal\"}:\n",
    "    if knowledgGraph == \"conceptnet5AssocGov\":\n",
    "        methodGraphsFileName = [os.path.join(graphsDir, \"assoc\", collection, \"conceptnet5AssocMod\" + \".txt\")]\n",
    "        methodGraphsFileName += [os.path.join(graphsDir, \"hal\", \"gov\" + \".txt\")]\n",
    "        print(\"methodGraphsFileName =\", methodGraphsFileName)\n",
    "qrelsDir = os.path.join(projectDir, \"qrels\")\n",
    "colQrelsDir = os.path.join(qrelsDir, collection)\n",
    "colQrelsFileName = os.path.join(colQrelsDir, \"qrels.csv\")\n",
    "print(\"col Qrels File Name =\", colQrelsFileName)\n",
    "runsFileName = os.path.join(projectDir, \"runs\", method, collection, knowledgGraph, \"indriRunQuery.runs\") \n",
    "print(\"runsFileName =\", runsFileName)\n",
    "evalsFileName = os.path.join(projectDir, \"evals\", method, collection, knowledgGraph, \"indriRunQuery.evals\") \n",
    "print(\"evalsFileName =\", evalsFileName)\n",
    "dumpindexStatementFilename = os.path.join(\"/\", \"tmp\", \"sbsb\", \"statement\" + \"_\" + collection + \"_\" + method + \"_\" + knowledgGraph )\n",
    "print(\"dumpindexStatementFilename =\", dumpindexStatementFilename)\n",
    "occuranceCountFilename = os.path.join(projectDir, \"occuranceCount\", \"occuranceCount\")\n",
    "print(\"occuranceCountFilename =\", occuranceCountFilename)\n",
    "countsResultsFile = os.path.join(projectDir,\"occuranceCount\",\"results\",collection+\".txt\")\n",
    "print(\"countsResultsFile =\", countsResultsFile)\n",
    "conceptnet5RelAllFilename =\"/scratch/saeid/data/conceptnet5_simp.csv\"\n",
    "print(\"conceptnet5RelAllFilename =\", conceptnet5RelAllFilename)\n",
    "cfgTmpOutFileName = cfgOutFileName + \".tmp\"\n",
    "print(\"cfgTmpOutFileName =\", cfgTmpOutFileName)\n",
    "docIdNameMapFileName = os.path.join(projectDir, \"occuranceCount\", \"results\", collection + \"_docIdNameMap.txt\")\n",
    "print(\"docIdNameMapFileName =\", docIdNameMapFileName)\n",
    "#dumpindexResFilename = os.path.join(projectDir,\"occuranceCount\",\"results\",collection +\"_dumpindexRes\"+\".txt\")\n",
    "#print(\"dumpindexResFilename =\", dumpindexResFilename)\n",
    "#expansionCount = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generate docIdNameMapFileName\n",
    "if not os.path.isfile(occuranceCountFilename):\n",
    "    cmd = ' '.join([occuranceCountFilename, colIndexDir, \"dm\", \">\", docIdNameMapFileName])\n",
    "    res = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE).stdout.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', 'FT924-6589']\n",
      "FBIS3-32596 \t 100696\n",
      "LA011890-0198 \t 509068\n",
      "LA011890-0199 \t 509069\n",
      "LA011890-0196 \t 509066\n",
      "LA011890-0197 \t 509067\n",
      "LA011890-0194 \t 509064\n",
      "FBIS3-32597 \t 100697\n",
      "LA011890-0192 \t 509062\n",
      "LA011890-0193 \t 509063\n",
      "LA011890-0190 \t 509060\n",
      "LA011890-0191 \t 509061\n",
      "FBIS3-32598 \t 100698\n",
      "FT924-2868 \t 127475\n",
      "FT924-2869 \t 127476\n",
      "FT932-3797 \t 51326\n",
      "FT924-2864 \t 127471\n",
      "FT924-2865 \t 127472\n",
      "FT924-2866 \t 127473\n",
      "FT924-2867 \t 127474\n",
      "FT924-2860 \t 127467\n",
      "FT924-2861 \t 127468\n",
      "FT924-2862 \t 127469\n"
     ]
    }
   ],
   "source": [
    "with open(docIdNameMapFileName, 'r') as f:\n",
    "    reader = list(csv.reader(f, delimiter = ' '))\n",
    "    print(reader[0])\n",
    "    docIdNameMap = {i[1]:i[0] for i in reader}\n",
    "    docNameIdMap = {i[0]:i[1] for i in reader}\n",
    "for i, (k, v) in enumerate(docIdNameMap.iteritems()):\n",
    "    print (k, '\\t', v)\n",
    "    if (i>20):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count_history = dict()\n",
    "if os.path.isfile(countsResultsFile):\n",
    "    with open(countsResultsFile, 'r') as f:\n",
    "        reader = csv.reader(f, delimiter = \"\\t\")\n",
    "        count_history = {k:int(float(v)) for k,v in list(reader)}\n",
    "    print(\"size of count_history =\", len(count_history))\n",
    "    for i, (k, v) in enumerate(count_history.iteritems()):\n",
    "        print (k, '\\t', v)\n",
    "        if (i>20):\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of conceptnet5RelAll = 2965684\n",
      "tripolitan \t tripoline \t Synonym\n",
      "tripolitan \t tripoli \t RelatedTo\n",
      "age of nemesis \t band \t IsA\n",
      "age of nemesis \t organisation \t InstanceOf\n",
      "age of nemesis \t progressive rock \t dbpedia/genre\n",
      "age of nemesis \t progressive metal \t dbpedia/genre\n",
      "joseph john annabring \t person \t InstanceOf\n",
      "british rail class 438 \t mean of transportation \t InstanceOf\n",
      "british rail class 438 \t train \t InstanceOf\n"
     ]
    }
   ],
   "source": [
    "conceptnet5RelAll = defaultdict(dict)\n",
    "conceptnet5RelAllInv = defaultdict(dict)\n",
    "with open(conceptnet5RelAllFilename, 'r') as f:\n",
    "    reader = csv.reader(f, delimiter = \",\")\n",
    "    for line in list(reader):\n",
    "        if line[0].strip() != \"\" and line[1].strip() != \"\" and line[2].strip() != \"\":\n",
    "            if all(c in string.printable for c in line[0]) and all(c in string.printable for c in line[1]) and all(c in string.printable for c in line[2]):\n",
    "                conceptnet5RelAll[line[1].strip()][line[2].strip()] = line[0].strip()\n",
    "                conceptnet5RelAllInv[line[2].strip()][line[1].strip()] = line[0].strip()\n",
    "print(\"size of conceptnet5RelAll =\", len(conceptnet5RelAll))\n",
    "for i, (k1, k2v) in enumerate(conceptnet5RelAll.iteritems()):\n",
    "    for j, (k2, v) in enumerate(k2v.iteritems()):\n",
    "        if (i>3 or j>3):\n",
    "            break\n",
    "        print (k1, '\\t', k2, '\\t', v)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing: ['go', 'read']\n"
     ]
    }
   ],
   "source": [
    "def splitStemText(text):\n",
    "    stemmedWords = []\n",
    "    text = re.sub('/|-|\\\"|_',' ', text) # replace - and slash with space\n",
    "    if method in {\"assocRestful\", \"lr\"}:\n",
    "        for text_ in text.split(' '):\n",
    "            text = text_.replace(\" \", \"_\")\n",
    "            url = \"http://conceptnet5.media.mit.edu/data/5.4/uri?language=en&text=\" + text\n",
    "            response = urllib2.urlopen(url)\n",
    "            data = response.read()\n",
    "            data_j = json.loads(data)\n",
    "            text = os.path.basename(data_j[u'uri'])\n",
    "            url = \"http://conceptnet5.media.mit.edu/data/5.4/uri?language=en&text=\" + text\n",
    "            response = urllib2.urlopen(url)\n",
    "            data = response.read()\n",
    "            data_j = json.loads(data)\n",
    "            #print(\"data_j =\", data_j)\n",
    "            #print(\"data_j[u'uri'] =\", data_j[u'uri'])\n",
    "            stemmedWords += [str(os.path.basename(data_j[u'uri']))]\n",
    "        #print(\"stemmedWords =\", stemmedWords)\n",
    "        return (stemmedWords)\n",
    "    else:\n",
    "        words = text.split()\n",
    "        for w in words:\n",
    "            w = re.sub('\\(|\\)|\\'s|,','', w) # remove paranthesis, apostrophe s, comma\n",
    "            w = re.sub('\\'','', w) # remove apostrophe\n",
    "            cmd = \"dumpindex \" + knowledgGraphIndexDir + \" t \" + w + \" | head -n1\"\n",
    "            #print(\"cmd =\", cmd)\n",
    "            stemmedWords.append(subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE).stdout.read().split()[1])\n",
    "        return (stemmedWords)\n",
    "\n",
    "print (\"testing:\", splitStemText(\"going reading\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def analyseQueries():\n",
    "    #    #print(\"intCoeff =\", intCoeff)\n",
    "    with open(cfgInFileName, 'r') as inFile:\n",
    "        reader = inFile.read()\n",
    "        soup = BeautifulSoup(reader, 'lxml')\n",
    "        if collection in {\"aquaint\", \"gov\"}:\n",
    "            tags = soup.find_all(['doc'])\n",
    "        elif collection in {\"robust\", \"trec7n8\", \"gov2\"}:\n",
    "            tags = soup.find_all(['top'])\n",
    "            #print(tags)\n",
    "        \n",
    "        origWordsAll = dict()\n",
    "        for tag in tags:\n",
    "            \n",
    "            if collection in {\"aquaint\", \"gov\"}:\n",
    "                docno = (tag.find('docno').string).strip()\n",
    "            elif collection in {\"robust\", \"trec7n8\", \"gov2\"}:\n",
    "                result = re.search('<num> Number: (.*)\\n', str(tag))                \n",
    "                docno = result.group(1).strip()\n",
    "            \n",
    "            if collection in {\"aquaint\", \"gov\"}:\n",
    "                origWords = splitStemText(tag.find('text').string.strip())\n",
    "            elif collection in {\"robust\", \"trec7n8\", \"gov2\"}:\n",
    "                result = re.search('<title>(.*)\\n', str(tag))                \n",
    "                origWords = splitStemText(result.group(1).strip())\n",
    "            #print(\"origWords =\", origWords)\n",
    "            origWordsAll[docno] = origWords\n",
    "            \n",
    "    return (origWordsAll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dumpindex_colIndexDir_hist = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "colDocumentCount = 528155\n"
     ]
    }
   ],
   "source": [
    "def dumpindex_colIndexDir(cmd2):\n",
    "    if ' '.join(cmd2) in dumpindex_colIndexDir_hist:\n",
    "        #print(\"exists!\")\n",
    "        return dumpindex_colIndexDir_hist[' '.join(cmd2)]\n",
    "    else:\n",
    "        #print(\"do not exists!\")\n",
    "        cmd = ' '.join([\"dumpindex\", colIndexDir] + cmd2)\n",
    "        #print(\"cmd =\", cmd)\n",
    "        res = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE).stdout.read()\n",
    "        #dumpindex_colIndexDir_hist[' '.join(cmd2)] = res\n",
    "        #with open(cmdResFilename, \"a\") as f:\n",
    "        #    f.write(' '.join(cmd2), \"\\t\", res) \n",
    "        return res\n",
    "\n",
    "cmd2 = [\"s\", \"|\", \"awk\", \"\\'NR==2{print $2}\\'\" ]\n",
    "colDocumentCount = int(np.float(dumpindex_colIndexDir(cmd2)))\n",
    "print(\"colDocumentCount =\", colDocumentCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indriRunQuery_hist = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "def genMultSimpQuery(argss):\n",
    "    #    #print(\"intCoeff =\", intCoeff)\n",
    "    soupNew = BeautifulSoup(\"\", 'lxml')\n",
    "    parameters_tag = soupNew.new_tag(\"parameters\")\n",
    "    index_tag = soupNew.new_tag(\"index\")\n",
    "    index_tag.string = colIndexDir\n",
    "    parameters_tag.append(index_tag)\n",
    "        #print(tags)\n",
    "\n",
    "    for args in argss:\n",
    "        #print(\"args =\", args)\n",
    "        (docno, origWords_, relW, intCoeff0, intCoeff1, intCoeff2, dirCoeff, featureWeights) = args\n",
    "        #print(\"(...) =\", docno, origWords_, intCoeff0, intCoeff1, intCoeff2, dirCoeff, featureWeights)\n",
    "        doc_tag = soupNew.new_tag(\"query\")\n",
    "\n",
    "        docno_tag = soupNew.new_tag(\"number\")\n",
    "        docno_tag.string = '_'.join([docno, ('_'.join(origWords_)).replace(' ', '_'), relW.replace(' ', '_')])\n",
    "        doc_tag.append(docno_tag)\n",
    "\n",
    "        text_tag = soupNew.new_tag(\"text\")\n",
    "        text_tag.string = \"#weight(\\n\" \n",
    "\n",
    "        text_tag.string += str(intCoeff0) + \" #combine(\" \n",
    "        for ow in set(origWords_):\n",
    "            ow = regex.sub(' ', ow)\n",
    "            text_tag.string += ow + \" \"\n",
    "        text_tag.string += \")\\n\"\n",
    "\n",
    "        if len(relW)>0: \n",
    "            relText_string1 = regex.sub(' ', relW)\n",
    "            text_tag.string += str(intCoeff1) + \" #combine(\" \n",
    "            #print(\"relText_string1 =\", relText_string1)\n",
    "            text_tag.string += relText_string1.encode('utf-8')\n",
    "            text_tag.string += \")\\n\"\n",
    "\n",
    "        text_tag.string += \") \"\n",
    "\n",
    "        doc_tag.append(text_tag)\n",
    "\n",
    "        parameters_tag.append(doc_tag)\n",
    "        #print(doc_tag)\n",
    "\n",
    "    rule_tag = soupNew.new_tag(\"rule\")\n",
    "    rule_tag.string = \"method:dir,mu:\" + str(dirCoeff)\n",
    "    #rule_tag.string = \"method:two\"\n",
    "    parameters_tag.append(rule_tag)\n",
    "\n",
    "    #intCoeff_tag = soupNew.new_tag(\"intCoeff\")\n",
    "    #intCoeff_tag.string = \"0.8\"\n",
    "    #parameters_tag.append(intCoeff_tag)\n",
    "\n",
    "    threads_tag = soupNew.new_tag(\"threads\")\n",
    "    threads_tag.string = \"32\"\n",
    "    parameters_tag.append(threads_tag)\n",
    "\n",
    "    count_tag = soupNew.new_tag(\"count\")\n",
    "    count_tag.string = \"10\"\n",
    "    parameters_tag.append(count_tag)\n",
    "\n",
    "    trecFormat_tag = soupNew.new_tag(\"trecFormat\")\n",
    "    trecFormat_tag.string = \"true\"\n",
    "    parameters_tag.append(trecFormat_tag)\n",
    "\n",
    "    soupNew.append(parameters_tag)\n",
    "    #print(soupNew.prettify())\n",
    "    #print(\"cfgTmpOutFileName =\", cfgTmpOutFileName)\n",
    "    with open( cfgTmpOutFileName, 'a+') as outFile:\n",
    "        soupNewStr = str(soupNew)\n",
    "        soupNewStr = soupNewStr.replace(\"</text>\", \"\\n</text>\\n\").replace(\"query>\", \"query>\\n\").replace(\"<text>\", \"\\n<text>\\n\").replace(\"</index>\", \"</index>\\n\").replace(\"\\n<index>\", \"<index>\")\n",
    "\n",
    "        outFile.write(soupNewStr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def weightRelConcept_hist_i(docno, origWords_, ow, relW, intCoeff0, intCoeff1, intCoeff2, dirCoeff, featureWeights):\n",
    "    \n",
    "    indriRunQuery_hist = [(docno, origWords_, relW, intCoeff0, intCoeff1, intCoeff2, dirCoeff, cfgTmpOutFileName)]\n",
    "    return indriRunQuery_hist\n",
    "\n",
    "\n",
    "indriRunQuery_hist = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indriRunQuery_hist_= dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def runCommandsinHist_i(cfgTmpOutFileName, indriRunQuery_hist, indriRunQuery_hist_, featureWeights):\n",
    "    with open( cfgTmpOutFileName, 'w+') as outFile:\n",
    "        outFile.write(\"\")\n",
    "    args = []\n",
    "    #print(\"indriRunQuery_hist =\", indriRunQuery_hist)\n",
    "    for (docno, origWords_, relW, intCoeff0, intCoeff1, intCoeff2, dirCoeff, cfgTmpOutFileName) in indriRunQuery_hist:\n",
    "        #print(\"cfgTmpOutFileName =\", cfgTmpOutFileName)\n",
    "        if '_'.join([docno, ('_'.join(origWords_)).replace(' ', '_'), relW.replace(' ', '_')]) not in indriRunQuery_hist_:\n",
    "            args += [(docno, origWords_, relW, intCoeff0, intCoeff1, intCoeff2, dirCoeff, featureWeights)]\n",
    "    #print(\"args =\", args)\n",
    "    if len(args)>0:\n",
    "        genMultSimpQuery(args)\n",
    "    cmd = ' '.join([\"IndriRunQuery\", cfgTmpOutFileName])\n",
    "    #print(\"cmd =\", cmd)\n",
    "    indriRun = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE).stdout.read()\n",
    "    #print(\"indriRun =\", indriRun)\n",
    "    indriRunQuery_hist_tmp = [[j for c, j in enumerate(i.split(' ')) if c in {0, 2, 4}] for i in indriRun.split('\\n') if len(i)>0]\n",
    "    #indriRunQuery_hist_ = dict()\n",
    "    for i in indriRunQuery_hist_tmp:\n",
    "        if i[0] in indriRunQuery_hist_:\n",
    "            indriRunQuery_hist_[i[0]] += [[i[1], i[2]]]\n",
    "        else:\n",
    "            indriRunQuery_hist_[i[0]] = [[i[1], i[2]]]\n",
    "    #indriRunQuery_hist_ = list(set(indriRunQuery_hist_))\n",
    "    return indriRunQuery_hist_\n",
    "    #dumpindex_colIndexDir_hist\n",
    "    #with open\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def weightRelConcept_hist_d(indriRunQuery_hist_, dumpindex_colIndexDir_hist, docno, origWords_, ow, relW, intCoeff0, intCoeff1, intCoeff2, dirCoeff, featureWeights):\n",
    "\n",
    "    #print(indriRunQuery_hist_)\n",
    "    #print(\"'_'.join([docno, '_'.join(origWords_), relW]).replace(' ', '_') =\", '_'.join([docno, '_'.join(origWords_), relW]).replace(' ', '_'))\n",
    "    indriRunQuery_hist_1 = indriRunQuery_hist_['_'.join([docno, '_'.join(origWords_), relW]).replace(' ', '_')]\n",
    "    \n",
    "    topTDocs = [i[0] for i in indriRunQuery_hist_1]\n",
    "    \n",
    "    cmd2 = (\"efb\", \"#od4(\" + regex.sub(' ', relW) + \"):\" + ','.join(topTDocs))\n",
    "    if cmd2 not in dumpindex_colIndexDir_hist:\n",
    "        dumpindex_colIndexDir_hist.add(cmd2)\n",
    "    \n",
    "    #print(\"topTDocs =\", topTDocs)\n",
    "    \n",
    "    for documentName in topTDocs:\n",
    "        cmd2 = (\"dcf\", docIdNameMap[documentName])\n",
    "        if cmd2 not in dumpindex_colIndexDir_hist:\n",
    "            dumpindex_colIndexDir_hist.add(cmd2)\n",
    "\n",
    "    for ow in origWords_:\n",
    "        cmd2 = (\"efb\", \"#uw(#4( \" + regex.sub(' ', ow) + \" ) #4( \" + regex.sub(' ', relW) + \" )):\" + ','.join(topTDocs))\n",
    "        if cmd2 not in dumpindex_colIndexDir_hist:\n",
    "            dumpindex_colIndexDir_hist.add(cmd2)\n",
    "        \n",
    "    for j1, ow1 in enumerate(origWords_):\n",
    "        for j2, ow2 in enumerate(origWords_):\n",
    "            if j1>j2:\n",
    "                cmd2 = ( \"efb\", \"#uw(#4( \" + regex.sub(' ', ow1) + \" ) #4( \" + regex.sub(' ', ow2) + \" ) #4( \" + regex.sub(' ', relW) + \" )):\" + ','.join(topTDocs))\n",
    "                if cmd2 not in dumpindex_colIndexDir_hist:\n",
    "                    dumpindex_colIndexDir_hist.add(cmd2)\n",
    "    return dumpindex_colIndexDir_hist\n",
    "\n",
    "dumpindex_colIndexDir_hist = set()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dumpindex_hist_ = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "def runCommandsinHist_d(dumpindex_colIndexDir_hist_list, dumpindex_hist_):\n",
    "    args = []\n",
    "    dumpindex_colIndexDir_hist_dict = dict()\n",
    "    tmp = set()\n",
    "    #print(\"len(dumpindex_colIndexDir_hist_list) =\", len(dumpindex_colIndexDir_hist_list))\n",
    "    for i in dumpindex_colIndexDir_hist_list:\n",
    "        #print(\"i =\", i)\n",
    "        if i[1] not in dumpindex_hist_.get(i[0], dict()):\n",
    "            #print(\"dumpindex_hist_.get(i[0], dict()).keys() =\", dumpindex_hist_.get(i[0], dict()).keys())\n",
    "            if i[1] in tmp:\n",
    "                continue\n",
    "            tmp.add(i[1])\n",
    "            #print(\"i[1] =\", i[1])\n",
    "            #print(i, \"not exists!\")\n",
    "            if i[0] in dumpindex_colIndexDir_hist_dict:\n",
    "                dumpindex_colIndexDir_hist_dict[i[0]] += [[re.sub(ur\"[^\\w\\d#(),:\\-\\s]+\",' ',j) for j in i[1:]]]\n",
    "            else:\n",
    "                dumpindex_colIndexDir_hist_dict[i[0]] =  [[re.sub(ur\"[^\\w\\d#(),:\\-\\s]+\",' ',j) for j in i[1:]]]\n",
    "    #print(dumpindex_colIndexDir_hist_dict)\n",
    "        #else:\n",
    "        #    print(\"exists!\")\n",
    "    #print(\"dumpindex_colIndexDir_hist_ =\", dumpindex_colIndexDir_hist_)\n",
    "    #print(\"len(dumpindex_colIndexDir_hist_.get('ef', dict()) =\", len(dumpindex_colIndexDir_hist_.get('ef', dict())))\n",
    "    #dumpindex_hist_ = dict()\n",
    "    for k, v in dumpindex_colIndexDir_hist_dict.iteritems():\n",
    "        fileName = dumpindexStatementFilename+\"_\"+k+\".tmp\"\n",
    "        with open(fileName, 'w') as f:\n",
    "            for i in v:\n",
    "                f.write(' '.join(i) + '\\n')\n",
    "        cmd = ' '.join([occuranceCountFilename, colIndexDir, k, fileName])\n",
    "        #print(\"cmd =\", cmd)\n",
    "        dumpindex_hist_tmp = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE).stdout.read()\n",
    "        dumpindex_hist_tmp = dict([tuple(j for j in i.split(':')) for i in dumpindex_hist_tmp.split(\"\\n\") if len(i)>0])\n",
    "        dumpindex_hist_tmp = {k1:v1.split(',') for k1,v1 in dumpindex_hist_tmp.iteritems()}\n",
    "        dumpindex_hist_tmp = {k1:[i for i in v1 if len(i)>0] for k1,v1 in dumpindex_hist_tmp.iteritems()}\n",
    "        #dumpindex_hist_tmp = {re.sub(ur\"[^\\w\\d#()\\s]+\",' ',k1):[i for i in v1 if len(i)>0] for k1,v1 in dumpindex_hist_tmp.iteritems()}\n",
    "        #dumpindex_tmp = [i for i in dumpindex_tmp.split(\"\\n\")]\n",
    "        #print(\"dumpindex_tmp =\", dumpindex_tmp)\n",
    "        #dumpindex_hist_[k] = dumpindex_hist_tmp\n",
    "        dumpindex_hist_[k] = dict(dumpindex_hist_.get(k, dict()).items() + dumpindex_hist_tmp.items())\n",
    "        #print(\"dumpindex_hist_[k].keys() =\", dumpindex_hist_[k].keys())\n",
    "        #print(\"dumpindex_hist_tmp =\", dumpindex_hist_tmp)\n",
    "        \n",
    "    return dumpindex_hist_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "def documentLength(documentName):\n",
    "    #cmd = ' '.join([\"dumpindex\", colIndexDir, \"dv\", docIdNameMap[documentName], \"|\", \"awk\", \"\\'END{print $1}\\'\"])\n",
    "    #cmd2 = [\"dv\", docIdNameMap[documentName], \"|\", \"awk\", \"\\'END{print $1}\\'\"]\n",
    "    #print(\"cmd =\", cmd)\n",
    "    #documentLength_ = int(dumpindex_colIndexDir(cmd2))\n",
    "    documentLength_ = int(dumpindex_hist_['dcf'][documentName][0])\n",
    "    return documentLength_\n",
    "\n",
    "#print(\"docIdNameMap[\\\"FT944-15973\\\"] = \", docIdNameMap[\"FT944-15973\"])\n",
    "#print(\"documentLength(\\\"FT944-15973\\\") =\", documentLength(\"FT944-15973\"))\n",
    "\n",
    "def weightRelConcept(docno, origWords_, ow, relW, intCoeff0, intCoeff1, intCoeff2, dirCoeff, count_history, featureWeights):\n",
    "    \n",
    "    features = dict()\n",
    "    \n",
    "    ##########\n",
    "    #cfgTmpOutFileName = os.path.join(\"/\",\"tmp\",\"sbsb\",str(random.randint(1,1e6))+\"_tmp.cfg\")\n",
    "    ##########\n",
    "    #(docno, origWords_, relW, intCoeff0, intCoeff1, dirCoeff, cfgTmpOutFileName)\n",
    "    #genOneSimpQuery(docno, origWords_, relW, intCoeff0, intCoeff1, dirCoeff, cfgTmpOutFileName)\n",
    "    \n",
    "    #indriRun = indriRunQuery_([cfgTmpOutFileName])\n",
    "    #subprocess.Popen(' '.join([\"rm\", cfgTmpOutFileName]), shell=True, stdout=subprocess.PIPE).stdout.read()\n",
    "    #indriRun = [i.split(' ') for i in indriRun.split('\\n')]\n",
    "    #indriRun = [i for i in indriRun if len(i)>1] # remove empty arrays\n",
    "    #print(\"'_'.join([docno, '_'.join(origWords_), relW]).replace(' ', '_') =\", '_'.join([docno, '_'.join(origWords_), relW]).replace(' ', '_'))\n",
    "    indriRunQuery_hist_1 = indriRunQuery_hist_['_'.join([docno, '_'.join(origWords_), relW]).replace(' ', '_')]\n",
    "    #print(\"indriRunQuery_hist_1 =\", indriRunQuery_hist_1)\n",
    "    topTDocs = [i[0] for i in indriRunQuery_hist_1]\n",
    "    \n",
    "    #topTDocScores_d = {i[2]:np.float(i[4]) for i in indriRun}\n",
    "    topTDocScores_d = {i[0]:np.float(i[1]) for i in indriRunQuery_hist_1}\n",
    "    \n",
    "    #scores = [ np.float(i[4]) for i in indriRun]\n",
    "    #topTDocs = [i[2] for i in indriRun]\n",
    "\n",
    "    #print(\"indriRun =\", indriRun)\n",
    "    #topTDocScore = np.sum([ np.float(i[4]) for i in indriRun])\n",
    "    #expTDocScore = np.float(indriRun[0][4])\n",
    "    expTDocScore = np.float(indriRunQuery_hist_1[0][1])\n",
    "    features[\"expTDocScore\"] = expTDocScore\n",
    "    #print(\"expTDocScore =\", features[\"expTDocScore\"])\n",
    "    \n",
    "    #cmd =' '.join([\"dumpindex\", colIndexDir, \"e\", \"\\\"#od4(\" + relW + \")\\\"\"])\n",
    "    #cmd2 = [\"e\", \"\\\"#od4(\" + relW + \")\\\"\"]\n",
    "    #print(\"cmd =\", cmd)\n",
    "    #relWDocuments = dumpindex_colIndexDir(cmd2)\n",
    "    relWTopDocuments = dumpindex_hist_['efb'][\"#od4(\" + regex.sub(' ', relW) + \")\"]\n",
    "    colTermFreq = relWTopDocuments[0]\n",
    "    del relWTopDocuments[0] # the first element was colTermFreq\n",
    "    #relWDocuments = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE).stdout.read()\n",
    "    #relWDocuments = [i.split(' ') for i in relWDocuments.split('\\n')]\n",
    "    #print(\"relWDocuments =\", relWDocuments)\n",
    "    #relWDocuments = [[docNameIdMap[i[0]], i[1], i[2], i[3]] for j, i in enumerate(relWDocuments) if len(i)==4 and j>0] # remove empty arrays\n",
    "    #print(\"relWDocuments =\", relWDocuments)\n",
    "    \n",
    "    #relWTopDocuments = [i for i in relWDocuments if i[0] in topTDocs]\n",
    "    #relWTopDocuments = [i for i in relWDocuments if i in topTDocs]\n",
    "    #relWTopDocuments = [[i, topTDocScores_d[i]] for i in relWDocuments if i in topTDocs] # topdocs and their scores that contain relW\n",
    "    #print(\"relWTopDocuments =\", relWTopDocuments)\n",
    "    #numerator = np.sum([int(i[1]) for i in relWTopDocuments]) # term count in the collection\n",
    "    numerator = colTermFreq # relW term count in the collection\n",
    "    #print(\"topTDocs =\", topTDocs)\n",
    "    denumerator = np.sum([documentLength(i) for i in topTDocs]) # number of terms in topDocs\n",
    "    topTermFrac = numerator / np.float(denumerator)\n",
    "    #print(\"numerator, denumerator, topTermFrac =\", numerator, denumerator, topTermFrac)\n",
    "    features[\"topTermFrac\"] = topTermFrac\n",
    "    \n",
    "    numCanDocs = len(set([i[0] for i in relWTopDocuments]))\n",
    "    features[\"numCanDocs\"] = numCanDocs\n",
    "    #print(\"numCanDocs =\", features[\"numCanDocs\"])\n",
    "    \n",
    "    if numCanDocs > 0:\n",
    "        avgCDocScore = np.sum([ topTDocScores_d[i] for i in set([i[0] for i in relWTopDocuments])])/np.float(numCanDocs)\n",
    "    else:\n",
    "        avgCDocScore = 0\n",
    "    features[\"avgCDocScore\"] = avgCDocScore\n",
    "    #print(\"avgCDocScore =\", features[\"avgCDocScore\"])\n",
    "\n",
    "    #for i in set([i[0] for i in relWTopDocuments]):\n",
    "    #    print(i, topTDocScores_d[i])\n",
    "    \n",
    "    l = [ topTDocScores_d[i] for i in set([i[0] for i in relWTopDocuments])]\n",
    "    if len(l)>0:\n",
    "        maxCDocScore = np.max(l)\n",
    "    else:\n",
    "        maxCDocScore = 0\n",
    "    features[\"maxCDocScore\"] = maxCDocScore\n",
    "    #print(\"maxCDocScore =\", features[\"maxCDocScore\"])\n",
    "    \n",
    "    if colTermFreq>0:\n",
    "        conIdf = np.log(colDocumentCount / colTermFreq)\n",
    "    else:\n",
    "        conIdf = np.log(colDocumentCount / 1)\n",
    "    features[\"conIdf\"] = conIdf\n",
    "    #print(\"conIdf =\", features[\"conIdf\"])\n",
    "    \n",
    "    coocurDocuments = dict()\n",
    "    coocurDocumentsTop = dict()\n",
    "    for ow in origWords_:\n",
    "        #cmd = ' '.join([\"dumpindex\", colIndexDir, \"e\", \"\\\"#uw(#4( \" + ow + \" ) #4( \" + relW + \" ))\\\"\"])\n",
    "        #cmd2 = [\"e\", \"\\\"#uw(#4( \" + ow + \" ) #4( \" + relW + \" ))\\\"\"]\n",
    "        #print(\"cmd =\", cmd)\n",
    "        #coocurDocuments_ = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE).stdout.read()\n",
    "        #coocurDocuments_ = dumpindex_colIndexDir(cmd2)\n",
    "        #coocurDocuments_ = [i.split(' ') for i in coocurDocuments_.split('\\n')]\n",
    "        coocurDocuments[ow] = [i for i in dumpindex_hist_['efb'][\"#uw(#4( \" + regex.sub(' ', ow) + \" ) #4( \" + regex.sub(' ', relW) + \" ))\"]]\n",
    "        #coocurDocuments[ow] = [docNameIdMap[i[0]] for i in coocurDocuments_ if len(i)==4] # remove empty arrays\n",
    "        #print(\"topTDocs =\", topTDocs)\n",
    "        coocurDocumentsTop[ow] = [i for i in coocurDocuments[ow] if i in topTDocs ] # remove empty arrays\n",
    "\n",
    "    #for k, v in coocurDocuments.iteritems():\n",
    "    #    print(k, len(v))\n",
    "        \n",
    "    #for v in coocurDocumentsTop.values():\n",
    "    #    print(v)\n",
    "    #    print(len(set(v)))\n",
    "        \n",
    "    avgColCor = np.sum([len(set(i)) for i in coocurDocuments.values()])/np.float(len(coocurDocuments)) \n",
    "    features[\"avgColCor\"] = avgColCor\n",
    "    #print(\"avgColCor =\", features[\"avgColCor\"])\n",
    "    \n",
    "    maxColCor = np.max([len(set(i)) for i in coocurDocuments.values()])\n",
    "    features[\"maxColCor\"] = maxColCor\n",
    "    #print(\"maxColCor =\", features[\"maxColCor\"])\n",
    "    \n",
    "    avgTopColCor = np.sum([len(set(i)) for i in coocurDocumentsTop.values()])/np.float(len(coocurDocumentsTop)) \n",
    "    features[\"avgTopColCor\"] = avgTopColCor\n",
    "    #print(\"avgTopColCor =\", features[\"avgTopColCor\"])\n",
    "    \n",
    "    l = [len(set(i)) for i in coocurDocumentsTop.values()]\n",
    "    if len(l)>0:\n",
    "        maxTopColCor = np.max(l)\n",
    "    else:\n",
    "        maxTopColCor = 0\n",
    "    features[\"maxTopColCor\"] = maxTopColCor\n",
    "    #print(\"maxTopColCor =\", features[\"maxTopColCor\"])\n",
    "\n",
    "    for j1, ow1 in enumerate(origWords_):\n",
    "        for j2, ow2 in enumerate(origWords_):\n",
    "            if j1>j2:\n",
    "                #coocurDocuments_ = subprocess.Popen(' '.join([\"dumpindex\", colIndexDir, \"e\", \"\\\"#uw(#4( \" + ow1 + \" ) #4( \" + ow2 + \" ) #4( \" + relW + \" ))\\\"\"]), shell=True, stdout=subprocess.PIPE).stdout.read()\n",
    "                #cmd2 = [ \"e\", \"\\\"#uw(#4( \" + ow1 + \" ) #4( \" + ow2 + \" ) #4( \" + relW + \" ))\\\"\"]\n",
    "                #coocurDocuments_ = dumpindex_colIndexDir(cmd2)\n",
    "                #coocurDocuments_ = [i.split(' ') for i in coocurDocuments_.split('\\n')]\n",
    "                coocurDocuments[ow1+ow2] = [i for i in dumpindex_hist_['efb'][\"#uw(#4( \" + regex.sub(' ', ow1) + \" ) #4( \" + regex.sub(' ', ow2) + \" ) #4( \" + regex.sub(' ', relW) + \" ))\"]]\n",
    "                #coocurDocuments[ow1+ow2] = [docNameIdMap[i[0]] for i in coocurDocuments_ if len(i)==4] # remove empty arrays\n",
    "                coocurDocumentsTop[ow1+ow2] = [i for i in coocurDocuments[ow1+ow2] if i in topTDocs ] # remove empty arrays\n",
    "\n",
    "    avgColPCor = np.sum([len(set(i)) for i in coocurDocuments.values()])/np.float(len(coocurDocuments)) \n",
    "    features[\"avgColPCor\"] = avgColPCor\n",
    "    #print(\"avgColPCor =\", features[\"avgColPCor\"])\n",
    "\n",
    "    l = [len(set(i)) for i in coocurDocuments.values()]\n",
    "    if len(l)>0:\n",
    "        maxColPCor = np.max(l)\n",
    "    else:\n",
    "        maxColPCor = 0\n",
    "    features[\"maxColPCor\"] = maxColPCor\n",
    "    #print(\"maxColPCor =\", features[\"maxColPCor\"])\n",
    "    \n",
    "    avgTopColPCor = np.sum([len(set(i)) for i in coocurDocumentsTop.values()])/np.float(len(coocurDocumentsTop)) \n",
    "    features[\"avgTopColPCor\"] = avgTopColPCor\n",
    "    #print(\"avgTopColPCor =\", features[\"avgTopColPCor\"])\n",
    "\n",
    "    l = [len(set(i)) for i in coocurDocumentsTop.values()] \n",
    "    if len(l)>0:\n",
    "        maxTopColPCor = np.max(l)\n",
    "    else:\n",
    "        maxTopColPCor = 0\n",
    "    features[\"maxTopColPCor\"] = maxTopColPCor\n",
    "    #print(\"maxTopColPCor =\", features[\"maxTopColPCor\"])\n",
    "    \n",
    "    #print(\"docno, origWords_, relW =\", docno, origWords_, relW)\n",
    "    #print(\"features =\", features)\n",
    "    \n",
    "    relWg = 0\n",
    "    for feature, weight in featureWeights.iteritems():\n",
    "        relWg += features[feature] * weight\n",
    "    \n",
    "    #print(\"features.keys() =\", features.keys())\n",
    "    return relWg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# keys of rWords are important\n",
    "# values of rWords1 are important\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "def getRelWords_train(args, indriRunQuery_hist_, dumpindex_hist_):\n",
    "    #print(\"origWords =\", origWords)\n",
    "    text_string = \"\"\n",
    "    \n",
    "    count_history, rWords, docno, origWords, expansionCount, intCoeff0, intCoeff1, intCoeff2, dirCoeff, featureWeights = args\n",
    "        \n",
    "    relWords = dict()\n",
    "    #if method == \"relAll\":\n",
    "            \n",
    "    if method == \"lr\":\n",
    "        origWords_ = [w.replace(\" \", \"_\") for w in origWords]\n",
    "        #relWords_l = []\n",
    "        for ow in origWords_:\n",
    "            #print(\"ow =\", ow)\n",
    "            #relWords_l += list(conceptnet5RelAll[ow])\n",
    "            #print(ow, \"---\", conceptnet5RelAll[ow])\n",
    "            indriRunQuery_hist = []\n",
    "            for relW in conceptnet5RelAll[ow].keys():\n",
    "                indriRunQuery_hist += weightRelConcept_hist_i(docno, origWords_, ow, relW, intCoeff0, intCoeff1, intCoeff2, dirCoeff, featureWeights)\n",
    "            #print(\"len(indriRunQuery_hist) =\", len(indriRunQuery_hist))\n",
    "            indriRunQuery_hist_ = runCommandsinHist_i(cfgTmpOutFileName, indriRunQuery_hist, indriRunQuery_hist_, featureWeights)\n",
    "            #print(\"len(indriRunQuery_hist_) =\", len(indriRunQuery_hist_))\n",
    "            \n",
    "            dumpindex_colIndexDir_hist = set()\n",
    "            for relW in conceptnet5RelAll[ow].keys():\n",
    "                dumpindex_colIndexDir_hist |= weightRelConcept_hist_d(indriRunQuery_hist_, dumpindex_colIndexDir_hist, docno, origWords_, ow, relW, intCoeff0, intCoeff1, intCoeff2, dirCoeff, featureWeights)\n",
    "            dumpindex_hist_ = runCommandsinHist_d(dumpindex_colIndexDir_hist, dumpindex_hist_)\n",
    "            #print(\"len(dumpindex_hist_['ef']) =\", len(dumpindex_hist_['ef']))\n",
    "            #print(\"len(dumpindex_hist_['dcf']) =\", len(dumpindex_hist_['dcf']))\n",
    "\n",
    "    return(indriRunQuery_hist_, dumpindex_hist_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# keys of rWords are important\n",
    "# values of rWords1 are important\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "def getRelWords(args, indriRunQuery_hist_, dumpindex_hist_):\n",
    "    #print(\"origWords =\", origWords)\n",
    "    text_string = \"\"\n",
    "    \n",
    "    count_history, rWords, docno, origWords, expansionCount, intCoeff0, intCoeff1, intCoeff2, dirCoeff, featureWeights = args\n",
    "        \n",
    "    relWords = dict()\n",
    "    #if method == \"relAll\":\n",
    "        \n",
    "    if method == \"assocRestful\":\n",
    "        origWords_ = [w.replace(\" \", \"_\") for w in origWords]\n",
    "        url = \"http://conceptnet5.media.mit.edu/data/5.4/assoc/list/en/\"+','.join(origWords_)\n",
    "        response = urllib2.urlopen(url)\n",
    "        data = response.read()\n",
    "        data_j = json.loads(data)\n",
    "        relWords = {str(d[0].encode('utf-8')).replace('/c/en/', '').replace('-', ' '):np.float(d[1]) for d in data_j[u'similar'] if '/c/en/' in str(d[0].encode('utf-8'))}\n",
    "    \n",
    "    elif method == \"lr\":\n",
    "        origWords_ = [w.replace(\" \", \"_\") for w in origWords]\n",
    "        #relWords_l = []\n",
    "\n",
    "        for ow in origWords_:\n",
    "            #relWords_l += list(conceptnet5RelAll[ow])\n",
    "            #print(ow, \"---\", conceptnet5RelAll[ow])\n",
    "            for relW in conceptnet5RelAll[ow].keys():\n",
    "                relWg = weightRelConcept(docno, origWords_, ow, relW, intCoeff0, intCoeff1, intCoeff2, dirCoeff, count_history, featureWeights)\n",
    "                if relW in relWords:\n",
    "                    relWords[relW] += relWg\n",
    "                else:\n",
    "                    relWords[relW] = relWg  \n",
    "    \n",
    "    else:\n",
    "        count_history = countExpr(count_history, origWords, rWords)\n",
    "        expressions = set()\n",
    "        for ow in origWords:\n",
    "            rWords_ow = rWords.get(ow, [])\n",
    "            expressions.add(\"#4( \" + ow.translate(None, string.punctuation) + \" )\")\n",
    "            if ow.strip() == \"\":\n",
    "                continue\n",
    "            if simMeasure == \"mi\":\n",
    "                N_w = countExpr_get_1(count_history, ow)\n",
    "            for rw_wv in rWords_ow:\n",
    "                (rw_w, rw_v) = rw_wv.items()[0]\n",
    "                if rw_w.strip() == \"\":\n",
    "                    continue           \n",
    "                if simMeasure == \"mi\":\n",
    "                    N_v = countExpr_get_1(count_history, rw_w)\n",
    "                    N_wv = countExpr_get_2(count_history, ow, rw_w)\n",
    "                    rw_v_ = mi_(N_w, N_v, N_wv)\n",
    "                elif simMeasure == \"cnet\":\n",
    "                    rw_v_ = rw_v\n",
    "                relWords[rw_w] = relWords.get(rw_w, 0) + rw_v_\n",
    "    \n",
    "    relWords_sorted = sorted(relWords.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    #print (\"origWords =\", origWords)\n",
    "    #print (\"relWords_sorted =\", relWords_sorted)\n",
    "    #counter = 0\n",
    "    relWords_sel = []\n",
    "    for counter, (rw_w, rw_v) in enumerate(relWords_sorted):\n",
    "        if (counter >= expansionCount):\n",
    "            break\n",
    "        if  all(c in string.printable for c in rw_w):\n",
    "            if rw_w not in origWords:\n",
    "                rw_w = regex.sub(' ', rw_w)\n",
    "                text_string += rw_w + \" \"\n",
    "                relWords_sel += [rw_w]\n",
    "        #counter += 1\n",
    "    #print(\"counter =\", counter)\n",
    "#    return(relWords_sel)\n",
    "    return(count_history, text_string, relWords_sel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getAllRelWords_train(count_history, rWords, origWordsAll, expansionCount1, N1, expansionCount2, intCoeff0, intCoeff1, intCoeff2, dirCoeff, featureWeights, indriRunQuery_hist_, dumpindex_hist_):\n",
    "    relText_stringAll = []\n",
    "    relText_string = dict()\n",
    "    relWords_sel = dict()\n",
    "    for c1, (docno, origWords) in enumerate(origWordsAll.iteritems()):\n",
    "        print(docno, \"---\", origWords)\n",
    "        args = (count_history, rWords, docno, origWords, expansionCount1, intCoeff0, intCoeff1, intCoeff2, dirCoeff, featureWeights)\n",
    "        indriRunQuery_hist_, dumpindex_hist_ = getRelWords_train(args, indriRunQuery_hist_, dumpindex_hist_)\n",
    "        print(c1, \"- len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) =\", len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())))\n",
    "    \n",
    "    #print(\"relText_stringAll =\", relText_stringAll)\n",
    "    return (indriRunQuery_hist_, dumpindex_hist_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getAllRelWords(count_history, rWords, origWordsAll, expansionCount1, N1, expansionCount2, intCoeff0, intCoeff1, intCoeff2, dirCoeff, featureWeights, indriRunQuery_hist_, dumpindex_hist_):\n",
    "    relText_stringAll = []\n",
    "    relText_string = dict()\n",
    "    relWords_sel = dict()\n",
    "    for c1, (docno, origWords) in enumerate(origWordsAll.iteritems()):\n",
    "        #print(str(c1), \"---\", docno, \"---\", origWords)\n",
    "        args = (count_history, rWords, docno, origWords, expansionCount1, intCoeff0, intCoeff1, intCoeff2, dirCoeff, featureWeights)\n",
    "        count_history, relText_string_, relWords_sel_ = getRelWords(args, indriRunQuery_hist_, dumpindex_hist_)\n",
    "        #print(\"len(indriRunQuery_hist_), len(dumpindex_hist_['dcf']), len(dumpindex_hist_['ef']) =\", len(indriRunQuery_hist_), len(dumpindex_hist_['dcf']), len(dumpindex_hist_['ef']))\n",
    "        relText_string[docno] = relText_string_\n",
    "        relWords_sel[docno] = relWords_sel_[0:N1]\n",
    "        #print(docno, \"---\", origWords, \"---\", relWords_sel_)\n",
    "    relText_stringAll += [relText_string]\n",
    "    \n",
    "    relText_string = dict()\n",
    "    relWords_sel = dict()\n",
    "    for docno, origWords_new in relWords_sel.iteritems():\n",
    "        args = (count_history, rWords, docno, origWords, expansionCount1, intCoeff0, intCoeff1, intCoeff2, dirCoeff, featureWeights)\n",
    "        count_history, relText_string_, relWords_sel_ = getRelWords(args, indriRunQuery_hist_, dumpindex_hist_)\n",
    "        relText_string[docno] = relText_string_\n",
    "        relWords_sel[docno] = relWords_sel_\n",
    "    relText_stringAll += [relText_string]\n",
    "    \n",
    "    #print(\"relText_stringAll =\", relText_stringAll)\n",
    "    return (count_history, relText_stringAll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "def genQueries(cfgOutFileName_, origWordsAll, relText_stringAll, intCoeff0, intCoeff1, intCoeff2, dirCoeff):\n",
    "    #    #print(\"intCoeff =\", intCoeff)\n",
    "    with open(cfgInFileName, 'r') as inFile:\n",
    "        reader = inFile.read()\n",
    "        soupNew = BeautifulSoup(\"\", 'lxml')\n",
    "        parameters_tag = soupNew.new_tag(\"parameters\")\n",
    "        index_tag = soupNew.new_tag(\"index\")\n",
    "        index_tag.string = colIndexDir\n",
    "        parameters_tag.append(index_tag)\n",
    "            #print(tags)\n",
    "\n",
    "        for docno, origWords in origWordsAll.iteritems():\n",
    "            doc_tag = soupNew.new_tag(\"query\")\n",
    "\n",
    "            docno_tag = soupNew.new_tag(\"number\")\n",
    "            docno_tag.string = docno\n",
    "            doc_tag.append(docno_tag)\n",
    "\n",
    "            text_tag = soupNew.new_tag(\"text\")\n",
    "            text_tag.string = \"#weight(\\n\" \n",
    "                    \n",
    "            text_tag.string += str(intCoeff0) + \" #combine(\" \n",
    "            for ow in set(origWords):\n",
    "                ow = regex.sub(' ', ow)\n",
    "                text_tag.string += ow + \" \"\n",
    "            text_tag.string += \")\\n\"\n",
    "\n",
    "            if len(relText_stringAll[0])>0:\n",
    "                if len(relText_stringAll[0][docno]) > 3: \n",
    "                    relText_string1 = regex.sub(' ', relText_stringAll[0][docno])\n",
    "                    text_tag.string += str(intCoeff1) + \" #combine(\" \n",
    "                    #print(\"relText_string1 =\", relText_string1)\n",
    "                    text_tag.string += relText_string1.encode('utf-8')\n",
    "                    text_tag.string += \")\\n\"\n",
    "            \n",
    "            #print(len(relText_stringAll[1]))\n",
    "            if len(relText_stringAll[1])>0:\n",
    "                if len(relText_stringAll[1][docno]) > 3: \n",
    "                    relText_string2 = regex.sub(' ', relText_stringAll[1][docno])\n",
    "                    text_tag.string += str(intCoeff2) + \" #combine(\" \n",
    "                    #print(\"relText_string2 =\", relText_string2)\n",
    "                    text_tag.string += relText_string2.encode('utf-8')\n",
    "                    text_tag.string += \")\\n\"\n",
    "            \n",
    "            #print(\"origWords =\", origWords)\n",
    "            #print(\"relWords_sel1 =\", relWords_sel1)\n",
    "            #print(\"relWords_sel2 =\", relWords_sel2)\n",
    "            \n",
    "            \n",
    "            text_tag.string += \") \"\n",
    "\n",
    "            doc_tag.append(text_tag)\n",
    "\n",
    "            parameters_tag.append(doc_tag)\n",
    "            #print(doc_tag)\n",
    "\n",
    "        rule_tag = soupNew.new_tag(\"rule\")\n",
    "        rule_tag.string = \"method:dir,mu:\" + str(dirCoeff)\n",
    "        #rule_tag.string = \"method:two\"\n",
    "        parameters_tag.append(rule_tag)\n",
    "\n",
    "        #intCoeff_tag = soupNew.new_tag(\"intCoeff\")\n",
    "        #intCoeff_tag.string = \"0.8\"\n",
    "        #parameters_tag.append(intCoeff_tag)\n",
    "\n",
    "        threads_tag = soupNew.new_tag(\"threads\")\n",
    "        threads_tag.string = \"32\"\n",
    "        parameters_tag.append(threads_tag)\n",
    "\n",
    "        count_tag = soupNew.new_tag(\"count\")\n",
    "        count_tag.string = \"1000\"\n",
    "        parameters_tag.append(count_tag)\n",
    "\n",
    "        trecFormat_tag = soupNew.new_tag(\"trecFormat\")\n",
    "        trecFormat_tag.string = \"true\"\n",
    "        parameters_tag.append(trecFormat_tag)\n",
    "\n",
    "        soupNew.append(parameters_tag)\n",
    "        #print(soupNew.prettify())\n",
    "    #print(\"outFileName =\", outFileName)\n",
    "    with open( cfgOutFileName_, 'w') as outFile:\n",
    "        soupNewStr = str(soupNew)\n",
    "        soupNewStr = soupNewStr.replace(\"</text>\", \"\\n</text>\\n\").replace(\"query>\", \"query>\\n\").replace(\"<text>\", \"\\n<text>\\n\").replace(\"</index>\", \"</index>\\n\").replace(\"\\n<index>\", \"<index>\")\n",
    "\n",
    "        outFile.write(soupNewStr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def precisionCompute_train(count_history, intCoeffs0, intCoeffs1, intCoeffs2, expansionCounts1, expansionCounts2, dirCoeffs, N1s, featureWeights_, indriRunQuery_hist_, dumpindex_hist_):\n",
    "    #randNum = random.randint(1,1e9)\n",
    "    origWordsAll = analyseQueries()\n",
    "    indriRunQuery_hist_, dumpindex_hist_ = getAllRelWords_train(count_history, rWords, origWordsAll, expansionCounts1[0], N1s[0], expansionCounts2[0], intCoeffs0[0], intCoeffs1[0], intCoeffs2[0], dirCoeffs[0], featureWeights_[0], indriRunQuery_hist_, dumpindex_hist_)\n",
    "                                    \n",
    "    return origWordsAll, count_history, indriRunQuery_hist_, dumpindex_hist_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "669 --- ['islamic', 'revolution']\n",
      "0 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 146 584 131\n",
      "668 --- ['poverty', 'disease']\n",
      "1 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 524 2089 625\n",
      "667 --- ['unmarried', 'partner', 'household']\n",
      "2 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 637 2864 762\n",
      "666 --- ['thatcher', 'resignation', 'impact']\n",
      "3 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 704 3317 1033\n",
      "665 --- ['poverty', 'africa', 'sub', 'sahara']\n",
      "4 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 891 5239 1256\n",
      "664 --- ['american', 'indian', 'museum']\n",
      "5 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 1150 7018 1566\n",
      "663 --- ['agent', 'orange', 'exposure']\n",
      "6 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 1543 9701 2175\n",
      "662 --- ['telemarketer', 'protection']\n",
      "7 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 1577 9825 2218\n",
      "661 --- ['melanoma', 'treatment', 'cause']\n",
      "8 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 1731 10863 2267\n",
      "660 --- ['whale', 'watch', 'california']\n",
      "9 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 2186 13967 2534\n",
      "627 --- ['russian', 'food', 'crisis']\n",
      "10 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 2636 17048 4009\n",
      "626 --- ['human', 'stampede']\n",
      "11 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 4117 22760 4753\n",
      "692 --- ['prostate', 'cancer', 'detection', 'treatment']\n",
      "12 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 4245 24031 4853\n",
      "693 --- ['newspaper', 'electronic', 'media']\n",
      "13 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 4479 25591 5364\n",
      "690 --- ['college', 'education', 'advantage']\n",
      "14 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 4882 28318 6024\n",
      "691 --- ['clear', 'cut', 'forest']\n",
      "15 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 5803 34563 7429\n",
      "696 --- ['safety', 'plastic', 'surgery']\n",
      "16 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 6043 36169 7806\n",
      "697 --- ['air', 'traffic', 'controller']\n",
      "17 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 6500 39208 8512\n",
      "694 --- ['compost', 'pile']\n",
      "18 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 6596 39570 8544\n",
      "695 --- ['white', 'collar', 'crime', 'sentence']\n",
      "19 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 7016 44078 9298\n",
      "698 --- ['literacy', 'rate', 'africa']\n",
      "20 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 7341 46137 9572\n",
      "699 --- ['term', 'limit']\n",
      "21 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 7675 47373 10385\n",
      "344 --- ['abuse', 'of', 'e', 'mail']\n",
      "22 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 7952 50316 11509\n",
      "345 --- ['overseas', 'tobacco', 'sale']\n",
      "23 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 8336 52889 12048\n",
      "346 --- ['educational', 'standard']\n",
      "24 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 8411 53162 12322\n",
      "347 --- ['wildlife', 'extinction']\n",
      "25 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 8454 53308 12392\n",
      "340 --- ['land', 'mine', 'ban']\n",
      "26 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 9061 57359 13836\n",
      "341 --- ['airport', 'security']\n",
      "27 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 9173 57755 14134\n",
      "342 --- ['diplomatic', 'expulsion']\n",
      "28 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 9214 57904 14177\n",
      "343 --- ['police', 'death']\n",
      "29 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 9473 58851 14776\n",
      "348 --- ['agoraphobia']\n",
      "30 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 9480 58861 14801\n",
      "349 --- ['metabolism']\n",
      "31 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 9488 58876 14814\n",
      "409 --- ['legal', 'pan', 'be', '103']\n",
      "32 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 10077 65149 16561\n",
      "408 --- ['tropical', 'storm']\n",
      "33 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 10249 65772 16638\n",
      "700 --- ['gasoline', 'tax', 'u_s']\n",
      "34 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 10841 69764 17865\n",
      "628 --- ['u_s', 'invasion', 'of', 'panama']\n",
      "35 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 10909 70418 17989\n",
      "678 --- ['joint', 'custody', 'impact']\n",
      "36 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 11038 71189 18490\n",
      "679 --- ['open', 'adoption', 'record']\n",
      "37 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 11848 76608 20245\n",
      "674 --- ['greenpeace', 'prosecute']\n",
      "38 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 11875 76706 20325\n",
      "675 --- ['olympics', 'train', 'swim']\n",
      "39 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 12647 81903 20581\n",
      "676 --- ['poppy', 'cultivation']\n",
      "40 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 12693 82069 20617\n",
      "677 --- ['lean', 'tower', 'of', 'pisa']\n",
      "41 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 12953 84786 20724\n",
      "670 --- ['u_s', 'election', 'apathy']\n",
      "42 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 13087 85672 21024\n",
      "671 --- ['salvation', 'army', 'benefit']\n",
      "43 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 13457 88123 21504\n",
      "672 --- ['nra', 'membership', 'profile']\n",
      "44 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 13549 88714 21643\n",
      "673 --- ['soviet', 'withdrawal', 'afghanistan']\n",
      "45 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 13611 89125 21747\n",
      "421 --- ['industrial', 'waste', 'disposal']\n",
      "46 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 14022 91861 22615\n",
      "420 --- ['carbon', 'monoxide', 'poison']\n",
      "47 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 14549 95359 22774\n",
      "442 --- ['heroic', 'act']\n",
      "48 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 14847 96436 23383\n",
      "423 --- ['milosevic', 'mirjana', 'markovic']\n",
      "49 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 14847 96436 23383\n",
      "422 --- ['art', 'steal', 'forge']\n",
      "50 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 15556 101139 24180\n",
      "425 --- ['counterfeit', 'money']\n",
      "51 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 16104 103127 24352\n",
      "424 --- ['suicide']\n",
      "52 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 16179 103242 24423\n",
      "379 --- ['mainstreaming']\n",
      "53 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 16179 103242 24423\n",
      "378 --- ['euro', 'opposition']\n",
      "54 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 16260 103532 24717\n",
      "416 --- ['three', 'gorge', 'project']\n",
      "55 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 16426 104602 24828\n",
      "417 --- ['creativity']\n",
      "56 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 16457 104648 24881\n",
      "410 --- ['schengen', 'agreement']\n",
      "57 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 16511 104842 24925\n",
      "411 --- ['salvage', 'shipwreck', 'treasure']\n",
      "58 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 16602 105424 24984\n",
      "412 --- ['airport', 'security']\n",
      "59 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 16714 105424 24984\n",
      "413 --- ['steel', 'production']\n",
      "60 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 17200 107158 25546\n",
      "371 --- ['health', 'insurance', 'holistic']\n",
      "61 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 17479 108986 25878\n",
      "370 --- ['food', 'drug', 'law']\n",
      "62 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 18269 113634 27041\n",
      "373 --- ['encryption', 'equipment', 'export']\n",
      "63 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 18309 113892 27131\n",
      "372 --- ['native', 'american', 'casino']\n",
      "64 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 18583 115548 27494\n",
      "375 --- ['hydrogen', 'energy']\n",
      "65 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 19087 117349 27647\n",
      "374 --- ['nobel', 'prize', 'winner']\n",
      "66 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 19207 118128 27765\n",
      "377 --- ['cigar', 'smoke']\n",
      "67 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 20037 121218 27890\n",
      "376 --- ['world', 'court']\n",
      "68 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 20707 123666 29241\n",
      "319 --- ['new', 'fuel', 'source']\n",
      "69 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 20983 125434 30192\n",
      "318 --- ['best', 'retirement', 'country']\n",
      "70 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 21630 129695 31756\n",
      "313 --- ['magnetic', 'levitation', 'maglev']\n",
      "71 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 21677 130000 31777\n",
      "312 --- ['hydroponics']\n",
      "72 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 21684 130009 31788\n",
      "311 --- ['industrial', 'espionage']\n",
      "73 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 21719 130097 31846\n",
      "310 --- ['radio', 'wave', 'and', 'brain', 'cancer']\n",
      "74 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 22703 145371 33333\n",
      "317 --- ['unsolicited', 'fax']\n",
      "75 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 22722 145440 33356\n",
      "316 --- ['polygamy', 'polyandry', 'polygyny']\n",
      "76 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 22757 145677 33442\n",
      "315 --- ['unexplained', 'highway', 'accident']\n",
      "77 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 23078 147796 33988\n",
      "314 --- ['marine', 'vegetation']\n",
      "78 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 23113 147916 34100\n",
      "393 --- ['mercy', 'kill']\n",
      "79 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 23584 149606 34453\n",
      "392 --- ['robotics']\n",
      "80 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 23594 149620 34465\n",
      "391 --- ['r', 'drug', 'price']\n",
      "81 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 24191 153279 35082\n",
      "390 --- ['orphan', 'drug']\n",
      "82 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 24392 153734 35138\n",
      "397 --- ['automobile', 'recall']\n",
      "83 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 24549 154289 35386\n",
      "396 --- ['sick', 'build', 'syndrome']\n",
      "84 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 24920 156687 35458\n",
      "395 --- ['tourism']\n",
      "85 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 24934 156707 35504\n",
      "394 --- ['home', 'school']\n",
      "86 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 25995 160529 36808\n",
      "399 --- ['oceanographic', 'vessel']\n",
      "87 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 26207 161276 36845\n",
      "398 --- ['dismantle', \"europe's\", 'arsenal']\n",
      "88 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 26258 161595 36983\n",
      "414 --- ['cuba', 'sugar', 'export']\n",
      "89 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 26748 164844 37355\n",
      "415 --- ['drug', 'golden', 'triangle']\n",
      "90 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 27071 166689 37482\n",
      "429 --- ['legionnaire', 'disease']\n",
      "91 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 27413 167391 37517\n",
      "428 --- ['decline', 'birth', 'rate']\n",
      "92 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 27764 169292 37962\n",
      "447 --- ['stirling', 'engine']\n",
      "93 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 27934 169877 38107\n",
      "368 --- ['in', 'vitro', 'fertilization']\n",
      "94 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 28065 170738 38223\n",
      "369 --- ['anorexia', 'nervosa', 'bulimia']\n",
      "95 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 28085 170866 38236\n",
      "366 --- ['commercial', 'cyanide', 'use']\n",
      "96 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 28237 171843 38493\n",
      "367 --- ['piracy']\n",
      "97 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 28266 171881 38559\n",
      "364 --- ['rabies']\n",
      "98 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 28290 171916 38576\n",
      "365 --- ['el', 'nino']\n",
      "99 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 28312 171985 38592\n",
      "362 --- ['human', 'smuggle']\n",
      "100 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 29787 174953 39326\n",
      "363 --- ['transportation', 'tunnel', 'disaster']\n",
      "101 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 30005 176347 39740\n",
      "360 --- ['drug', 'legalization', 'benefit']\n",
      "102 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 30260 177613 40150\n",
      "361 --- ['clothe', 'sweatshop']\n",
      "103 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 30532 178602 40187\n",
      "308 --- ['implant', 'dentistry']\n",
      "104 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 30571 178735 40233\n",
      "309 --- ['rap', 'and', 'crime']\n",
      "105 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 30979 181046 40738\n",
      "449 --- ['antibiotic', 'ineffectiveness']\n",
      "106 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 31014 181173 40783\n",
      "440 --- ['child', 'labor']\n",
      "107 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 32330 186126 41356\n",
      "443 --- ['u_s', 'investment', 'africa']\n",
      "108 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 32420 186589 41498\n",
      "301 --- ['international', 'organize', 'crime']\n",
      "109 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 32708 188294 41843\n",
      "302 --- ['poliomyelitis', 'and', 'post', 'polio']\n",
      "110 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 33165 192903 42249\n",
      "303 --- ['hubble', 'telescope', 'achievement']\n",
      "111 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 33304 193821 42311\n",
      "304 --- ['endanger', 'specie', 'mammal']\n",
      "112 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 33493 195048 42440\n",
      "305 --- ['most', 'dangerous', 'vehicle']\n",
      "113 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 33730 196549 43047\n",
      "306 --- ['african', 'civilian', 'death']\n",
      "114 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 33915 197506 43337\n",
      "307 --- ['new', 'hydroelectric', 'project']\n",
      "115 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 34109 198471 43741\n",
      "380 --- ['obesity', 'medical', 'treatment']\n",
      "116 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 34179 198842 43787\n",
      "381 --- ['alternative', 'medicine']\n",
      "117 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 34381 199540 44004\n",
      "382 --- ['hydrogen', 'fuel', 'automobile']\n",
      "118 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 34621 200688 44012\n",
      "383 --- ['mental', 'illness', 'drug']\n",
      "119 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 34851 201931 44206\n",
      "384 --- ['space', 'station', 'moon']\n",
      "120 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 35940 209179 44531\n",
      "385 --- ['hybrid', 'fuel', 'car']\n",
      "121 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 36847 214969 45011\n",
      "386 --- ['teach', 'disable', 'child']\n",
      "122 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 38207 221859 45423\n",
      "387 --- ['radioactive', 'waste']\n",
      "123 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 38612 222692 45535\n",
      "388 --- ['organic', 'soil', 'enhancement']\n",
      "124 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 38721 223387 45765\n",
      "389 --- ['illegal', 'technology', 'transfer']\n",
      "125 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 38851 224224 46176\n",
      "444 --- ['supercritical', 'fluid']\n",
      "126 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 38894 224370 46191\n",
      "418 --- ['quilt', 'income']\n",
      "127 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 39010 224774 46311\n",
      "419 --- ['recycle', 'automobile', 'tire']\n",
      "128 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 39270 226316 46501\n",
      "445 --- ['woman', 'clergy']\n",
      "129 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 39878 228594 46731\n",
      "640 --- ['maternity', 'leave', 'policy']\n",
      "130 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 40212 230748 47002\n",
      "643 --- ['salmon', 'dam', 'pacific', 'northwest']\n",
      "131 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 40328 231965 47204\n",
      "642 --- ['tiananmen', 'square', 'protester']\n",
      "132 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 40510 233145 47298\n",
      "645 --- ['software', 'piracy']\n",
      "133 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 40602 233430 47318\n",
      "644 --- ['exotic', 'animal', 'import']\n",
      "134 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 41286 237892 47659\n",
      "438 --- ['tourism', 'increase']\n",
      "135 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 41429 238369 47903\n",
      "439 --- ['invention', 'scientific', 'discovery']\n",
      "136 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 41585 239363 48056\n",
      "436 --- ['railway', 'accident']\n",
      "137 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 41760 239733 48222\n",
      "437 --- ['deregulation', 'gas', 'electric']\n",
      "138 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 42098 241895 48658\n",
      "434 --- ['estonia', 'economy']\n",
      "139 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 42163 242120 48768\n",
      "435 --- ['curb', 'population', 'growth']\n",
      "140 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 42526 244493 49255\n",
      "432 --- ['profile', 'motorist', 'police']\n",
      "141 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 42719 245453 49568\n",
      "433 --- ['greek', 'philosophy', 'stoicism']\n",
      "142 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 42851 246310 49789\n",
      "430 --- ['killer', 'bee', 'attack']\n",
      "143 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 43408 249928 49871\n",
      "431 --- ['robotic', 'technology']\n",
      "144 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 43445 250009 49891\n",
      "623 --- ['toxic', 'chemical', 'weapon']\n",
      "145 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 43643 251279 49999\n",
      "622 --- ['price', 'fix']\n",
      "146 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 44252 252735 50754\n",
      "621 --- ['woman', 'ordain', 'church', 'of', 'england']\n",
      "147 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 45186 266280 51168\n",
      "620 --- ['france', 'nuclear', 'test']\n",
      "148 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 45710 269710 51663\n",
      "339 --- [\"alzheimer's\", 'drug', 'treatment']\n",
      "149 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 45915 270709 51768\n",
      "338 --- ['risk', 'of', 'aspirin']\n",
      "150 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 46051 271533 51858\n",
      "625 --- ['arrest', 'bomb', 'wtc']\n",
      "151 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 46273 272961 51991\n",
      "624 --- ['sdi', 'star', 'war']\n",
      "152 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 46926 277198 52038\n",
      "335 --- ['adoptive', 'biological', 'parent']\n",
      "153 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 47140 278653 52082\n",
      "334 --- ['export', 'control', 'cryptography']\n",
      "154 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 47370 280093 52190\n",
      "337 --- ['viral', 'hepatitis']\n",
      "155 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 47398 280188 52203\n",
      "336 --- ['black', 'bear', 'attack']\n",
      "156 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 48391 286199 53688\n",
      "331 --- ['world', 'bank', 'criticism']\n",
      "157 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 49222 291175 55017\n",
      "330 --- ['iran', 'iraq', 'cooperation']\n",
      "158 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 49294 291641 55119\n",
      "333 --- ['antibiotic', 'bacteria', 'disease']\n",
      "159 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 49723 293890 55156\n",
      "332 --- ['income', 'tax', 'evasion']\n",
      "160 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 50279 296678 55408\n",
      "683 --- ['czechoslovakia', 'breakup']\n",
      "161 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 50305 296765 55448\n",
      "629 --- ['abortion', 'clinic', 'attack']\n",
      "162 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 50678 298721 55702\n",
      "407 --- ['poach', 'wildlife', 'preserve']\n",
      "163 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 51025 300937 55841\n",
      "406 --- [\"parkinson's\", 'disease']\n",
      "164 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 51355 301597 55952\n",
      "405 --- ['cosmic', 'event']\n",
      "165 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 51503 302113 55998\n",
      "656 --- ['lead', 'poison', 'child']\n",
      "166 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 53526 312672 56527\n",
      "657 --- ['school', 'prayer', 'ban']\n",
      "167 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 54081 315554 56848\n",
      "654 --- ['same', 'sex', 'school']\n",
      "168 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 54758 319332 57307\n",
      "404 --- ['ireland', 'peace', 'talk']\n",
      "169 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 55422 323712 58130\n",
      "652 --- ['oic', 'balkan', '1990s']\n",
      "170 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 55467 323998 58216\n",
      "653 --- ['eta', 'basque', 'terrorism']\n",
      "171 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 55518 324325 58247\n",
      "650 --- ['tax', 'evasion', 'indict']\n",
      "172 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 56059 325976 58339\n",
      "651 --- ['u_s', 'ethnic', 'population']\n",
      "173 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 56288 327137 58581\n",
      "403 --- ['osteoporosis']\n",
      "174 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 56301 327155 58595\n",
      "658 --- ['teenage', 'pregnancy']\n",
      "175 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 56328 327247 58615\n",
      "402 --- ['behavioral', 'genetics']\n",
      "176 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 56350 327318 58639\n",
      "630 --- ['gulf', 'war', 'syndrome']\n",
      "177 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 56836 329789 58973\n",
      "631 --- ['mandela', 'south', 'africa', 'president']\n",
      "178 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 57242 333961 59381\n",
      "632 --- ['southeast', 'asia', 'tin', 'mine']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-4b583e138ff1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mfeatureWeights_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'maxTopColCor'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'expTDocScore'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'maxColPCor'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'avgCDocScore'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'topTermFrac'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'avgColCor'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'avgTopColCor'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'numCanDocs'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'maxTopColPCor'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'maxCDocScore'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'avgColPCor'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'conIdf'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'avgTopColPCor'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'maxColCor'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mfeatureWeights_l\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeatureWeights_1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0morigWordsAll\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount_history\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindriRunQuery_hist_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdumpindex_hist_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprecisionCompute_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcount_history\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mintCoeffs0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mintCoeffs1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mintCoeffs2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpansionCounts1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpansionCounts2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdirCoeffs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN1s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatureWeights_l\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindriRunQuery_hist_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdumpindex_hist_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-25-043a697309eb>\u001b[0m in \u001b[0;36mprecisionCompute_train\u001b[1;34m(count_history, intCoeffs0, intCoeffs1, intCoeffs2, expansionCounts1, expansionCounts2, dirCoeffs, N1s, featureWeights_, indriRunQuery_hist_, dumpindex_hist_)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;31m#randNum = random.randint(1,1e9)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0morigWordsAll\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0manalyseQueries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mindriRunQuery_hist_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdumpindex_hist_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetAllRelWords_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcount_history\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrWords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morigWordsAll\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpansionCounts1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN1s\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpansionCounts2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mintCoeffs0\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mintCoeffs1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mintCoeffs2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdirCoeffs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatureWeights_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindriRunQuery_hist_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdumpindex_hist_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0morigWordsAll\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount_history\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindriRunQuery_hist_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdumpindex_hist_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-22-e87288d3945f>\u001b[0m in \u001b[0;36mgetAllRelWords_train\u001b[1;34m(count_history, rWords, origWordsAll, expansionCount1, N1, expansionCount2, intCoeff0, intCoeff1, intCoeff2, dirCoeff, featureWeights, indriRunQuery_hist_, dumpindex_hist_)\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocno\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"---\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morigWords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcount_history\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrWords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdocno\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morigWords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpansionCount1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mintCoeff0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mintCoeff1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mintCoeff2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdirCoeff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatureWeights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mindriRunQuery_hist_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdumpindex_hist_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetRelWords_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindriRunQuery_hist_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdumpindex_hist_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"- len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) =\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindriRunQuery_hist_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdumpindex_hist_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'efb'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdumpindex_hist_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'dcf'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-20-f1226a83e5a1>\u001b[0m in \u001b[0;36mgetRelWords_train\u001b[1;34m(args, indriRunQuery_hist_, dumpindex_hist_)\u001b[0m\n\u001b[0;32m     28\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mrelW\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mconceptnet5RelAll\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mow\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m                 \u001b[0mdumpindex_colIndexDir_hist\u001b[0m \u001b[1;33m|=\u001b[0m \u001b[0mweightRelConcept_hist_d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindriRunQuery_hist_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdumpindex_colIndexDir_hist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdocno\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morigWords_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrelW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mintCoeff0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mintCoeff1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mintCoeff2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdirCoeff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatureWeights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m             \u001b[0mdumpindex_hist_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrunCommandsinHist_d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdumpindex_colIndexDir_hist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdumpindex_hist_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m             \u001b[1;31m#print(\"len(dumpindex_hist_['ef']) =\", len(dumpindex_hist_['ef']))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[1;31m#print(\"len(dumpindex_hist_['dcf']) =\", len(dumpindex_hist_['dcf']))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-dd5ddb0cd633>\u001b[0m in \u001b[0;36mrunCommandsinHist_d\u001b[1;34m(dumpindex_colIndexDir_hist_list, dumpindex_hist_)\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0mcmd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0moccuranceCountFilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolIndexDir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfileName\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;31m#print(\"cmd =\", cmd)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         \u001b[0mdumpindex_hist_tmp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshell\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m         \u001b[0mdumpindex_hist_tmp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mj\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m':'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdumpindex_hist_tmp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0mdumpindex_hist_tmp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mk1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mv1\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdumpindex_hist_tmp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "intCoeffs0 = [ 0.7 ]\n",
    "intCoeffs1 = [ 0.3 ]\n",
    "#intCoeffs2 = [ 0.18 ]\n",
    "intCoeffs2 = [ 0.5 ]\n",
    "#intCoeffs1 = np.arange(0.1, 1, 0.1)\n",
    "expansionCounts1 = [ 85 ]\n",
    "#expansionCounts2 = [ 145 ]\n",
    "expansionCounts2 = [ 0 ]\n",
    "#expansionCounts2 = range(15, 220, 5)\n",
    "dirCoeffs = [ 1600 ]\n",
    "#dirCoeffs = range(200, 4000, 200)\n",
    "N1s = [0]\n",
    "rWords = dict()\n",
    "featureWeights_1 = {'maxTopColCor': 0.1, 'expTDocScore': 0.1, 'maxColPCor': 0.1, 'avgCDocScore': 0.1, 'topTermFrac': 0.1, 'avgColCor': 0.1, 'avgTopColCor': 0.1, 'numCanDocs': 0.1, 'maxTopColPCor': 0.1, 'maxCDocScore': 0.1, 'avgColPCor': 0.1, 'conIdf': 0.1, 'avgTopColPCor': 0.1, 'maxColCor': 0.1}\n",
    "featureWeights_l=[featureWeights_1]\n",
    "origWordsAll, count_history, indriRunQuery_hist_, dumpindex_hist_ = precisionCompute_train(count_history, intCoeffs0, intCoeffs1, intCoeffs2, expansionCounts1, expansionCounts2, dirCoeffs, N1s, featureWeights_l, indriRunQuery_hist_, dumpindex_hist_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def precisionCompute(origWordsAll, count_history, intCoeffs0, intCoeffs1, intCoeffs2, expansionCounts1, expansionCounts2, dirCoeffs, N1s, featureWeights_, indriRunQuery_hist_, dumpindex_hist_):\n",
    "    #mapPrecs = dict()\n",
    "    for featureWeights in featureWeights_:\n",
    "        for N1 in N1s:\n",
    "            for intCoeff0 in intCoeffs0:\n",
    "                for intCoeff1 in intCoeffs1:\n",
    "                    for intCoeff2 in intCoeffs2:\n",
    "                        for expansionCount1 in expansionCounts1:\n",
    "                            for expansionCount2 in expansionCounts2:\n",
    "                                for dirCoeff in dirCoeffs:\n",
    "                                    \n",
    "                                    \n",
    "                                    #res = getAllRelWords_p(count_history, rWords, origWordsAll, expansionCount1, N1, expansionCount2, intCoeff0, intCoeff1, intCoeff2, dirCoeff, featureWeights)\n",
    "                                    \n",
    "                                    count_history, relText_stringAll = getAllRelWords(count_history, rWords, origWordsAll, expansionCount1, N1, expansionCount2, intCoeff0, intCoeff1, intCoeff2, dirCoeff, featureWeights, indriRunQuery_hist_, dumpindex_hist_)\n",
    "                                    \n",
    "                                    print(\"len(relText_stringAll[1]) =\", len(relText_stringAll[1]))\n",
    "                                    ##########\n",
    "                                    #cfgOutFileName = os.path.join(\"tmp\",\"sbsb\",str(randNum)+\".cfg\")\n",
    "                                    #runsFileName = os.path.join(\"tmp\",\"sbsb\",str(randNum)+\".run\")\n",
    "                                    #evalsFileName = os.path.join(\"tmp\",\"sbsb\",str(randNum)+\".eval\")\n",
    "                                    ##########\n",
    "                                    genQueries(cfgOutFileName, origWordsAll, relText_stringAll, intCoeff0, intCoeff1, intCoeff2, dirCoeff)\n",
    "                                    subprocess.Popen(\"IndriRunQuery \" + cfgOutFileName + \" > \" + runsFileName, shell=True, stdout=subprocess.PIPE).stdout.read()\n",
    "                                    cmd = \"trec_eval -q \" + colQrelsFileName + \" \" + runsFileName + \" > \" + evalsFileName\n",
    "                                    subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE).stdout.read()\n",
    "                                    cmd = \"cat \" + evalsFileName + \" | grep map | grep all | grep -v gm | awk '{print $3}' \"\n",
    "                                    #print (\"cmd = \", cmd)\n",
    "                                    mapPrec = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE).stdout.read()\n",
    "                                    print(\"featureWeights =\", featureWeights)\n",
    "                                    print(\"intCoeff0, intCoeff1, intCoeff2, expansionCount1, expansionCount2, dirCoeff, map precision, N1 =\", intCoeff0, intCoeff1, intCoeff2, expansionCount1, expansionCount2, dirCoeff, N1, mapPrec)\n",
    "                                    #mapPrecs[intCoeff] = mapPrec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "intCoeffs0 = [ 0.7 ]\n",
    "intCoeffs1 = [ 0.3 ]\n",
    "#intCoeffs2 = [ 0.18 ]\n",
    "intCoeffs2 = [ 0.5 ]\n",
    "#intCoeffs1 = np.arange(0.1, 1, 0.1)\n",
    "expansionCounts1 = [ 85 ]\n",
    "#expansionCounts2 = [ 145 ]\n",
    "expansionCounts2 = [ 0 ]\n",
    "#expansionCounts2 = range(15, 220, 5)\n",
    "dirCoeffs = [ 1600 ]\n",
    "#dirCoeffs = range(200, 4000, 200)\n",
    "N1s = [0]\n",
    "#N1s = range(1, 30, 1)\n",
    "#mapPrecs = \n",
    "rWords = dict()\n",
    "featureWeights_l=[]\n",
    "featureWeights_1 = {'maxTopColCor': 0.1, 'expTDocScore': 0.1, 'maxColPCor': 0.1, 'avgCDocScore': 0.1, 'topTermFrac': 0.1, 'avgColCor': 0.1, 'avgTopColCor': 0.1, 'numCanDocs': 0.1, 'maxTopColPCor': 0.1, 'maxCDocScore': 0.1, 'avgColPCor': 0.1, 'conIdf': 0.1, 'avgTopColPCor': 0.1, 'maxColCor': 0.1}\n",
    "for expTDocScore in [-150, -50, -5, -0.5, 0.5, 5, 50, 150 ]:\n",
    "    featureWeights_1[\"expTDocScore\"] = expTDocScore\n",
    "    featureWeights_l += [dict(featureWeights_1)] \n",
    "precisionCompute(origWordsAll, count_history, intCoeffs0, intCoeffs1, intCoeffs2, expansionCounts1, expansionCounts2, dirCoeffs, N1s, featureWeights_l, indriRunQuery_hist_, dumpindex_hist_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
