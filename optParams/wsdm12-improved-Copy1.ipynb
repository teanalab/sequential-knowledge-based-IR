{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function \n",
    "from bs4 import BeautifulSoup\n",
    "from BeautifulSoup import SoupStrainer as sopstrain\n",
    "import subprocess\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import numpy as np\n",
    "import string\n",
    "import operator\n",
    "import csv\n",
    "import urllib2\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import StringIO\n",
    "import random\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cfgInFileName = /home/fj9124/projects/ir/seq_kb_ir/configs/trec7n8/queryTrec7n8\n",
      "cfgOutFileName = /home/fj9124/projects/ir/seq_kb_ir/configs/trec7n8/wsdmImpr/cnet/indriRunQuery.cfg\n",
      "colIndexDir = /scratch/index/indri_5_7/trec7n8\n",
      "knowledgGraph index Dir = /scratch/index/indri_5_7/trec7n8\n",
      "col Qrels File Name = /home/fj9124/projects/ir/seq_kb_ir/qrels/trec7n8/qrels.csv\n",
      "runsFileName = /home/fj9124/projects/ir/seq_kb_ir/runs/wsdmImpr/trec7n8/cnet/indriRunQuery.runs\n",
      "evalsFileName = /home/fj9124/projects/ir/seq_kb_ir/evals/wsdmImpr/trec7n8/cnet/indriRunQuery.evals\n",
      "dumpindexStatementFilename = /tmp/sbsb/statement_trec7n8_wsdmImpr_cnet\n",
      "occuranceCountFilename = /home/fj9124/projects/ir/seq_kb_ir/occuranceCount/occuranceCount\n",
      "countsResultsFile = /home/fj9124/projects/ir/seq_kb_ir/occuranceCount/results/trec7n8.txt\n",
      "conceptnet5RelAllFilename = /scratch/saeid/data/conceptnet5_simp.csv\n",
      "cfgTmpOutFileName = /home/fj9124/projects/ir/seq_kb_ir/configs/trec7n8/wsdmImpr/cnet/indriRunQuery.cfg.tmp\n",
      "docIdNameMapFileName = /home/fj9124/projects/ir/seq_kb_ir/occuranceCount/results/trec7n8_docIdNameMap.txt\n"
     ]
    }
   ],
   "source": [
    "knowledgGraph = \"cnet\" # conceptnet5AssocMod, gov, conceptnet5AssocMi, conceptnet5AssocHdl\n",
    "#knowledgGraph_ = ''.join([k.capitalize() if i>0 else k for i, k in enumerate(knowledgGraph)])\n",
    "collection = \"trec7n8\"\n",
    "method = \"wsdmImpr\" # hal, mi, assoc, assocMi, assocHal, assoc2\n",
    "simMeasure = \"\" # mi, cnet\n",
    "projectDir = \"/home/fj9124/projects/ir/seq_kb_ir/\" \n",
    "indexDir = \"/scratch/index/indri_5_7/\"\n",
    "colMethodConfigsDir = os.path.join(projectDir, \"configs\", collection, method)\n",
    "cfgInFileName = os.path.join(projectDir, \"configs\", collection, \"query\" + collection.capitalize()) \n",
    "print(\"cfgInFileName =\", cfgInFileName)\n",
    "cfgOutFileName=os.path.join(colMethodConfigsDir, knowledgGraph, \"indriRunQuery.cfg\") \n",
    "print(\"cfgOutFileName =\", cfgOutFileName)\n",
    "colIndexDir = os.path.join(indexDir, collection) \n",
    "print(\"colIndexDir =\", colIndexDir)\n",
    "if knowledgGraph in {\"cnet\"}:\n",
    "    knowledgGraphIndexDir = os.path.join(indexDir, collection)   \n",
    "else:\n",
    "    knowledgGraphIndexDir = os.path.join(indexDir, knowledgGraph)   \n",
    "print(\"knowledgGraph index Dir =\", knowledgGraphIndexDir)\n",
    "graphsDir = os.path.join(projectDir, \"graphs\")\n",
    "#methodGraphsDir = os.path.join(graphsDir, method)\n",
    "methodGraphsFileName = []\n",
    "if method == \"hal\":\n",
    "    methodGraphsFileName = [os.path.join(graphsDir, method, knowledgGraph + \".txt\")]\n",
    "    print(\"methodGraphsFileName =\", methodGraphsFileName)\n",
    "elif method.translate(None, string.digits) in {\"mi\", \"assoc\"}:\n",
    "    methodGraphsFileName = [os.path.join(graphsDir, method.translate(None, string.digits), knowledgGraph, \"graph.txt\")]\n",
    "    print(\"methodGraphsFileName =\", methodGraphsFileName)\n",
    "elif method in {\"assocMi\"}:\n",
    "    if knowledgGraph == \"conceptnet5AssocGov\":\n",
    "        methodGraphsFileName = [os.path.join(graphsDir, \"assoc\", collection, \"conceptnet5AssocMod\" + \".txt\")]\n",
    "        methodGraphsFileName += [os.path.join(graphsDir, \"mi\", collection, \"gov\" + \".txt\")]\n",
    "        print(\"methodGraphsFileName =\", methodGraphsFileName)\n",
    "elif method in {\"assocHal\"}:\n",
    "    if knowledgGraph == \"conceptnet5AssocGov\":\n",
    "        methodGraphsFileName = [os.path.join(graphsDir, \"assoc\", collection, \"conceptnet5AssocMod\" + \".txt\")]\n",
    "        methodGraphsFileName += [os.path.join(graphsDir, \"hal\", \"gov\" + \".txt\")]\n",
    "        print(\"methodGraphsFileName =\", methodGraphsFileName)\n",
    "qrelsDir = os.path.join(projectDir, \"qrels\")\n",
    "colQrelsDir = os.path.join(qrelsDir, collection)\n",
    "colQrelsFileName = os.path.join(colQrelsDir, \"qrels.csv\")\n",
    "print(\"col Qrels File Name =\", colQrelsFileName)\n",
    "runsFileName = os.path.join(projectDir, \"runs\", method, collection, knowledgGraph, \"indriRunQuery.runs\") \n",
    "print(\"runsFileName =\", runsFileName)\n",
    "evalsFileName = os.path.join(projectDir, \"evals\", method, collection, knowledgGraph, \"indriRunQuery.evals\") \n",
    "print(\"evalsFileName =\", evalsFileName)\n",
    "dumpindexStatementFilename = os.path.join(\"/\", \"tmp\", \"sbsb\", \"statement\" + \"_\" + collection + \"_\" + method + \"_\" + knowledgGraph )\n",
    "print(\"dumpindexStatementFilename =\", dumpindexStatementFilename)\n",
    "occuranceCountFilename = os.path.join(projectDir, \"occuranceCount\", \"occuranceCount\")\n",
    "print(\"occuranceCountFilename =\", occuranceCountFilename)\n",
    "countsResultsFile = os.path.join(projectDir,\"occuranceCount\",\"results\",collection+\".txt\")\n",
    "print(\"countsResultsFile =\", countsResultsFile)\n",
    "conceptnet5RelAllFilename =\"/scratch/saeid/data/conceptnet5_simp.csv\"\n",
    "print(\"conceptnet5RelAllFilename =\", conceptnet5RelAllFilename)\n",
    "cfgTmpOutFileName = cfgOutFileName + \".tmp\"\n",
    "print(\"cfgTmpOutFileName =\", cfgTmpOutFileName)\n",
    "docIdNameMapFileName = os.path.join(projectDir, \"occuranceCount\", \"results\", collection + \"_docIdNameMap.txt\")\n",
    "print(\"docIdNameMapFileName =\", docIdNameMapFileName)\n",
    "#dumpindexResFilename = os.path.join(projectDir,\"occuranceCount\",\"results\",collection +\"_dumpindexRes\"+\".txt\")\n",
    "#print(\"dumpindexResFilename =\", dumpindexResFilename)\n",
    "#expansionCount = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indriRunQuery_hist_= dict()\n",
    "dumpindex_hist_ = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# trec_eval -q ../qrels/trec7n8/qrels.csv indriRunQuery.res | grep map | grep -v all | awk '{if ($3 < 0.1) printf $2 \",\"}'\n",
    "if collection == \"gov\":\n",
    "    diffTopics = [4, 6, 7, 8, 9, 10, 12, 18, 19, 21, 22, 24, 26, 27, 30, 31, 32, 33, 36, 37, 40, 45, 46, 47, 48, 50, 52, 53, 54, 57, 58, 59, 63, 65, 66, 67, 70, 73, 78, 79, 80, 82, 84, 86, 92, 93, 94, 95, 96, 98, 99, 101, 102, 103, 104, 105, 106, 107, 109, 110, 111, 112, 113, 115, 116, 117, 118, 120, 123, 125, 126, 127, 129, 132, 133, 134, 135, 136, 137, 140, 142, 143, 147, 148, 149, 152, 153, 154, 155, 156, 157, 158, 160, 161, 162, 163, 165, 166, 167, 168, 169, 171, 175, 176, 177, 178, 179, 180, 182, 183, 185, 186, 188, 189, 191, 192, 194, 195, 196, 197, 200, 201, 202, 203, 204, 207, 208, 211, 212, 214, 215, 218, 219, 221, 224]\n",
    "if collection == \"robust\":\n",
    "    diffTopics = [301, 305, 309, 314, 315, 318, 319, 320, 322, 325, 327, 332, 336, 340, 342, 343, 344, 345, 346, 347, 350, 352, 354, 355, 356, 359, 363, 367, 371, 372, 376, 378, 379, 380, 381, 383, 386, 388, 389, 390, 393, 394, 397, 398, 401, 405, 409, 412, 419, 421, 426, 432, 435, 436, 437, 439, 440, 442, 448, 449, 605, 608, 610, 620, 622, 625, 626, 627, 638, 650, 651, 655, 659, 666, 668, 680, 683, 684, 688, 689, 690]\n",
    "if collection == \"trec7n8\":\n",
    "    diffTopics = [352,354,356,359,363,367,370,371,372,373,376,378,379,381,383,386,388,389,390,393,394,397,398,401,405,408,419,421,422,426,432,435,436,437,439,440,442,443,445,447,448,449]\n",
    "diffTopic = [str(i) for i in diffTopics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generate docIdNameMapFileName\n",
    "if not os.path.isfile(occuranceCountFilename):\n",
    "    cmd = ' '.join([occuranceCountFilename, colIndexDir, \"dm\", \">\", docIdNameMapFileName])\n",
    "    res = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE).stdout.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', 'FR940104-1-00001']\n",
      "LA082389-0020 \t 441223\n",
      "FBIS3-32596 \t 246524\n",
      "LA011890-0198 \t 465144\n",
      "LA011890-0199 \t 465145\n",
      "LA011890-0196 \t 465142\n",
      "LA011890-0197 \t 465143\n",
      "LA011890-0194 \t 465140\n",
      "FT932-3799 \t 88713\n",
      "LA011890-0192 \t 465138\n",
      "LA011890-0193 \t 465139\n",
      "LA011890-0190 \t 465136\n",
      "LA011890-0191 \t 465137\n",
      "FT932-3796 \t 88710\n",
      "FT924-2868 \t 187695\n",
      "FT924-2869 \t 187696\n",
      "FT932-3797 \t 88711\n",
      "FT924-2864 \t 187691\n",
      "FT924-2865 \t 187692\n",
      "FT924-2866 \t 187693\n",
      "FT924-2867 \t 187694\n",
      "FT924-2860 \t 187687\n",
      "FT924-2861 \t 187688\n"
     ]
    }
   ],
   "source": [
    "with open(docIdNameMapFileName, 'r') as f:\n",
    "    reader = list(csv.reader(f, delimiter = ' '))\n",
    "    print(reader[0])\n",
    "    docIdNameMap = {i[1]:i[0] for i in reader}\n",
    "    docNameIdMap = {i[0]:i[1] for i in reader}\n",
    "for i, (k, v) in enumerate(docIdNameMap.iteritems()):\n",
    "    print (k, '\\t', v)\n",
    "    if (i>20):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of count_history = 81446\n",
      "#uw(#4( storm ) #4( be appear in sky )) \t 0\n",
      "#uw(#4( work on sundai )) \t 137\n",
      "#uw(#4( calm time )) \t 23\n",
      "#uw(#4( kill ) #4( prai )) \t 394\n",
      "#uw(#4( bank ) #4( teller offic )) \t 5\n",
      "#uw(#4( water ) #4( hydrogen )) \t 347\n",
      "#uw(#4( drug ) #4( take )) \t 8847\n",
      "#uw(#4( space ) #4( mostli empti )) \t 3\n",
      "#uw(#4( chang ) #4( edg )) \t 3803\n",
      "#uw(#4( happin )) \t 0\n",
      "#uw(#4( trade ) #4( countri )) \t 45838\n",
      "#uw(#4( game ) #4( leisur activ )) \t 23\n",
      "#uw(#4( round in shape )) \t 4\n",
      "#uw(#4( good ) #4( other person )) \t 411\n",
      "#uw(#4( smoke ) #4( fire )) \t 1842\n",
      "#uw(#4( spice up salad )) \t 1\n",
      "#uw(#4( be wound )) \t 410\n",
      "#uw(#4( good ) #4( hear nice new )) \t 0\n",
      "#uw(#4( teacher ) #4( teach student of class )) \t 0\n",
      "#uw(#4( deliveri )) \t 11268\n",
      "#uw(#4( bank ) #4( teller monei )) \t 23\n",
      "#uw(#4( swap share )) \t 106\n"
     ]
    }
   ],
   "source": [
    "count_history = dict()\n",
    "if os.path.isfile(countsResultsFile):\n",
    "    with open(countsResultsFile, 'r') as f:\n",
    "        reader = csv.reader(f, delimiter = \"\\t\")\n",
    "        count_history = {k:int(float(v)) for k,v in list(reader)}\n",
    "    print(\"size of count_history =\", len(count_history))\n",
    "    for i, (k, v) in enumerate(count_history.iteritems()):\n",
    "        print (k, '\\t', v)\n",
    "        if (i>20):\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of conceptnet5RelAll = 2965684\n",
      "tripolitan \t tripoline \t Synonym\n",
      "tripolitan \t tripoli \t RelatedTo\n",
      "age of nemesis \t band \t IsA\n",
      "age of nemesis \t organisation \t InstanceOf\n",
      "age of nemesis \t progressive rock \t dbpedia/genre\n",
      "age of nemesis \t progressive metal \t dbpedia/genre\n",
      "joseph john annabring \t person \t InstanceOf\n",
      "british rail class 438 \t mean of transportation \t InstanceOf\n",
      "british rail class 438 \t train \t InstanceOf\n"
     ]
    }
   ],
   "source": [
    "conceptnet5RelAll = defaultdict(dict)\n",
    "conceptnet5RelAllInv = defaultdict(dict)\n",
    "with open(conceptnet5RelAllFilename, 'r') as f:\n",
    "    reader = csv.reader(f, delimiter = \",\")\n",
    "    for line in list(reader):\n",
    "        if line[0].strip() != \"\" and line[1].strip() != \"\" and line[2].strip() != \"\":\n",
    "            if all(c in string.printable for c in line[0]) and all(c in string.printable for c in line[1]) and all(c in string.printable for c in line[2]):\n",
    "                conceptnet5RelAll[line[1].strip()][line[2].strip()] = line[0].strip()\n",
    "                conceptnet5RelAllInv[line[2].strip()][line[1].strip()] = line[0].strip()\n",
    "print(\"size of conceptnet5RelAll =\", len(conceptnet5RelAll))\n",
    "for i, (k1, k2v) in enumerate(conceptnet5RelAll.iteritems()):\n",
    "    for j, (k2, v) in enumerate(k2v.iteritems()):\n",
    "        if (i>3 or j>3):\n",
    "            break\n",
    "        print (k1, '\\t', k2, '\\t', v)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing: ['go', 'read']\n"
     ]
    }
   ],
   "source": [
    "def splitStemText(text):\n",
    "    stemmedWords = []\n",
    "    text = re.sub('/|-|\\\"|_',' ', text) # replace - and slash with space\n",
    "    if method in {\"assocRestful\", \"lr\", \"wsdmImpr\"}:\n",
    "        for text_ in text.split(' '):\n",
    "            text = text_.replace(\" \", \"_\")\n",
    "            url = \"http://conceptnet5.media.mit.edu/data/5.4/uri?language=en&text=\" + text\n",
    "            response = urllib2.urlopen(url)\n",
    "            data = response.read()\n",
    "            data_j = json.loads(data)\n",
    "            text = os.path.basename(data_j[u'uri'])\n",
    "            url = \"http://conceptnet5.media.mit.edu/data/5.4/uri?language=en&text=\" + text\n",
    "            response = urllib2.urlopen(url)\n",
    "            data = response.read()\n",
    "            data_j = json.loads(data)\n",
    "            #print(\"data_j =\", data_j)\n",
    "            #print(\"data_j[u'uri'] =\", data_j[u'uri'])\n",
    "            stemmedWords += [str(os.path.basename(data_j[u'uri']))]\n",
    "        #print(\"stemmedWords =\", stemmedWords)\n",
    "        return (stemmedWords)\n",
    "    else:\n",
    "        words = text.split()\n",
    "        for w in words:\n",
    "            w = re.sub('\\(|\\)|\\'s|,','', w) # remove paranthesis, apostrophe s, comma\n",
    "            w = re.sub('\\'','', w) # remove apostrophe\n",
    "            cmd = \"dumpindex \" + knowledgGraphIndexDir + \" t \" + w + \" | head -n1\"\n",
    "            #print(\"cmd =\", cmd)\n",
    "            stemmedWords.append(subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE).stdout.read().split()[1])\n",
    "        return (stemmedWords)\n",
    "\n",
    "print (\"testing:\", splitStemText(\"going reading\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def analyseQueries():\n",
    "    #    #print(\"intCoeff =\", intCoeff)\n",
    "    with open(cfgInFileName, 'r') as inFile:\n",
    "        reader = inFile.read()\n",
    "        soup = BeautifulSoup(reader, 'lxml')\n",
    "        if collection in {\"aquaint\", \"gov\"}:\n",
    "            tags = soup.find_all(['doc'])\n",
    "        elif collection in {\"robust\", \"trec7n8\", \"gov2\"}:\n",
    "            tags = soup.find_all(['top'])\n",
    "            #print(tags)\n",
    "        \n",
    "        origWordsAll = dict()\n",
    "        for tag in tags:\n",
    "            \n",
    "            if collection in {\"aquaint\", \"gov\"}:\n",
    "                docno = (tag.find('docno').string).strip()\n",
    "            elif collection in {\"robust\", \"trec7n8\", \"gov2\"}:\n",
    "                result = re.search('<num> Number: (.*)\\n', str(tag))                \n",
    "                docno = result.group(1).strip()\n",
    "            \n",
    "            if collection in {\"aquaint\", \"gov\"}:\n",
    "                origWords = splitStemText(tag.find('text').string.strip())\n",
    "            elif collection in {\"robust\", \"trec7n8\", \"gov2\"}:\n",
    "                result = re.search('<title>(.*)\\n', str(tag))                \n",
    "                origWords = splitStemText(result.group(1).strip())\n",
    "            #print(\"origWords =\", origWords)\n",
    "            origWordsAll[docno] = origWords\n",
    "            \n",
    "    return (origWordsAll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "colDocumentCount = 472526\n"
     ]
    }
   ],
   "source": [
    "cmd = ' '.join([\"dumpindex\", colIndexDir, \"s\", \"|\", \"awk\", \"\\'NR==2{print $2}\\'\"])\n",
    "res = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE).stdout.read()\n",
    "colDocumentCount = int(np.float(res))\n",
    "print(\"colDocumentCount =\", colDocumentCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "def genMultSimpQuery(argss):\n",
    "    #    #print(\"intCoeff =\", intCoeff)\n",
    "    soupNew = BeautifulSoup(\"\", 'lxml')\n",
    "    parameters_tag = soupNew.new_tag(\"parameters\")\n",
    "    index_tag = soupNew.new_tag(\"index\")\n",
    "    index_tag.string = colIndexDir\n",
    "    parameters_tag.append(index_tag)\n",
    "        #print(tags)\n",
    "\n",
    "    for args in argss:\n",
    "        #print(\"args =\", args)\n",
    "        (docno, origWords_, relW, intCoeff0, intCoeff1, intCoeff2, dirCoeff, featureWeights) = args\n",
    "        #print(\"(...) =\", docno, origWords_, intCoeff0, intCoeff1, intCoeff2, dirCoeff, featureWeights)\n",
    "        doc_tag = soupNew.new_tag(\"query\")\n",
    "\n",
    "        docno_tag = soupNew.new_tag(\"number\")\n",
    "        docno_tag.string = '_'.join([docno, ('_'.join(origWords_)).replace(' ', '_'), relW.replace(' ', '_')])\n",
    "        doc_tag.append(docno_tag)\n",
    "\n",
    "        text_tag = soupNew.new_tag(\"text\")\n",
    "        text_tag.string = \"#weight(\\n\" \n",
    "\n",
    "        text_tag.string += str(intCoeff0) + \" #combine(\" \n",
    "        for ow in set(origWords_):\n",
    "            ow = regex.sub(' ', ow)\n",
    "            text_tag.string += ow + \" \"\n",
    "        text_tag.string += \")\\n\"\n",
    "\n",
    "        if len(relW)>0: \n",
    "            relText_string1 = regex.sub(' ', relW)\n",
    "            text_tag.string += str(intCoeff1) + \" #combine(\" \n",
    "            #print(\"relText_string1 =\", relText_string1)\n",
    "            text_tag.string += relText_string1.encode('utf-8')\n",
    "            text_tag.string += \")\\n\"\n",
    "\n",
    "        text_tag.string += \") \"\n",
    "\n",
    "        doc_tag.append(text_tag)\n",
    "\n",
    "        parameters_tag.append(doc_tag)\n",
    "        #print(doc_tag)\n",
    "\n",
    "    rule_tag = soupNew.new_tag(\"rule\")\n",
    "    rule_tag.string = \"method:dir,mu:\" + str(dirCoeff)\n",
    "    #rule_tag.string = \"method:two\"\n",
    "    parameters_tag.append(rule_tag)\n",
    "\n",
    "    #intCoeff_tag = soupNew.new_tag(\"intCoeff\")\n",
    "    #intCoeff_tag.string = \"0.8\"\n",
    "    #parameters_tag.append(intCoeff_tag)\n",
    "\n",
    "    threads_tag = soupNew.new_tag(\"threads\")\n",
    "    threads_tag.string = \"32\"\n",
    "    parameters_tag.append(threads_tag)\n",
    "\n",
    "    count_tag = soupNew.new_tag(\"count\")\n",
    "    count_tag.string = \"10\"\n",
    "    parameters_tag.append(count_tag)\n",
    "\n",
    "    trecFormat_tag = soupNew.new_tag(\"trecFormat\")\n",
    "    trecFormat_tag.string = \"true\"\n",
    "    parameters_tag.append(trecFormat_tag)\n",
    "\n",
    "    soupNew.append(parameters_tag)\n",
    "    #print(soupNew.prettify())\n",
    "    #print(\"cfgTmpOutFileName =\", cfgTmpOutFileName)\n",
    "    with open( cfgTmpOutFileName, 'a+') as outFile:\n",
    "        soupNewStr = str(soupNew)\n",
    "        soupNewStr = soupNewStr.replace(\"</text>\", \"\\n</text>\\n\").replace(\"query>\", \"query>\\n\").replace(\"<text>\", \"\\n<text>\\n\").replace(\"</index>\", \"</index>\\n\").replace(\"\\n<index>\", \"<index>\")\n",
    "\n",
    "        outFile.write(soupNewStr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def weightRelConcept_hist_i(docno, origWords_, ow, relW, intCoeff0, intCoeff1, intCoeff2, dirCoeff, featureWeights):\n",
    "    \n",
    "    indriRunQuery_hist = [(docno, origWords_, relW, intCoeff0, intCoeff1, intCoeff2, dirCoeff, cfgTmpOutFileName)]\n",
    "    return indriRunQuery_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def runCommandsinHist_i(cfgTmpOutFileName, indriRunQuery_hist, indriRunQuery_hist_, featureWeights):\n",
    "    with open( cfgTmpOutFileName, 'w+') as outFile:\n",
    "        outFile.write(\"\")\n",
    "    args = []\n",
    "    #print(\"indriRunQuery_hist =\", indriRunQuery_hist)\n",
    "    for (docno, origWords_, relW, intCoeff0, intCoeff1, intCoeff2, dirCoeff, cfgTmpOutFileName) in indriRunQuery_hist:\n",
    "        #print(\"cfgTmpOutFileName =\", cfgTmpOutFileName)\n",
    "        if '_'.join([docno, ('_'.join(origWords_)).replace(' ', '_'), relW.replace(' ', '_')]) not in indriRunQuery_hist_:\n",
    "            args += [(docno, origWords_, relW, intCoeff0, intCoeff1, intCoeff2, dirCoeff, featureWeights)]\n",
    "    #print(\"args =\", args)\n",
    "    if len(args)>0:\n",
    "        genMultSimpQuery(args)\n",
    "    cmd = ' '.join([\"IndriRunQuery\", cfgTmpOutFileName])\n",
    "    #print(\"cmd =\", cmd)\n",
    "    indriRun = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE).stdout.read()\n",
    "    #print(\"indriRun =\", indriRun)\n",
    "    indriRunQuery_hist_tmp = [[j for c, j in enumerate(i.split(' ')) if c in {0, 2, 4}] for i in indriRun.split('\\n') if len(i)>0]\n",
    "    #indriRunQuery_hist_ = dict()\n",
    "    for i in indriRunQuery_hist_tmp:\n",
    "        if i[0] in indriRunQuery_hist_:\n",
    "            indriRunQuery_hist_[i[0]] += [[i[1], i[2]]]\n",
    "        else:\n",
    "            indriRunQuery_hist_[i[0]] = [[i[1], i[2]]]\n",
    "    #indriRunQuery_hist_ = list(set(indriRunQuery_hist_))\n",
    "    return indriRunQuery_hist_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def weightRelConcept_hist_d(indriRunQuery_hist_, dumpindex_hist, docno, origWords_, ow, relW, intCoeff0, intCoeff1, intCoeff2, dirCoeff, featureWeights):\n",
    "\n",
    "    #print(\"'_'.join([docno, '_'.join(origWords_), relW]).replace(' ', '_') =\", '_'.join([docno, '_'.join(origWords_), relW]).replace(' ', '_'))\n",
    "    indriRunQuery_hist_1 = indriRunQuery_hist_['_'.join([docno, '_'.join(origWords_), relW]).replace(' ', '_')]\n",
    "    #print(\"indriRunQuery_hist_1 =\", indriRunQuery_hist_1)\n",
    "    topTDocs = [i[0] for i in indriRunQuery_hist_1]\n",
    "    #print(\"topTDocs =\", topTDocs)\n",
    "    \n",
    "    expr = \"#od4(\" + regex.sub(' ', relW) + \"):\" + ','.join(topTDocs)\n",
    "    cmd2 = (\"efb\", expr)\n",
    "    #print(\"expr =\", expr)\n",
    "    if cmd2 not in dumpindex_hist:\n",
    "        dumpindex_hist.add(cmd2)\n",
    "    \n",
    "    #print(\"topTDocs =\", topTDocs)\n",
    "    \n",
    "    for documentName in topTDocs:\n",
    "        cmd2 = (\"dcf\", docIdNameMap[documentName])\n",
    "        if cmd2 not in dumpindex_hist:\n",
    "            dumpindex_hist.add(cmd2)\n",
    "\n",
    "    for ow in origWords_:\n",
    "        cmd2 = (\"efb\", \"#uw(#4( \" + regex.sub(' ', ow) + \" ) #4( \" + regex.sub(' ', relW) + \" )):\" + ','.join(topTDocs))\n",
    "        if cmd2 not in dumpindex_hist:\n",
    "            dumpindex_hist.add(cmd2)\n",
    "        \n",
    "    for j1, ow1 in enumerate(origWords_):\n",
    "        for j2, ow2 in enumerate(origWords_):\n",
    "            if j1>j2:\n",
    "                cmd2 = (\"efb\", \"#uw(#4( \" + regex.sub(' ', ow1) + \" ) #4( \" + regex.sub(' ', ow2) + \" ) #4( \" + regex.sub(' ', relW) + \" )):\" + ','.join(topTDocs))\n",
    "                if cmd2 not in dumpindex_hist:\n",
    "                    dumpindex_hist.add(cmd2)\n",
    "    return dumpindex_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "def runCommandsinHist_d(docno, dumpindex_hist, dumpindex_hist_):\n",
    "    args = []\n",
    "    dumpindex_hist_dict = dict()\n",
    "    tmp = set()\n",
    "    for i in dumpindex_hist:\n",
    "        if i[1] not in dumpindex_hist_.get(docno + \"_\" + i[0], dict()):\n",
    "            if i[1] in tmp:\n",
    "                continue\n",
    "            tmp.add(i[1])\n",
    "            if i[0] in dumpindex_hist_dict:\n",
    "                dumpindex_hist_dict[i[0]] += [[re.sub(ur\"[^\\w\\d#(),:\\-\\s]+\",' ',j) for j in i[1:]]]\n",
    "            else:\n",
    "                dumpindex_hist_dict[i[0]] =  [[re.sub(ur\"[^\\w\\d#(),:\\-\\s]+\",' ',j) for j in i[1:]]]\n",
    "                \n",
    "    for k, v in dumpindex_hist_dict.iteritems():\n",
    "        fileName = dumpindexStatementFilename+\"_\"+k+\".tmp\"\n",
    "        with open(fileName, 'w') as f:\n",
    "            for i in v:\n",
    "                f.write(' '.join(i) + '\\n')\n",
    "                #print(\"i =\", i)\n",
    "        cmd = ' '.join([occuranceCountFilename, colIndexDir, k, fileName])\n",
    "        #print(\"cmd =\", cmd)\n",
    "        dumpindex_hist_tmp = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE).stdout.read()\n",
    "        #print(\"dumpindex_hist_tmp =\", dumpindex_hist_tmp)\n",
    "        dumpindex_hist_tmp = dict([tuple(j for c2, j in enumerate(i.split(':')) if c2 in {0,1} ) for i in dumpindex_hist_tmp.split(\"\\n\") if len(i)>0])\n",
    "        #print(\"dumpindex_hist_tmp.get(\\\"#od4(travel)\\\",\\\"\\\") =\", dumpindex_hist_tmp.get(\"#od4(travel)\", dict()) )\n",
    "        dumpindex_hist_tmp = {k1:v1.split(',') for k1,v1 in dumpindex_hist_tmp.iteritems()}\n",
    "        dumpindex_hist_tmp = {k1:[i for i in v1 if len(i)>0] for k1,v1 in dumpindex_hist_tmp.iteritems()}\n",
    "        #dumpindex_hist_tmp = {re.sub(ur\"[^\\w\\d#()\\s]+\",' ',k1):[i for i in v1 if len(i)>0] for k1,v1 in dumpindex_hist_tmp.iteritems()}\n",
    "        #dumpindex_tmp = [i for i in dumpindex_tmp.split(\"\\n\")]\n",
    "        #print(\"dumpindex_hist_tmp =\", dumpindex_hist_tmp)\n",
    "        #dumpindex_hist_[k] = dumpindex_hist_tmp\n",
    "        dumpindex_hist_[docno + \"_\" + k] = dict(dumpindex_hist_.get(docno + \"_\" + k, dict()).items() + dumpindex_hist_tmp.items())\n",
    "    #print(\"dumpindex_hist_['efb'].get('#od4(travel)', dict())]\", dumpindex_hist_['efb'].get(\"#od4(travel)\", dict()))\n",
    "        #print(\"dumpindex_hist_[k].keys() =\", dumpindex_hist_[k].keys())\n",
    "    #print(\"dumpindex_hist_ =\", dumpindex_hist_)\n",
    "        \n",
    "    return dumpindex_hist_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "def documentLength(docno, documentName):\n",
    "    #cmd = ' '.join([\"dumpindex\", colIndexDir, \"dv\", docIdNameMap[documentName], \"|\", \"awk\", \"\\'END{print $1}\\'\"])\n",
    "    #cmd2 = [\"dv\", docIdNameMap[documentName], \"|\", \"awk\", \"\\'END{print $1}\\'\"]\n",
    "    #print(\"cmd =\", cmd)\n",
    "    #documentLength_ = int(dumpindex_colIndexDir(cmd2))\n",
    "    documentLength_ = int(dumpindex_hist_[docno + \"_\" + 'dcf'][documentName][0])\n",
    "    return documentLength_\n",
    "\n",
    "#print(\"docIdNameMap[\\\"FT944-15973\\\"] = \", docIdNameMap[\"FT944-15973\"])\n",
    "#print(\"documentLength(\\\"FT944-15973\\\") =\", documentLength(\"FT944-15973\"))\n",
    "\n",
    "def weightRelConcept(docno, origWords_, ow, relW, intCoeff0, intCoeff1, intCoeff2, dirCoeff, count_history, featureWeights):\n",
    "    \n",
    "    features = dict()\n",
    "    \n",
    "    ##########\n",
    "    #cfgTmpOutFileName = os.path.join(\"/\",\"tmp\",\"sbsb\",str(random.randint(1,1e6))+\"_tmp.cfg\")\n",
    "    ##########\n",
    "    #(docno, origWords_, relW, intCoeff0, intCoeff1, dirCoeff, cfgTmpOutFileName)\n",
    "    #genOneSimpQuery(docno, origWords_, relW, intCoeff0, intCoeff1, dirCoeff, cfgTmpOutFileName)\n",
    "    \n",
    "    #indriRun = indriRunQuery_([cfgTmpOutFileName])\n",
    "    #subprocess.Popen(' '.join([\"rm\", cfgTmpOutFileName]), shell=True, stdout=subprocess.PIPE).stdout.read()\n",
    "    #indriRun = [i.split(' ') for i in indriRun.split('\\n')]\n",
    "    #indriRun = [i for i in indriRun if len(i)>1] # remove empty arrays\n",
    "    #print(\"'_'.join([docno, '_'.join(origWords_), relW]).replace(' ', '_') =\", '_'.join([docno, '_'.join(origWords_), relW]).replace(' ', '_'))\n",
    "    indriRunQuery_hist_1 = indriRunQuery_hist_['_'.join([docno, '_'.join(origWords_), relW]).replace(' ', '_')]\n",
    "    #print(\"indriRunQuery_hist_1 =\", indriRunQuery_hist_1)\n",
    "    topTDocs = [i[0] for i in indriRunQuery_hist_1]\n",
    "    \n",
    "    #topTDocScores_d = {i[2]:np.float(i[4]) for i in indriRun}\n",
    "    topTDocScores_d = {i[0]:np.float(i[1]) for i in indriRunQuery_hist_1}\n",
    "    #print(\"topTDocScores_d =\", topTDocScores_d)\n",
    "    \n",
    "    #scores = [ np.float(i[4]) for i in indriRun]\n",
    "    #topTDocs = [i[2] for i in indriRun]\n",
    "\n",
    "    #print(\"indriRun =\", indriRun)\n",
    "    #topTDocScore = np.sum([ np.float(i[4]) for i in indriRun])\n",
    "    #expTDocScore = np.float(indriRun[0][4])\n",
    "    expTDocScore = np.float(indriRunQuery_hist_1[0][1])\n",
    "    features[\"expTDocScore\"] = expTDocScore\n",
    "    #print(\"expTDocScore =\", features[\"expTDocScore\"])\n",
    "    \n",
    "    #cmd =' '.join([\"dumpindex\", colIndexDir, \"e\", \"\\\"#od4(\" + relW + \")\\\"\"])\n",
    "    #cmd2 = [\"e\", \"\\\"#od4(\" + relW + \")\\\"\"]\n",
    "    #print(\"cmd =\", cmd)\n",
    "    #relWDocuments = dumpindex_colIndexDir(cmd2)\n",
    "    expr = \"#od4(\" + regex.sub(' ', relW) + \")\"\n",
    "    #print (\"dumpindex_hist_ =\", dumpindex_hist_)\n",
    "    #print (\"expr =\", expr)\n",
    "    #print (\"docno + '_' + 'efb' =\", docno + '_' + 'efb')\n",
    "    relWTopDocuments = list(dumpindex_hist_[docno + \"_\" + 'efb'][expr])\n",
    "    #print(\"relWTopDocuments =\", relWTopDocuments)\n",
    "    colTermFreq = np.float(relWTopDocuments[0])\n",
    "    del relWTopDocuments[0] # the first element was colTermFreq\n",
    "    #relWDocuments = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE).stdout.read()\n",
    "    #relWDocuments = [i.split(' ') for i in relWDocuments.split('\\n')]\n",
    "    #print(\"relWDocuments =\", relWDocuments)\n",
    "    #relWDocuments = [[docNameIdMap[i[0]], i[1], i[2], i[3]] for j, i in enumerate(relWDocuments) if len(i)==4 and j>0] # remove empty arrays\n",
    "    \n",
    "    #relWTopDocuments = [i for i in relWDocuments if i[0] in topTDocs]\n",
    "    #relWTopDocuments = [i for i in relWDocuments if i in topTDocs]\n",
    "    #relWTopDocuments = [[i, topTDocScores_d[i]] for i in relWDocuments if i in topTDocs] # topdocs and their scores that contain relW\n",
    "    #print(\"relWTopDocuments =\", relWTopDocuments)\n",
    "    #numerator = np.sum([int(i[1]) for i in relWTopDocuments]) # term count in the collection\n",
    "    numerator = colTermFreq # relW term count in the collection\n",
    "    #print(\"topTDocs =\", topTDocs)\n",
    "    denumerator = np.sum([documentLength(docno, i) for i in topTDocs]) # number of terms in topDocs\n",
    "    topTermFrac = numerator / np.float(denumerator)\n",
    "    #print(\"numerator, denumerator, topTermFrac =\", numerator, denumerator, topTermFrac)\n",
    "    features[\"topTermFrac\"] = topTermFrac\n",
    "    \n",
    "    numCanDocs = len(set([i[0] for i in relWTopDocuments]))\n",
    "    features[\"numCanDocs\"] = numCanDocs\n",
    "    #print(\"numCanDocs =\", features[\"numCanDocs\"])\n",
    "    \n",
    "    if numCanDocs > 0:\n",
    "        avgCDocScore = np.sum([ topTDocScores_d[i] for i in set([i for i in relWTopDocuments])])/np.float(numCanDocs)\n",
    "    else:\n",
    "        avgCDocScore = 0\n",
    "    features[\"avgCDocScore\"] = avgCDocScore\n",
    "    #print(\"avgCDocScore =\", features[\"avgCDocScore\"])\n",
    "\n",
    "    #for i in set([i[0] for i in relWTopDocuments]):\n",
    "    #    print(i, topTDocScores_d[i])\n",
    "    \n",
    "    l = [ topTDocScores_d[i] for i in set(relWTopDocuments)]\n",
    "    if len(l)>0:\n",
    "        maxCDocScore = np.max(l)\n",
    "    else:\n",
    "        maxCDocScore = 0\n",
    "    features[\"maxCDocScore\"] = maxCDocScore\n",
    "    #print(\"maxCDocScore =\", features[\"maxCDocScore\"])\n",
    "    \n",
    "    if colTermFreq>0:\n",
    "        conIdf = np.log(colDocumentCount / colTermFreq)\n",
    "    else:\n",
    "        conIdf = np.log(colDocumentCount / 1)\n",
    "    features[\"conIdf\"] = conIdf\n",
    "    #print(\"conIdf =\", features[\"conIdf\"])\n",
    "    \n",
    "    coocurDocuments = dict()\n",
    "    coocurDocumentsTop = dict()\n",
    "    for ow in origWords_:\n",
    "        #cmd = ' '.join([\"dumpindex\", colIndexDir, \"e\", \"\\\"#uw(#4( \" + ow + \" ) #4( \" + relW + \" ))\\\"\"])\n",
    "        #cmd2 = [\"e\", \"\\\"#uw(#4( \" + ow + \" ) #4( \" + relW + \" ))\\\"\"]\n",
    "        #print(\"cmd =\", cmd)\n",
    "        #coocurDocuments_ = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE).stdout.read()\n",
    "        #coocurDocuments_ = dumpindex_colIndexDir(cmd2)\n",
    "        #coocurDocuments_ = [i.split(' ') for i in coocurDocuments_.split('\\n')]\n",
    "        coocurDocuments[ow] = [i for i in dumpindex_hist_[docno + \"_\" + 'efb'][\"#uw(#4( \" + regex.sub(' ', ow) + \" ) #4( \" + regex.sub(' ', relW) + \" ))\"]]\n",
    "        #coocurDocuments[ow] = [docNameIdMap[i[0]] for i in coocurDocuments_ if len(i)==4] # remove empty arrays\n",
    "        #print(\"topTDocs =\", topTDocs)\n",
    "        coocurDocumentsTop[ow] = [i for i in coocurDocuments[ow] if i in topTDocs ] # remove empty arrays\n",
    "\n",
    "    #for k, v in coocurDocuments.iteritems():\n",
    "    #    print(k, len(v))\n",
    "        \n",
    "    #for v in coocurDocumentsTop.values():\n",
    "    #    print(v)\n",
    "    #    print(len(set(v)))\n",
    "        \n",
    "    avgColCor = np.sum([len(set(i)) for i in coocurDocuments.values()])/np.float(len(coocurDocuments)) \n",
    "    features[\"avgColCor\"] = avgColCor\n",
    "    #print(\"avgColCor =\", features[\"avgColCor\"])\n",
    "    \n",
    "    maxColCor = np.max([len(set(i)) for i in coocurDocuments.values()])\n",
    "    features[\"maxColCor\"] = maxColCor\n",
    "    #print(\"maxColCor =\", features[\"maxColCor\"])\n",
    "    \n",
    "    avgTopColCor = np.sum([len(set(i)) for i in coocurDocumentsTop.values()])/np.float(len(coocurDocumentsTop)) \n",
    "    features[\"avgTopColCor\"] = avgTopColCor\n",
    "    #print(\"avgTopColCor =\", features[\"avgTopColCor\"])\n",
    "    \n",
    "    l = [len(set(i)) for i in coocurDocumentsTop.values()]\n",
    "    if len(l)>0:\n",
    "        maxTopColCor = np.max(l)\n",
    "    else:\n",
    "        maxTopColCor = 0\n",
    "    features[\"maxTopColCor\"] = maxTopColCor\n",
    "    #print(\"maxTopColCor =\", features[\"maxTopColCor\"])\n",
    "\n",
    "    for j1, ow1 in enumerate(origWords_):\n",
    "        for j2, ow2 in enumerate(origWords_):\n",
    "            if j1>j2:\n",
    "                #coocurDocuments_ = subprocess.Popen(' '.join([\"dumpindex\", colIndexDir, \"e\", \"\\\"#uw(#4( \" + ow1 + \" ) #4( \" + ow2 + \" ) #4( \" + relW + \" ))\\\"\"]), shell=True, stdout=subprocess.PIPE).stdout.read()\n",
    "                #cmd2 = [ \"e\", \"\\\"#uw(#4( \" + ow1 + \" ) #4( \" + ow2 + \" ) #4( \" + relW + \" ))\\\"\"]\n",
    "                #coocurDocuments_ = dumpindex_colIndexDir(cmd2)\n",
    "                #coocurDocuments_ = [i.split(' ') for i in coocurDocuments_.split('\\n')]\n",
    "                coocurDocuments[ow1+ow2] = [i for i in dumpindex_hist_[docno + \"_\" + 'efb'][\"#uw(#4( \" + regex.sub(' ', ow1) + \" ) #4( \" + regex.sub(' ', ow2) + \" ) #4( \" + regex.sub(' ', relW) + \" ))\"]]\n",
    "                #coocurDocuments[ow1+ow2] = [docNameIdMap[i[0]] for i in coocurDocuments_ if len(i)==4] # remove empty arrays\n",
    "                coocurDocumentsTop[ow1+ow2] = [i for i in coocurDocuments[ow1+ow2] if i in topTDocs ] # remove empty arrays\n",
    "\n",
    "    avgColPCor = np.sum([len(set(i)) for i in coocurDocuments.values()])/np.float(len(coocurDocuments)) \n",
    "    features[\"avgColPCor\"] = avgColPCor\n",
    "    #print(\"avgColPCor =\", features[\"avgColPCor\"])\n",
    "\n",
    "    l = [len(set(i)) for i in coocurDocuments.values()]\n",
    "    if len(l)>0:\n",
    "        maxColPCor = np.max(l)\n",
    "    else:\n",
    "        maxColPCor = 0\n",
    "    features[\"maxColPCor\"] = maxColPCor\n",
    "    #print(\"maxColPCor =\", features[\"maxColPCor\"])\n",
    "    \n",
    "    avgTopColPCor = np.sum([len(set(i)) for i in coocurDocumentsTop.values()])/np.float(len(coocurDocumentsTop)) \n",
    "    features[\"avgTopColPCor\"] = avgTopColPCor\n",
    "    #print(\"avgTopColPCor =\", features[\"avgTopColPCor\"])\n",
    "\n",
    "    l = [len(set(i)) for i in coocurDocumentsTop.values()] \n",
    "    if len(l)>0:\n",
    "        maxTopColPCor = np.max(l)\n",
    "    else:\n",
    "        maxTopColPCor = 0\n",
    "    features[\"maxTopColPCor\"] = maxTopColPCor\n",
    "    #print(\"maxTopColPCor =\", features[\"maxTopColPCor\"])\n",
    "    \n",
    "    #print(\"docno, origWords_, relW =\", docno, origWords_, relW)\n",
    "    #print(\"features =\", features)\n",
    "    \n",
    "    relWg = 0\n",
    "    for feature, weight in featureWeights.iteritems():\n",
    "        relWg += features[feature] * weight\n",
    "    \n",
    "    #print(\"features.keys() =\", features.keys())\n",
    "    return relWg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# keys of rWords are important\n",
    "# values of rWords1 are important\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "def getRelWords_train(args, indriRunQuery_hist_, dumpindex_hist_):\n",
    "    #print(\"origWords =\", origWords)\n",
    "    text_string = \"\"\n",
    "    \n",
    "    count_history, rWords, docno, origWords, expansionCount, intCoeff0, intCoeff1, intCoeff2, dirCoeff, featureWeights = args\n",
    "        \n",
    "    relWords = dict()\n",
    "    #if method == \"relAll\":\n",
    "            \n",
    "    if method in {\"lr\", \"wsdmImpr\"}:\n",
    "        origWords_ = [w.replace(\" \", \"_\") for w in origWords]\n",
    "        #relWords_l = []\n",
    "        for ow in origWords_:\n",
    "            #print(\"ow =\", ow)\n",
    "            #relWords_l += list(conceptnet5RelAll[ow])\n",
    "            #print(ow, \"---\", conceptnet5RelAll[ow])\n",
    "            indriRunQuery_hist = []\n",
    "            for relW in conceptnet5RelAll[ow].keys():\n",
    "                indriRunQuery_hist += weightRelConcept_hist_i(docno, origWords_, ow, relW, intCoeff0, intCoeff1, intCoeff2, dirCoeff, featureWeights)\n",
    "            #print(\"len(indriRunQuery_hist) =\", len(indriRunQuery_hist))\n",
    "            indriRunQuery_hist_ = runCommandsinHist_i(cfgTmpOutFileName, indriRunQuery_hist, indriRunQuery_hist_, featureWeights)\n",
    "            #print(\"len(indriRunQuery_hist_) =\", len(indriRunQuery_hist_))\n",
    "            \n",
    "            dumpindex_hist = set()\n",
    "            for relW in conceptnet5RelAll[ow].keys():\n",
    "                dumpindex_hist |= weightRelConcept_hist_d(indriRunQuery_hist_, dumpindex_hist, docno, origWords_, ow, relW, intCoeff0, intCoeff1, intCoeff2, dirCoeff, featureWeights)\n",
    "            #print(\"dumpindex_hist =\", dumpindex_hist)\n",
    "            dumpindex_hist_ = runCommandsinHist_d(docno, dumpindex_hist, dumpindex_hist_)\n",
    "            #print(\"dumpindex_hist_ =\", dumpindex_hist_)\n",
    "            #print(\"len(dumpindex_hist_['ef']) =\", len(dumpindex_hist_['ef']))\n",
    "            #print(\"len(dumpindex_hist_['dcf']) =\", len(dumpindex_hist_['dcf']))\n",
    "\n",
    "    return(indriRunQuery_hist_, dumpindex_hist_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# keys of rWords are important\n",
    "# values of rWords1 are important\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "def getRelWords(args, indriRunQuery_hist_, dumpindex_hist_):\n",
    "    #print(\"origWords =\", origWords)\n",
    "    text_string = \"\"\n",
    "    \n",
    "    count_history, rWords, docno, origWords, expansionCount, intCoeff0, intCoeff1, intCoeff2, dirCoeff, featureWeights = args\n",
    "        \n",
    "    relWords = dict()\n",
    "    #if method == \"relAll\":\n",
    "        \n",
    "    if method == \"assocRestful\":\n",
    "        origWords_ = [w.replace(\" \", \"_\") for w in origWords]\n",
    "        url = \"http://conceptnet5.media.mit.edu/data/5.4/assoc/list/en/\"+','.join(origWords_)\n",
    "        response = urllib2.urlopen(url)\n",
    "        data = response.read()\n",
    "        data_j = json.loads(data)\n",
    "        relWords = {str(d[0].encode('utf-8')).replace('/c/en/', '').replace('-', ' '):np.float(d[1]) for d in data_j[u'similar'] if '/c/en/' in str(d[0].encode('utf-8'))}\n",
    "    \n",
    "    elif method in {\"lr\", \"wsdmImpr\"}:\n",
    "        origWords_ = [w.replace(\" \", \"_\") for w in origWords]\n",
    "        #relWords_l = []\n",
    "\n",
    "        for ow in origWords_:\n",
    "            #relWords_l += list(conceptnet5RelAll[ow])\n",
    "            #print(ow, \"---\", conceptnet5RelAll[ow])\n",
    "            for relW in conceptnet5RelAll[ow].keys():\n",
    "                relWg = weightRelConcept(docno, origWords_, ow, relW, intCoeff0, intCoeff1, intCoeff2, dirCoeff, count_history, featureWeights)\n",
    "                if relW in relWords:\n",
    "                    relWords[relW] += relWg\n",
    "                else:\n",
    "                    relWords[relW] = relWg  \n",
    "    \n",
    "    else:\n",
    "        count_history = countExpr(count_history, origWords, rWords)\n",
    "        expressions = set()\n",
    "        for ow in origWords:\n",
    "            rWords_ow = rWords.get(ow, [])\n",
    "            expressions.add(\"#4( \" + ow.translate(None, string.punctuation) + \" )\")\n",
    "            if ow.strip() == \"\":\n",
    "                continue\n",
    "            if simMeasure == \"mi\":\n",
    "                N_w = countExpr_get_1(count_history, ow)\n",
    "            for rw_wv in rWords_ow:\n",
    "                (rw_w, rw_v) = rw_wv.items()[0]\n",
    "                if rw_w.strip() == \"\":\n",
    "                    continue           \n",
    "                if simMeasure == \"mi\":\n",
    "                    N_v = countExpr_get_1(count_history, rw_w)\n",
    "                    N_wv = countExpr_get_2(count_history, ow, rw_w)\n",
    "                    rw_v_ = mi_(N_w, N_v, N_wv)\n",
    "                elif simMeasure == \"cnet\":\n",
    "                    rw_v_ = rw_v\n",
    "                relWords[rw_w] = relWords.get(rw_w, 0) + rw_v_\n",
    "    \n",
    "    relWords_sorted = sorted(relWords.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    #print (\"origWords =\", origWords)\n",
    "    #print (\"relWords_sorted =\", relWords_sorted)\n",
    "    #counter = 0\n",
    "    relWords_sel = []\n",
    "    for counter, (rw_w, rw_v) in enumerate(relWords_sorted):\n",
    "        if (counter >= expansionCount):\n",
    "            break\n",
    "        if  all(c in string.printable for c in rw_w):\n",
    "            if rw_w not in origWords:\n",
    "                rw_w = regex.sub(' ', rw_w)\n",
    "                text_string += rw_w + \" \"\n",
    "                relWords_sel += [rw_w]\n",
    "        #counter += 1\n",
    "    #print(\"counter =\", counter)\n",
    "    #rint(\"relWords_sel =\", relWords_sel)\n",
    "#    return(relWords_sel)\n",
    "    return(count_history, text_string, relWords_sel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getAllRelWords_train(count_history, rWords, origWordsAll, expansionCount1, N1, expansionCount2, intCoeff0, intCoeff1, intCoeff2, dirCoeff, featureWeights, indriRunQuery_hist_, dumpindex_hist_):\n",
    "    relText_stringAll = []\n",
    "    relText_string = dict()\n",
    "    relWords_sel = dict()\n",
    "    for c1, (docno, origWords) in enumerate(origWordsAll.iteritems()):\n",
    "        print(\"c1 =\", c1, \"docno =\", docno, \"--- origWords =\", origWords)\n",
    "        args = (count_history, rWords, docno, origWords, expansionCount1, intCoeff0, intCoeff1, intCoeff2, dirCoeff, featureWeights)\n",
    "        indriRunQuery_hist_, dumpindex_hist_ = getRelWords_train(args, indriRunQuery_hist_, dumpindex_hist_)\n",
    "        #print(\"c1 =\", c1, \"- len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) =\", len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())))\n",
    "    \n",
    "    #print(\"relText_stringAll =\", relText_stringAll)\n",
    "    return (indriRunQuery_hist_, dumpindex_hist_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getAllRelWords(count_history, rWords, origWordsAll, expansionCount1, N1, expansionCount2, intCoeff0, intCoeff1, intCoeff2, dirCoeff, featureWeights, indriRunQuery_hist_, dumpindex_hist_):\n",
    "    relText_stringAll = []\n",
    "    relText_string = dict()\n",
    "    relWords_sel = dict()\n",
    "    for c1, (docno, origWords) in enumerate(origWordsAll.iteritems()):\n",
    "        #print(str(c1), \"---\", docno, \"---\", origWords)\n",
    "        args = (count_history, rWords, docno, origWords, expansionCount1, intCoeff0, intCoeff1, intCoeff2, dirCoeff, featureWeights)\n",
    "        count_history, relText_string_, relWords_sel_ = getRelWords(args, indriRunQuery_hist_, dumpindex_hist_)\n",
    "        #print(\"len(indriRunQuery_hist_), len(dumpindex_hist_['dcf']), len(dumpindex_hist_['ef']) =\", len(indriRunQuery_hist_), len(dumpindex_hist_['dcf']), len(dumpindex_hist_['ef']))\n",
    "        relText_string[docno] = relText_string_\n",
    "        relWords_sel[docno] = relWords_sel_[0:N1]\n",
    "        #print(docno, \"---\", origWords, \"---\", relWords_sel_)\n",
    "    relText_stringAll += [relText_string]\n",
    "    \"\"\"\n",
    "    relText_string = dict()\n",
    "    relWords_sel = dict()\n",
    "    for docno, origWords_new in relWords_sel.iteritems():\n",
    "        args = (count_history, rWords, docno, origWords, expansionCount1, intCoeff0, intCoeff1, intCoeff2, dirCoeff, featureWeights)\n",
    "        count_history, relText_string_, relWords_sel_ = getRelWords(args, indriRunQuery_hist_, dumpindex_hist_)\n",
    "        relText_string[docno] = relText_string_\n",
    "        relWords_sel[docno] = relWords_sel_\n",
    "    relText_stringAll += [relText_string]\n",
    "    \"\"\"\n",
    "    #print(\"relText_stringAll =\", relText_stringAll)\n",
    "    return (count_history, relText_stringAll, relWords_sel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "def genQueries(cfgOutFileName_, origWordsAll, relWords_sel2, relText_stringAll, intCoeff0, intCoeff1, intCoeff2, dirCoeff):\n",
    "    #    #print(\"intCoeff =\", intCoeff)\n",
    "    with open(cfgInFileName, 'r') as inFile:\n",
    "        reader = inFile.read()\n",
    "        soupNew = BeautifulSoup(\"\", 'lxml')\n",
    "        parameters_tag = soupNew.new_tag(\"parameters\")\n",
    "        index_tag = soupNew.new_tag(\"index\")\n",
    "        index_tag.string = colIndexDir\n",
    "        parameters_tag.append(index_tag)\n",
    "            #print(tags)\n",
    "\n",
    "        for docno, origWords in origWordsAll.iteritems():\n",
    "            doc_tag = soupNew.new_tag(\"query\")\n",
    "\n",
    "            docno_tag = soupNew.new_tag(\"number\")\n",
    "            docno_tag.string = docno\n",
    "            doc_tag.append(docno_tag)\n",
    "\n",
    "            text_tag = soupNew.new_tag(\"text\")\n",
    "            text_tag.string = \"#weight(\\n\" \n",
    "                    \n",
    "            text_tag.string += str(intCoeff0) + \" #combine(\" \n",
    "            for ow in set(origWords):\n",
    "                ow = regex.sub(' ', ow)\n",
    "                text_tag.string += ow + \" \"\n",
    "            text_tag.string += \")\\n\"\n",
    "\n",
    "            if len(relText_stringAll[0])>0:\n",
    "                if len(relText_stringAll[0][docno]) > 3: \n",
    "                    relText_string1 = regex.sub(' ', relText_stringAll[0][docno])\n",
    "                    text_tag.string += str(intCoeff1) + \" #combine(\" \n",
    "                    #print(\"relText_string1 =\", relText_string1)\n",
    "                    text_tag.string += relText_string1.encode('utf-8')\n",
    "                    text_tag.string += \")\\n\"\n",
    "            \n",
    "            \"\"\"\n",
    "            #print(len(relText_stringAll[1]))\n",
    "            if len(relText_stringAll[1])>0:\n",
    "                if len(relText_stringAll[1][docno]) > 3: \n",
    "                    relText_string2 = regex.sub(' ', relText_stringAll[1][docno])\n",
    "                    text_tag.string += str(intCoeff2) + \" #combine(\" \n",
    "                    #print(\"relText_string2 =\", relText_string2)\n",
    "                    text_tag.string += relText_string2.encode('utf-8')\n",
    "                    text_tag.string += \")\\n\"\n",
    "            \"\"\"\n",
    "            #print(\"origWords =\", origWords)\n",
    "            #print(\"relWords_sel1 =\", relWords_sel1)\n",
    "            #print(\"relWords_sel2 =\", relWords_sel2)\n",
    "            \n",
    "            \n",
    "            text_tag.string += \") \"\n",
    "\n",
    "            doc_tag.append(text_tag)\n",
    "\n",
    "            parameters_tag.append(doc_tag)\n",
    "            #print(doc_tag)\n",
    "\n",
    "        rule_tag = soupNew.new_tag(\"rule\")\n",
    "        rule_tag.string = \"method:dir,mu:\" + str(dirCoeff)\n",
    "        #rule_tag.string = \"method:two\"\n",
    "        parameters_tag.append(rule_tag)\n",
    "\n",
    "        #intCoeff_tag = soupNew.new_tag(\"intCoeff\")\n",
    "        #intCoeff_tag.string = \"0.8\"\n",
    "        #parameters_tag.append(intCoeff_tag)\n",
    "\n",
    "        threads_tag = soupNew.new_tag(\"threads\")\n",
    "        threads_tag.string = \"32\"\n",
    "        parameters_tag.append(threads_tag)\n",
    "\n",
    "        count_tag = soupNew.new_tag(\"count\")\n",
    "        count_tag.string = \"1000\"\n",
    "        parameters_tag.append(count_tag)\n",
    "\n",
    "        trecFormat_tag = soupNew.new_tag(\"trecFormat\")\n",
    "        trecFormat_tag.string = \"true\"\n",
    "        parameters_tag.append(trecFormat_tag)\n",
    "\n",
    "        soupNew.append(parameters_tag)\n",
    "        #print(soupNew.prettify())\n",
    "    #print(\"outFileName =\", outFileName)\n",
    "    with open( cfgOutFileName_, 'w') as outFile:\n",
    "        soupNewStr = str(soupNew)\n",
    "        soupNewStr = soupNewStr.replace(\"</text>\", \"\\n</text>\\n\").replace(\"query>\", \"query>\\n\").replace(\"<text>\", \"\\n<text>\\n\").replace(\"</index>\", \"</index>\\n\").replace(\"\\n<index>\", \"<index>\")\n",
    "\n",
    "        outFile.write(soupNewStr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def precisionCompute_train(count_history, intCoeffs0, intCoeffs1, intCoeffs2, expansionCounts1, expansionCounts2, dirCoeffs, N1s, featureWeights_, indriRunQuery_hist_, dumpindex_hist_):\n",
    "    #randNum = random.randint(1,1e9)\n",
    "    origWordsAll = analyseQueries()\n",
    "    indriRunQuery_hist_, dumpindex_hist_ = getAllRelWords_train(count_history, rWords, origWordsAll, expansionCounts1[0], N1s[0], expansionCounts2[0], intCoeffs0[0], intCoeffs1[0], intCoeffs2[0], dirCoeffs[0], featureWeights_[0], indriRunQuery_hist_, dumpindex_hist_)\n",
    "                                    \n",
    "    return origWordsAll, count_history, indriRunQuery_hist_, dumpindex_hist_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c1 = 0 docno = 407 --- origWords = ['poach', 'wildlife', 'preserve']\n",
      "c1 = 0 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 347 0 0\n",
      "c1 = 1 docno = 406 --- origWords = [\"parkinson's\", 'disease']\n",
      "c1 = 1 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 677 0 0\n",
      "c1 = 2 docno = 405 --- origWords = ['cosmic', 'event']\n",
      "c1 = 2 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 825 0 0\n",
      "c1 = 3 docno = 404 --- origWords = ['ireland', 'peace', 'talk']\n",
      "c1 = 3 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 1489 0 0\n",
      "c1 = 4 docno = 403 --- origWords = ['osteoporosis']\n",
      "c1 = 4 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 1502 0 0\n",
      "c1 = 5 docno = 402 --- origWords = ['behavioral', 'genetics']\n",
      "c1 = 5 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 1524 0 0\n",
      "c1 = 6 docno = 401 --- origWords = ['foreign', 'minority', 'germany']\n",
      "c1 = 6 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 1724 0 0\n",
      "c1 = 7 docno = 400 --- origWords = ['amazon', 'rain', 'forest']\n",
      "c1 = 7 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 2244 0 0\n",
      "c1 = 8 docno = 409 --- origWords = ['legal', 'pan', 'be', '103']\n",
      "c1 = 8 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 2833 0 0\n",
      "c1 = 9 docno = 408 --- origWords = ['tropical', 'storm']\n",
      "c1 = 9 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 3005 0 0\n",
      "c1 = 10 docno = 366 --- origWords = ['commercial', 'cyanide', 'use']\n",
      "c1 = 10 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 3157 0 0\n",
      "c1 = 11 docno = 420 --- origWords = ['carbon', 'monoxide', 'poison']\n",
      "c1 = 11 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 3684 0 0\n",
      "c1 = 12 docno = 364 --- origWords = ['rabies']\n",
      "c1 = 12 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 3708 0 0\n",
      "c1 = 13 docno = 422 --- origWords = ['art', 'steal', 'forge']\n",
      "c1 = 13 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 4417 0 0\n",
      "c1 = 14 docno = 425 --- origWords = ['counterfeit', 'money']\n",
      "c1 = 14 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 4965 0 0\n",
      "c1 = 15 docno = 363 --- origWords = ['transportation', 'tunnel', 'disaster']\n",
      "c1 = 15 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 5183 0 0\n",
      "c1 = 16 docno = 379 --- origWords = ['mainstreaming']\n",
      "c1 = 16 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 5183 0 0\n",
      "c1 = 17 docno = 378 --- origWords = ['euro', 'opposition']\n",
      "c1 = 17 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 5264 0 0\n",
      "c1 = 18 docno = 416 --- origWords = ['three', 'gorge', 'project']\n",
      "c1 = 18 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 5430 0 0\n",
      "c1 = 19 docno = 417 --- origWords = ['creativity']\n",
      "c1 = 19 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 5461 0 0\n",
      "c1 = 20 docno = 410 --- origWords = ['schengen', 'agreement']\n",
      "c1 = 20 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 5515 0 0\n",
      "c1 = 21 docno = 427 --- origWords = ['uv', 'damage', 'eye']\n",
      "c1 = 21 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 6385 0 0\n",
      "c1 = 22 docno = 412 --- origWords = ['airport', 'security']\n",
      "c1 = 22 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 6497 0 0\n",
      "c1 = 23 docno = 413 --- origWords = ['steel', 'production']\n",
      "c1 = 23 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 6983 0 0\n",
      "c1 = 24 docno = 371 --- origWords = ['health', 'insurance', 'holistic']\n",
      "c1 = 24 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 7262 0 0\n",
      "c1 = 25 docno = 370 --- origWords = ['food', 'drug', 'law']\n",
      "c1 = 25 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 8052 0 0\n",
      "c1 = 26 docno = 373 --- origWords = ['encryption', 'equipment', 'export']\n",
      "c1 = 26 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 8092 0 0\n",
      "c1 = 27 docno = 361 --- origWords = ['clothe', 'sweatshop']\n",
      "c1 = 27 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 8364 0 0\n",
      "c1 = 28 docno = 418 --- origWords = ['quilt', 'income']\n",
      "c1 = 28 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 8480 0 0\n",
      "c1 = 29 docno = 419 --- origWords = ['recycle', 'automobile', 'tire']\n",
      "c1 = 29 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 8740 0 0\n",
      "c1 = 30 docno = 377 --- origWords = ['cigar', 'smoke']\n",
      "c1 = 30 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 9570 0 0\n",
      "c1 = 31 docno = 376 --- origWords = ['world', 'court']\n",
      "c1 = 31 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 10240 0 0\n",
      "c1 = 32 docno = 393 --- origWords = ['mercy', 'kill']\n",
      "c1 = 32 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 10711 0 0\n",
      "c1 = 33 docno = 392 --- origWords = ['robotics']\n",
      "c1 = 33 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 10721 0 0\n",
      "c1 = 34 docno = 391 --- origWords = ['r', 'drug', 'price']\n",
      "c1 = 34 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 11318 0 0\n",
      "c1 = 35 docno = 390 --- origWords = ['orphan', 'drug']\n",
      "c1 = 35 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 11519 0 0\n",
      "c1 = 36 docno = 397 --- origWords = ['automobile', 'recall']\n",
      "c1 = 36 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 11676 0 0\n",
      "c1 = 37 docno = 396 --- origWords = ['sick', 'build', 'syndrome']\n",
      "c1 = 37 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 12047 0 0\n",
      "c1 = 38 docno = 395 --- origWords = ['tourism']\n",
      "c1 = 38 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 12061 0 0\n",
      "c1 = 39 docno = 394 --- origWords = ['home', 'school']\n",
      "c1 = 39 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 13122 0 0\n",
      "c1 = 40 docno = 399 --- origWords = ['oceanographic', 'vessel']\n",
      "c1 = 40 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 13334 0 0\n",
      "c1 = 41 docno = 398 --- origWords = ['dismantle', \"europe's\", 'arsenal']\n",
      "c1 = 41 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 13385 0 0\n",
      "c1 = 42 docno = 414 --- origWords = ['cuba', 'sugar', 'export']\n",
      "c1 = 42 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 13875 0 0\n",
      "c1 = 43 docno = 415 --- origWords = ['drug', 'golden', 'triangle']\n",
      "c1 = 43 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 14198 0 0\n",
      "c1 = 44 docno = 429 --- origWords = ['legionnaire', 'disease']\n",
      "c1 = 44 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 14540 0 0\n",
      "c1 = 45 docno = 428 --- origWords = ['decline', 'birth', 'rate']\n",
      "c1 = 45 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 14891 0 0\n",
      "c1 = 46 docno = 368 --- origWords = ['in', 'vitro', 'fertilization']\n",
      "c1 = 46 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 15022 0 0\n",
      "c1 = 47 docno = 369 --- origWords = ['anorexia', 'nervosa', 'bulimia']\n",
      "c1 = 47 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 15042 0 0\n",
      "c1 = 48 docno = 421 --- origWords = ['industrial', 'waste', 'disposal']\n",
      "c1 = 48 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 15453 0 0\n",
      "c1 = 49 docno = 367 --- origWords = ['piracy']\n",
      "c1 = 49 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 15482 0 0\n",
      "c1 = 50 docno = 423 --- origWords = ['milosevic', 'mirjana', 'markovic']\n",
      "c1 = 50 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 15482 0 0\n",
      "c1 = 51 docno = 365 --- origWords = ['el', 'nino']\n",
      "c1 = 51 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 15504 0 0\n",
      "c1 = 52 docno = 362 --- origWords = ['human', 'smuggle']\n",
      "c1 = 52 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 16979 0 0\n",
      "c1 = 53 docno = 424 --- origWords = ['suicide']\n",
      "c1 = 53 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 17054 0 0\n",
      "c1 = 54 docno = 360 --- origWords = ['drug', 'legalization', 'benefit']\n",
      "c1 = 54 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 17309 0 0\n",
      "c1 = 55 docno = 426 --- origWords = ['law', 'enforcement', 'dog']\n",
      "c1 = 55 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 18800 0 0\n",
      "c1 = 56 docno = 449 --- origWords = ['antibiotic', 'ineffectiveness']\n",
      "c1 = 56 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 18835 0 0\n",
      "c1 = 57 docno = 448 --- origWords = ['ship', 'loss']\n",
      "c1 = 57 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 19021 0 0\n",
      "c1 = 58 docno = 443 --- origWords = ['u_s', 'investment', 'africa']\n",
      "c1 = 58 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 19111 0 0\n",
      "c1 = 59 docno = 442 --- origWords = ['heroic', 'act']\n",
      "c1 = 59 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 19409 0 0\n",
      "c1 = 60 docno = 441 --- origWords = ['lyme', 'disease']\n",
      "c1 = 60 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 19740 0 0\n",
      "c1 = 61 docno = 440 --- origWords = ['child', 'labor']\n",
      "c1 = 61 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 21056 0 0\n",
      "c1 = 62 docno = 447 --- origWords = ['stirling', 'engine']\n",
      "c1 = 62 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 21226 0 0\n",
      "c1 = 63 docno = 446 --- origWords = ['tourist', 'violence']\n",
      "c1 = 63 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 21372 0 0\n",
      "c1 = 64 docno = 445 --- origWords = ['woman', 'clergy']\n",
      "c1 = 64 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 21980 0 0\n",
      "c1 = 65 docno = 444 --- origWords = ['supercritical', 'fluid']\n",
      "c1 = 65 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 22023 0 0\n",
      "c1 = 66 docno = 380 --- origWords = ['obesity', 'medical', 'treatment']\n",
      "c1 = 66 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 22093 0 0\n",
      "c1 = 67 docno = 381 --- origWords = ['alternative', 'medicine']\n",
      "c1 = 67 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 22295 0 0\n",
      "c1 = 68 docno = 382 --- origWords = ['hydrogen', 'fuel', 'automobile']\n",
      "c1 = 68 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 22535 0 0\n",
      "c1 = 69 docno = 383 --- origWords = ['mental', 'illness', 'drug']\n",
      "c1 = 69 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 22765 0 0\n",
      "c1 = 70 docno = 384 --- origWords = ['space', 'station', 'moon']\n",
      "c1 = 70 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 23854 0 0\n",
      "c1 = 71 docno = 385 --- origWords = ['hybrid', 'fuel', 'car']\n",
      "c1 = 71 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 24761 0 0\n",
      "c1 = 72 docno = 386 --- origWords = ['teach', 'disable', 'child']\n",
      "c1 = 72 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 26121 0 0\n",
      "c1 = 73 docno = 387 --- origWords = ['radioactive', 'waste']\n",
      "c1 = 73 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 26526 0 0\n",
      "c1 = 74 docno = 388 --- origWords = ['organic', 'soil', 'enhancement']\n",
      "c1 = 74 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 26635 0 0\n",
      "c1 = 75 docno = 389 --- origWords = ['illegal', 'technology', 'transfer']\n",
      "c1 = 75 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 26765 0 0\n",
      "c1 = 76 docno = 411 --- origWords = ['salvage', 'shipwreck', 'treasure']\n",
      "c1 = 76 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 26856 0 0\n",
      "c1 = 77 docno = 372 --- origWords = ['native', 'american', 'casino']\n",
      "c1 = 77 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 27130 0 0\n",
      "c1 = 78 docno = 375 --- origWords = ['hydrogen', 'energy']\n",
      "c1 = 78 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 27634 0 0\n",
      "c1 = 79 docno = 374 --- origWords = ['nobel', 'prize', 'winner']\n",
      "c1 = 79 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 27754 0 0\n",
      "c1 = 80 docno = 438 --- origWords = ['tourism', 'increase']\n",
      "c1 = 80 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 27897 0 0\n",
      "c1 = 81 docno = 439 --- origWords = ['invention', 'scientific', 'discovery']\n",
      "c1 = 81 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 28053 0 0\n",
      "c1 = 82 docno = 436 --- origWords = ['railway', 'accident']\n",
      "c1 = 82 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 28228 0 0\n",
      "c1 = 83 docno = 437 --- origWords = ['deregulation', 'gas', 'electric']\n",
      "c1 = 83 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 28566 0 0\n",
      "c1 = 84 docno = 434 --- origWords = ['estonia', 'economy']\n",
      "c1 = 84 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 28631 0 0\n",
      "c1 = 85 docno = 435 --- origWords = ['curb', 'population', 'growth']\n",
      "c1 = 85 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 28994 0 0\n",
      "c1 = 86 docno = 432 --- origWords = ['profile', 'motorist', 'police']\n",
      "c1 = 86 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 29187 0 0\n",
      "c1 = 87 docno = 433 --- origWords = ['greek', 'philosophy', 'stoicism']\n",
      "c1 = 87 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 29319 0 0\n",
      "c1 = 88 docno = 430 --- origWords = ['killer', 'bee', 'attack']\n",
      "c1 = 88 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 29876 0 0\n",
      "c1 = 89 docno = 431 --- origWords = ['robotic', 'technology']\n",
      "c1 = 89 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 29913 0 0\n",
      "c1 = 90 docno = 450 --- origWords = ['king', 'hussein', 'peace']\n",
      "c1 = 90 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 30477 0 0\n",
      "c1 = 91 docno = 357 --- origWords = ['territorial', 'water', 'dispute']\n",
      "c1 = 91 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 31450 0 0\n",
      "c1 = 92 docno = 356 --- origWords = ['postmenopausal', 'estrogen', 'britain']\n",
      "c1 = 92 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 31472 0 0\n",
      "c1 = 93 docno = 355 --- origWords = ['ocean', 'remote', 'sense']\n",
      "c1 = 93 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 31892 0 0\n",
      "c1 = 94 docno = 354 --- origWords = ['journalist', 'risk']\n",
      "c1 = 94 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 31976 0 0\n",
      "c1 = 95 docno = 353 --- origWords = ['antarctica', 'exploration']\n",
      "c1 = 95 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 32052 0 0\n",
      "c1 = 96 docno = 352 --- origWords = ['british', 'chunnel', 'impact']\n",
      "c1 = 96 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 32116 0 0\n",
      "c1 = 97 docno = 351 --- origWords = ['falkland', 'petroleum', 'exploration']\n",
      "c1 = 97 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 32182 0 0\n",
      "c1 = 98 docno = 359 --- origWords = ['mutual', 'fund', 'predictor']\n",
      "c1 = 98 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 32218 0 0\n",
      "c1 = 99 docno = 358 --- origWords = ['blood', 'alcohol', 'fatality']\n",
      "c1 = 99 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 32701 0 0\n"
     ]
    }
   ],
   "source": [
    "#training\n",
    "intCoeffs0 = [ 0.7 ]\n",
    "intCoeffs1 = [ 0.3 ]\n",
    "#intCoeffs2 = [ 0.18 ]\n",
    "intCoeffs2 = [ 0.5 ]\n",
    "#intCoeffs1 = np.arange(0.1, 1, 0.1)\n",
    "expansionCounts1 = [ 85 ]\n",
    "#expansionCounts2 = [ 145 ]\n",
    "expansionCounts2 = [ 0 ]\n",
    "#expansionCounts2 = range(15, 220, 5)\n",
    "dirCoeffs = [ 1600 ]\n",
    "#dirCoeffs = range(200, 4000, 200)\n",
    "N1s = [0]\n",
    "rWords = dict()\n",
    "featureWeights_1 = {'maxTopColCor': 0.1, 'expTDocScore': 0.1, 'maxColPCor': 0.1, 'avgCDocScore': 0.1, 'topTermFrac': 0.1, 'avgColCor': 0.1, 'avgTopColCor': 0.1, 'numCanDocs': 0.1, 'maxTopColPCor': 0.1, 'maxCDocScore': 0.1, 'avgColPCor': 0.1, 'conIdf': 0.1, 'avgTopColPCor': 0.1, 'maxColCor': 0.1}\n",
    "featureWeights_l=[featureWeights_1]\n",
    "origWordsAll, count_history, indriRunQuery_hist_, dumpindex_hist_ = precisionCompute_train(count_history, intCoeffs0, intCoeffs1, intCoeffs2, expansionCounts1, expansionCounts2, dirCoeffs, N1s, featureWeights_l, indriRunQuery_hist_, dumpindex_hist_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def precisionCompute(origWordsAll, count_history, intCoeffs0, intCoeffs1, intCoeffs2, expansionCounts1, expansionCounts2, dirCoeffs, N1s, featureWeights_, indriRunQuery_hist_, dumpindex_hist_):\n",
    "    #mapPrecs = dict()\n",
    "    for featureWeights in featureWeights_:\n",
    "        for N1 in N1s:\n",
    "            for intCoeff0 in intCoeffs0:\n",
    "                for intCoeff1 in intCoeffs1:\n",
    "                    for intCoeff2 in intCoeffs2:\n",
    "                        for expansionCount1 in expansionCounts1:\n",
    "                            for expansionCount2 in expansionCounts2:\n",
    "                                for dirCoeff in dirCoeffs:\n",
    "                                    \n",
    "                                    \n",
    "                                    #res = getAllRelWords_p(count_history, rWords, origWordsAll, expansionCount1, N1, expansionCount2, intCoeff0, intCoeff1, intCoeff2, dirCoeff, featureWeights)\n",
    "                                    \n",
    "                                    count_history, relText_stringAll, relWords_sel = getAllRelWords(count_history, rWords, origWordsAll, expansionCount1, N1, expansionCount2, intCoeff0, intCoeff1, intCoeff2, dirCoeff, featureWeights, indriRunQuery_hist_, dumpindex_hist_)\n",
    "                                    \n",
    "                                    #print(\"len(relText_stringAll[1]) =\", len(relText_stringAll[1]))\n",
    "                                    ##########\n",
    "                                    #cfgOutFileName = os.path.join(\"tmp\",\"sbsb\",str(randNum)+\".cfg\")\n",
    "                                    #runsFileName = os.path.join(\"tmp\",\"sbsb\",str(randNum)+\".run\")\n",
    "                                    #evalsFileName = os.path.join(\"tmp\",\"sbsb\",str(randNum)+\".eval\")\n",
    "                                    ##########\n",
    "                                    genQueries(cfgOutFileName, origWordsAll, relText_stringAll, intCoeff0, intCoeff1, intCoeff2, dirCoeff)\n",
    "                                    subprocess.Popen(\"IndriRunQuery \" + cfgOutFileName + \" > \" + runsFileName, shell=True, stdout=subprocess.PIPE).stdout.read()\n",
    "                                    cmd = \"trec_eval -q \" + colQrelsFileName + \" \" + runsFileName + \" > \" + evalsFileName\n",
    "                                    subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE).stdout.read()\n",
    "                                    cmd = \"cat \" + evalsFileName + \" | grep map | grep all | grep -v gm | awk '{print $3}' \"\n",
    "                                    #print (\"cmd = \", cmd)\n",
    "                                    mapPrec = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE).stdout.read()\n",
    "                                    print(\"featureWeights =\", featureWeights)\n",
    "                                    print(\"intCoeff0, intCoeff1, intCoeff2, expansionCount1, expansionCount2, dirCoeff, map precision, N1 =\", intCoeff0, intCoeff1, intCoeff2, expansionCount1, expansionCount2, dirCoeff, N1, mapPrec)\n",
    "                                    #mapPrecs[intCoeff] = mapPrec\n",
    "    return relWords_sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "featureWeights = {'maxTopColCor': 400, 'maxColPCor': 0.1, 'conIdf': 50, 'expTDocScore': 190, 'topTermFrac': 0.1, 'avgColCor': 500, 'avgTopColCor': 0.1, 'numCanDocs': 0.1, 'maxTopColPCor': 0.1, 'avgCDocScore': -40, 'avgColPCor': 0.1, 'maxCDocScore': 0.1, 'avgTopColPCor': 0.1, 'maxColCor': 0.1}\n",
      "intCoeff0, intCoeff1, intCoeff2, expansionCount1, expansionCount2, dirCoeff, map precision, N1 = 0.7 0.5 0.5 10 0 1600 3 0.2128\n",
      "\n"
     ]
    }
   ],
   "source": [
    "intCoeffs0 = [ 0.7 ]\n",
    "intCoeffs1 = [ 0.5 ]\n",
    "#intCoeffs2 = [ 0.18 ]\n",
    "intCoeffs2 = [ 0.5 ]\n",
    "#intCoeffs1 = np.arange(0.1, 1, 0.1)\n",
    "expansionCounts1 = [ 10 ]\n",
    "#expansionCounts2 = [ 145 ]\n",
    "expansionCounts2 = [ 0 ]\n",
    "#expansionCounts2 = range(15, 220, 5)\n",
    "dirCoeffs = [ 1600 ]\n",
    "#dirCoeffs = range(200, 4000, 200)\n",
    "N1s = [3]\n",
    "#N1s = range(1, 30, 1)\n",
    "#mapPrecs = \n",
    "rWords = dict()\n",
    "featureWeights_l=[]\n",
    "featureWeights_1 = {'maxTopColCor': 400, 'expTDocScore': 190, 'maxColPCor': 0.1, 'avgCDocScore': -40, 'topTermFrac': 0.1, 'avgColCor': 500, 'avgTopColCor': 0.1, 'numCanDocs': 0.1, 'maxTopColPCor': 0.1, 'maxCDocScore': 0.1, 'avgColPCor': 0.1, 'conIdf': 0.1, 'avgTopColPCor': 0.1, 'maxColCor': 0.1}\n",
    "for conIdf in [50] :\n",
    "#for avgCDocScore in np.arrange():\n",
    "    featureWeights_1[\"conIdf\"] = conIdf\n",
    "    featureWeights_l += [dict(featureWeights_1)]\n",
    "relWords_sel = precisionCompute(origWordsAll, count_history, intCoeffs0, intCoeffs1, intCoeffs2, expansionCounts1, expansionCounts2, dirCoeffs, N1s, featureWeights_l, indriRunQuery_hist_, dumpindex_hist_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'407': ['hunt', 'animal', 'forest'], '406': ['treatable', 'hiv', 'ill'], '405': ['large', 'celebration', 'universe'], '404': ['mean', 'silence', 'be'], '403': ['fracture', 'menopause', 'bone'], '402': ['behavior', 'behaviorism', 'biology'], '401': ['europe', 'country', 'place'], '400': ['film', 'natural', 'nature'], '409': ['album', 'film', 'god'], '408': ['tropic', 'weather', 'hurricane'], '421': ['dispose', 'use', 'act'], '367': ['law', 'intellectual property', 'pirate'], '423': [], '365': ['arm', 'stream', 'm'], '362': ['crime', 'charge', 'society'], '424': ['album', 'disease', 'death'], '414': ['cane', 'caribbean', 'commodity'], '415': ['illicit', 'substance', 'abuse'], '416': ['projection', 'year', 'one'], '417': ['creative', 'creativeness', 'advertise'], '410': ['accord', 'luxembourg', 'agree'], '411': ['ship', 'treasurer', 'salvager'], '412': ['guarantee', 'air', 'police'], '413': ['factory', 'consumption', 'item'], '371': ['your', 'life', 'benefit'], '370': ['person', 'organisation', 'consume'], '373': ['equip', 'encrypt', 'exporter'], '372': ['film', 'indigenous', 'person'], '375': ['element', 'light', 'gas'], '374': ['win', 'film', 'nobel prize'], '377': ['smoker', 'cigarette', 'tobacco'], '376': ['big', 'place', 'area'], '393': ['punishment', 'punish', 'harm'], '392': ['application', 'robot', 'manufacture'], '391': ['medicine', 'compare', 'rise'], '390': ['useful', 'use', 'take'], '397': ['freeway', 'car', 'wheel'], '396': ['up', 'single', 'a'], '395': ['travel', 'sightsee', 'tourist'], '394': ['jesus', 'wall', 'location'], '399': ['ocean', 'ship', 'sea'], '398': ['remove', 'weapon', 'arm'], '379': [], '378': ['eurosceptic', 'obstacle', 'proceed'], '429': ['bacteria', 'it', 'be'], '428': ['change', 'first', 'be'], '368': ['fertile', 'an', 'inning'], '369': ['anorexia nervosa', 'eat disorder', 'eat'], '366': ['album', 'consume', 'commercialize'], '420': ['dioxide', 'chemical', 'poisonous'], '364': ['disease', 'death', 'viral'], '422': ['artwork', 'film', 'do'], '425': ['fake', 'banknote', 'copy'], '363': ['road', 'train', 'place'], '427': ['two', 'ultraviolet', 'protect'], '426': ['enforce', 'lawyer', 'crime'], '449': ['ineffective', 'medicine', 'bacteria'], '448': ['organisation', 'profit', 'failure'], '443': ['continent', 'tunisia', 'invest'], '442': ['behave', 'behaviour', 'unit'], '441': ['lyme disease', 'infection', 'cause'], '440': ['birth', 'work', 'give birth'], '447': ['place', 'generic', 'build'], '446': ['violent', 'visitor', 'tourism'], '445': ['sex', 'priest', 'rabbi'], '444': ['flow', 'container', 'gas'], '380': ['illness', 'plan', 'obese'], '381': ['album', 'single', 'organisation'], '382': ['water', 'gas', 'engine'], '383': ['hospital', 'brain', 'fun'], '384': ['astronaut', 'planet', 'earth'], '385': ['gas', 'engine', 'gasoline'], '386': ['learn', 'school', 'fun'], '387': ['single', 'scrap', 'pollution'], '388': ['fertilizer', 'album', 'product'], '389': ['change', 'over', 'organisation'], '418': ['homemade', 'handmade', 'warm'], '419': ['car', 'truck', 'wheel'], '438': ['double', 'progressive', 'momentum'], '439': ['science', 'einstein', 'new'], '436': ['passenger', 'accidence', 'railroad'], '437': ['organisation', 'natural', 'phase'], '434': ['country', 'main', 'cheap'], '435': ['grow', 'size', 'city'], '432': ['person', 'book', 'album'], '433': ['hellen', 'hellenism', 'hellene'], '430': ['be', 'kill', 'honey'], '431': ['technique', 'technical', 'application'], '450': ['organisation', 'leader', 'state'], '360': ['good', 'concert', 'punishment'], '361': ['factory', 'garment', 'shop'], '357': ['territory', 'two', 'necessary'], '356': ['menopause', 'post', 'monarchy'], '355': ['album', 'river', 'band'], '354': ['person', 'film', 'event'], '353': ['antarctic', 'continent', 'expedition'], '352': ['channel tunnel', 'tunnel', 'channel'], '351': ['exploratory', 'falkland island', 'falkland islander'], '359': ['asset', 'reciprocal', 'pay'], '358': ['drink', 'person', 'intoxicate']}\n"
     ]
    }
   ],
   "source": [
    "print(relWords_sel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'407': ['poach', 'wildlife', 'preserve'], '406': [\"parkinson's\", 'disease'], '405': ['cosmic', 'event'], '404': ['ireland', 'peace', 'talk'], '403': ['osteoporosis'], '402': ['behavioral', 'genetics'], '401': ['foreign', 'minority', 'germany'], '400': ['amazon', 'rain', 'forest'], '409': ['legal', 'pan', 'be', '103'], '408': ['tropical', 'storm'], '366': ['commercial', 'cyanide', 'use'], '420': ['carbon', 'monoxide', 'poison'], '364': ['rabies'], '422': ['art', 'steal', 'forge'], '425': ['counterfeit', 'money'], '363': ['transportation', 'tunnel', 'disaster'], '379': ['mainstreaming'], '378': ['euro', 'opposition'], '416': ['three', 'gorge', 'project'], '417': ['creativity'], '410': ['schengen', 'agreement'], '427': ['uv', 'damage', 'eye'], '412': ['airport', 'security'], '413': ['steel', 'production'], '371': ['health', 'insurance', 'holistic'], '370': ['food', 'drug', 'law'], '373': ['encryption', 'equipment', 'export'], '361': ['clothe', 'sweatshop'], '418': ['quilt', 'income'], '419': ['recycle', 'automobile', 'tire'], '377': ['cigar', 'smoke'], '376': ['world', 'court'], '393': ['mercy', 'kill'], '392': ['robotics'], '391': ['r', 'drug', 'price'], '390': ['orphan', 'drug'], '397': ['automobile', 'recall'], '396': ['sick', 'build', 'syndrome'], '395': ['tourism'], '394': ['home', 'school'], '399': ['oceanographic', 'vessel'], '398': ['dismantle', \"europe's\", 'arsenal'], '414': ['cuba', 'sugar', 'export'], '415': ['drug', 'golden', 'triangle'], '429': ['legionnaire', 'disease'], '428': ['decline', 'birth', 'rate'], '368': ['in', 'vitro', 'fertilization'], '369': ['anorexia', 'nervosa', 'bulimia'], '421': ['industrial', 'waste', 'disposal'], '367': ['piracy'], '423': ['milosevic', 'mirjana', 'markovic'], '365': ['el', 'nino'], '362': ['human', 'smuggle'], '424': ['suicide'], '360': ['drug', 'legalization', 'benefit'], '426': ['law', 'enforcement', 'dog'], '449': ['antibiotic', 'ineffectiveness'], '448': ['ship', 'loss'], '443': ['u_s', 'investment', 'africa'], '442': ['heroic', 'act'], '441': ['lyme', 'disease'], '440': ['child', 'labor'], '447': ['stirling', 'engine'], '446': ['tourist', 'violence'], '445': ['woman', 'clergy'], '444': ['supercritical', 'fluid'], '380': ['obesity', 'medical', 'treatment'], '381': ['alternative', 'medicine'], '382': ['hydrogen', 'fuel', 'automobile'], '383': ['mental', 'illness', 'drug'], '384': ['space', 'station', 'moon'], '385': ['hybrid', 'fuel', 'car'], '386': ['teach', 'disable', 'child'], '387': ['radioactive', 'waste'], '388': ['organic', 'soil', 'enhancement'], '389': ['illegal', 'technology', 'transfer'], '411': ['salvage', 'shipwreck', 'treasure'], '372': ['native', 'american', 'casino'], '375': ['hydrogen', 'energy'], '374': ['nobel', 'prize', 'winner'], '438': ['tourism', 'increase'], '439': ['invention', 'scientific', 'discovery'], '436': ['railway', 'accident'], '437': ['deregulation', 'gas', 'electric'], '434': ['estonia', 'economy'], '435': ['curb', 'population', 'growth'], '432': ['profile', 'motorist', 'police'], '433': ['greek', 'philosophy', 'stoicism'], '430': ['killer', 'bee', 'attack'], '431': ['robotic', 'technology'], '450': ['king', 'hussein', 'peace'], '357': ['territorial', 'water', 'dispute'], '356': ['postmenopausal', 'estrogen', 'britain'], '355': ['ocean', 'remote', 'sense'], '354': ['journalist', 'risk'], '353': ['antarctica', 'exploration'], '352': ['british', 'chunnel', 'impact'], '351': ['falkland', 'petroleum', 'exploration'], '359': ['mutual', 'fund', 'predictor'], '358': ['blood', 'alcohol', 'fatality']}\n"
     ]
    }
   ],
   "source": [
    "print(origWordsAll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def precisionCompute_train2(relWords_sel, count_history, intCoeffs0, intCoeffs1, intCoeffs2, expansionCounts1, expansionCounts2, dirCoeffs, N1s, featureWeights_, indriRunQuery_hist_, dumpindex_hist_):\n",
    "\n",
    "    indriRunQuery_hist_, dumpindex_hist_ = getAllRelWords_train(count_history, rWords, relWords_sel, expansionCounts1[0], N1s[0], expansionCounts2[0], intCoeffs0[0], intCoeffs1[0], intCoeffs2[0], dirCoeffs[0], featureWeights_[0], indriRunQuery_hist_, dumpindex_hist_)\n",
    "                                    \n",
    "    return indriRunQuery_hist_, dumpindex_hist_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c1 = 0 docno = 407 --- origWords = ['hunt', 'animal', 'forest']\n",
      "c1 = 0 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 33582 0 0\n",
      "c1 = 1 docno = 406 --- origWords = ['treatable', 'hiv', 'ill']\n",
      "c1 = 1 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 33745 0 0\n",
      "c1 = 2 docno = 405 --- origWords = ['large', 'celebration', 'universe']\n",
      "c1 = 2 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 33998 0 0\n",
      "c1 = 3 docno = 404 --- origWords = ['mean', 'silence', 'be']\n",
      "c1 = 3 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 34682 0 0\n",
      "c1 = 4 docno = 403 --- origWords = ['fracture', 'menopause', 'bone']\n",
      "c1 = 4 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 35364 0 0\n",
      "c1 = 5 docno = 402 --- origWords = ['behavior', 'behaviorism', 'biology']\n",
      "c1 = 5 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 35456 0 0\n",
      "c1 = 6 docno = 401 --- origWords = ['europe', 'country', 'place']\n",
      "c1 = 6 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 36177 0 0\n",
      "c1 = 7 docno = 400 --- origWords = ['film', 'natural', 'nature']\n",
      "c1 = 7 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 36585 0 0\n",
      "c1 = 8 docno = 409 --- origWords = ['album', 'film', 'god']\n",
      "c1 = 8 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 37095 0 0\n",
      "c1 = 9 docno = 408 --- origWords = ['tropic', 'weather', 'hurricane']\n",
      "c1 = 9 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 37764 0 0\n",
      "c1 = 10 docno = 421 --- origWords = ['dispose', 'use', 'act']\n",
      "c1 = 10 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 38129 0 0\n",
      "c1 = 11 docno = 367 --- origWords = ['law', 'intellectual property', 'pirate']\n",
      "c1 = 11 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 38475 0 0\n",
      "c1 = 12 docno = 423 --- origWords = []\n",
      "c1 = 12 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 38475 0 0\n",
      "c1 = 13 docno = 365 --- origWords = ['arm', 'stream', 'm']\n",
      "c1 = 13 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 40443 0 0\n",
      "c1 = 14 docno = 362 --- origWords = ['crime', 'charge', 'society']\n",
      "c1 = 14 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 40965 0 0\n",
      "c1 = 15 docno = 424 --- origWords = ['album', 'disease', 'death']\n",
      "c1 = 15 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 41517 0 0\n",
      "c1 = 16 docno = 414 --- origWords = ['cane', 'caribbean', 'commodity']\n",
      "c1 = 16 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 41623 0 0\n",
      "c1 = 17 docno = 415 --- origWords = ['illicit', 'substance', 'abuse']\n",
      "c1 = 17 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 41716 0 0\n",
      "c1 = 18 docno = 416 --- origWords = ['projection', 'year', 'one']\n",
      "c1 = 18 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 42476 0 0\n",
      "c1 = 19 docno = 417 --- origWords = ['creative', 'creativeness', 'advertise']\n",
      "c1 = 19 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 42545 0 0\n",
      "c1 = 20 docno = 410 --- origWords = ['accord', 'luxembourg', 'agree']\n",
      "c1 = 20 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 42799 0 0\n",
      "c1 = 21 docno = 411 --- origWords = ['ship', 'treasurer', 'salvager']\n",
      "c1 = 21 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 43067 0 0\n",
      "c1 = 22 docno = 412 --- origWords = ['guarantee', 'air', 'police']\n",
      "c1 = 22 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 43546 0 0\n",
      "c1 = 23 docno = 413 --- origWords = ['factory', 'consumption', 'item']\n",
      "c1 = 23 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 43835 0 0\n",
      "c1 = 24 docno = 371 --- origWords = ['your', 'life', 'benefit']\n",
      "c1 = 24 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 44218 0 0\n",
      "c1 = 25 docno = 370 --- origWords = ['person', 'organisation', 'consume']\n",
      "c1 = 25 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 54985 0 0\n",
      "c1 = 26 docno = 373 --- origWords = ['equip', 'encrypt', 'exporter']\n",
      "c1 = 26 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 55026 0 0\n",
      "c1 = 27 docno = 372 --- origWords = ['film', 'indigenous', 'person']\n",
      "c1 = 27 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 65832 0 0\n",
      "c1 = 28 docno = 375 --- origWords = ['element', 'light', 'gas']\n",
      "c1 = 28 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 66582 0 0\n",
      "c1 = 29 docno = 374 --- origWords = ['win', 'film', 'nobel prize']\n",
      "c1 = 29 - len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) = 66765 0 0\n",
      "c1 = 30 docno = 377 --- origWords = ['smoker', 'cigarette', 'tobacco']\n",
      "c1 ="
     ]
    }
   ],
   "source": [
    "intCoeffs0 = [ 0.7 ]\n",
    "intCoeffs1 = [ 0.5 ]\n",
    "#intCoeffs2 = [ 0.18 ]\n",
    "intCoeffs2 = [ 0.5 ]\n",
    "#intCoeffs1 = np.arange(0.1, 1, 0.1)\n",
    "expansionCounts1 = [ 10 ]\n",
    "#expansionCounts2 = [ 145 ]\n",
    "expansionCounts2 = [ 0 ]\n",
    "#expansionCounts2 = range(15, 220, 5)\n",
    "dirCoeffs = [ 1600 ]\n",
    "#dirCoeffs = range(200, 4000, 200)\n",
    "N1s = [3]\n",
    "rWords = dict()\n",
    "featureWeights_1 = {'maxTopColCor': 0.1, 'expTDocScore': 0.1, 'maxColPCor': 0.1, 'avgCDocScore': 0.1, 'topTermFrac': 0.1, 'avgColCor': 0.1, 'avgTopColCor': 0.1, 'numCanDocs': 0.1, 'maxTopColPCor': 0.1, 'maxCDocScore': 0.1, 'avgColPCor': 0.1, 'conIdf': 0.1, 'avgTopColPCor': 0.1, 'maxColCor': 0.1}\n",
    "featureWeights_l=[featureWeights_1]\n",
    "indriRunQuery_hist_, dumpindex_hist_ = precisionCompute_train2(relWords_sel, count_history, intCoeffs0, intCoeffs1, intCoeffs2, expansionCounts1, expansionCounts2, dirCoeffs, N1s, featureWeights_l, indriRunQuery_hist_, dumpindex_hist_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def precisionCompute2(relWords_sel, count_history, intCoeffs0, intCoeffs1, intCoeffs2, expansionCounts1, expansionCounts2, dirCoeffs, N1s, featureWeights_, indriRunQuery_hist_, dumpindex_hist_):\n",
    "    #mapPrecs = dict()\n",
    "    for featureWeights in featureWeights_:\n",
    "        for N1 in N1s:\n",
    "            for intCoeff0 in intCoeffs0:\n",
    "                for intCoeff1 in intCoeffs1:\n",
    "                    for intCoeff2 in intCoeffs2:\n",
    "                        for expansionCount1 in expansionCounts1:\n",
    "                            for expansionCount2 in expansionCounts2:\n",
    "                                for dirCoeff in dirCoeffs:\n",
    "                                    \n",
    "                                    \n",
    "                                    #res = getAllRelWords_p(count_history, rWords, origWordsAll, expansionCount1, N1, expansionCount2, intCoeff0, intCoeff1, intCoeff2, dirCoeff, featureWeights)\n",
    "                                    \n",
    "                                    count_history, relText_stringAll, relWords_sel2 = getAllRelWords(count_history, rWords, relWords_sel, expansionCount1, N1, expansionCount2, intCoeff0, intCoeff1, intCoeff2, dirCoeff, featureWeights, indriRunQuery_hist_, dumpindex_hist_)\n",
    "                                    \n",
    "                                    #print(\"len(relText_stringAll[1]) =\", len(relText_stringAll[1]))\n",
    "                                    ##########\n",
    "                                    #cfgOutFileName = os.path.join(\"tmp\",\"sbsb\",str(randNum)+\".cfg\")\n",
    "                                    #runsFileName = os.path.join(\"tmp\",\"sbsb\",str(randNum)+\".run\")\n",
    "                                    #evalsFileName = os.path.join(\"tmp\",\"sbsb\",str(randNum)+\".eval\")\n",
    "                                    ##########\n",
    "                                    genQueries(cfgOutFileName, origWordsAll, relWords_sel2, relText_stringAll, intCoeff0, intCoeff1, intCoeff2, dirCoeff)\n",
    "                                    subprocess.Popen(\"IndriRunQuery \" + cfgOutFileName + \" > \" + runsFileName, shell=True, stdout=subprocess.PIPE).stdout.read()\n",
    "                                    cmd = \"trec_eval -q \" + colQrelsFileName + \" \" + runsFileName + \" > \" + evalsFileName\n",
    "                                    subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE).stdout.read()\n",
    "                                    cmd = \"cat \" + evalsFileName + \" | grep map | grep all | grep -v gm | awk '{print $3}' \"\n",
    "                                    #print (\"cmd = \", cmd)\n",
    "                                    mapPrec = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE).stdout.read()\n",
    "                                    print(\"featureWeights =\", featureWeights)\n",
    "                                    print(\"intCoeff0, intCoeff1, intCoeff2, expansionCount1, expansionCount2, dirCoeff, map precision, N1 =\", intCoeff0, intCoeff1, intCoeff2, expansionCount1, expansionCount2, dirCoeff, N1, mapPrec)\n",
    "                                    #mapPrecs[intCoeff] = mapPrec\n",
    "    return relWords_sel"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
