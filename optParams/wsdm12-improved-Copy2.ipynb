{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function \n",
    "from bs4 import BeautifulSoup\n",
    "from BeautifulSoup import SoupStrainer as sopstrain\n",
    "import subprocess\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import numpy as np\n",
    "import string\n",
    "import operator\n",
    "import csv\n",
    "import urllib2\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import StringIO\n",
    "import random\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cfgInFileName = /home/fj9124/projects/ir/seq_kb_ir/configs/gov/queryGov\n",
      "cfgOutFileName = /home/fj9124/projects/ir/seq_kb_ir/configs/gov/wsdmImpr/cnet/indriRunQuery.cfg\n",
      "colIndexDir = /scratch/index/indri_5_7/gov\n",
      "knowledgGraph index Dir = /scratch/index/indri_5_7/gov\n",
      "col Qrels File Name = /home/fj9124/projects/ir/seq_kb_ir/qrels/gov/qrels.csv\n",
      "runsFileName = /home/fj9124/projects/ir/seq_kb_ir/runs/wsdmImpr/gov/cnet/indriRunQuery.runs\n",
      "evalsFileName = /home/fj9124/projects/ir/seq_kb_ir/evals/wsdmImpr/gov/cnet/indriRunQuery.evals\n",
      "dumpindexStatementFilename = /tmp/sbsb/statement_gov_wsdmImpr_cnet\n",
      "occuranceCountFilename = /home/fj9124/projects/ir/seq_kb_ir/occuranceCount/occuranceCount\n",
      "countsResultsFile = /home/fj9124/projects/ir/seq_kb_ir/occuranceCount/results/gov.txt\n",
      "conceptnet5RelAllFilename = /scratch/saeid/data/conceptnet5_simp.csv\n",
      "cfgTmpOutFileName = /home/fj9124/projects/ir/seq_kb_ir/configs/gov/wsdmImpr/cnet/indriRunQuery.cfg.tmp\n",
      "docIdNameMapFileName = /home/fj9124/projects/ir/seq_kb_ir/occuranceCount/results/gov_docIdNameMap.txt\n"
     ]
    }
   ],
   "source": [
    "knowledgGraph = \"cnet\" # conceptnet5AssocMod, gov, conceptnet5AssocMi, conceptnet5AssocHdl\n",
    "#knowledgGraph_ = ''.join([k.capitalize() if i>0 else k for i, k in enumerate(knowledgGraph)])\n",
    "collection = \"gov\"\n",
    "method = \"wsdmImpr\" # hal, mi, assoc, assocMi, assocHal, assoc2\n",
    "simMeasure = \"\" # mi, cnet\n",
    "projectDir = \"/home/fj9124/projects/ir/seq_kb_ir/\" \n",
    "indexDir = \"/scratch/index/indri_5_7/\"\n",
    "colMethodConfigsDir = os.path.join(projectDir, \"configs\", collection, method)\n",
    "cfgInFileName = os.path.join(projectDir, \"configs\", collection, \"query\" + collection.capitalize()) \n",
    "print(\"cfgInFileName =\", cfgInFileName)\n",
    "cfgOutFileName=os.path.join(colMethodConfigsDir, knowledgGraph, \"indriRunQuery.cfg\") \n",
    "print(\"cfgOutFileName =\", cfgOutFileName)\n",
    "colIndexDir = os.path.join(indexDir, collection) \n",
    "print(\"colIndexDir =\", colIndexDir)\n",
    "if knowledgGraph in {\"cnet\"}:\n",
    "    knowledgGraphIndexDir = os.path.join(indexDir, collection)   \n",
    "else:\n",
    "    knowledgGraphIndexDir = os.path.join(indexDir, knowledgGraph)   \n",
    "print(\"knowledgGraph index Dir =\", knowledgGraphIndexDir)\n",
    "graphsDir = os.path.join(projectDir, \"graphs\")\n",
    "#methodGraphsDir = os.path.join(graphsDir, method)\n",
    "methodGraphsFileName = []\n",
    "if method == \"hal\":\n",
    "    methodGraphsFileName = [os.path.join(graphsDir, method, knowledgGraph + \".txt\")]\n",
    "    print(\"methodGraphsFileName =\", methodGraphsFileName)\n",
    "elif method.translate(None, string.digits) in {\"mi\", \"assoc\"}:\n",
    "    methodGraphsFileName = [os.path.join(graphsDir, method.translate(None, string.digits), knowledgGraph, \"graph.txt\")]\n",
    "    print(\"methodGraphsFileName =\", methodGraphsFileName)\n",
    "elif method in {\"assocMi\"}:\n",
    "    if knowledgGraph == \"conceptnet5AssocGov\":\n",
    "        methodGraphsFileName = [os.path.join(graphsDir, \"assoc\", collection, \"conceptnet5AssocMod\" + \".txt\")]\n",
    "        methodGraphsFileName += [os.path.join(graphsDir, \"mi\", collection, \"gov\" + \".txt\")]\n",
    "        print(\"methodGraphsFileName =\", methodGraphsFileName)\n",
    "elif method in {\"assocHal\"}:\n",
    "    if knowledgGraph == \"conceptnet5AssocGov\":\n",
    "        methodGraphsFileName = [os.path.join(graphsDir, \"assoc\", collection, \"conceptnet5AssocMod\" + \".txt\")]\n",
    "        methodGraphsFileName += [os.path.join(graphsDir, \"hal\", \"gov\" + \".txt\")]\n",
    "        print(\"methodGraphsFileName =\", methodGraphsFileName)\n",
    "qrelsDir = os.path.join(projectDir, \"qrels\")\n",
    "colQrelsDir = os.path.join(qrelsDir, collection)\n",
    "colQrelsFileName = os.path.join(colQrelsDir, \"qrels.csv\")\n",
    "print(\"col Qrels File Name =\", colQrelsFileName)\n",
    "runsFileName = os.path.join(projectDir, \"runs\", method, collection, knowledgGraph, \"indriRunQuery.runs\") \n",
    "print(\"runsFileName =\", runsFileName)\n",
    "evalsFileName = os.path.join(projectDir, \"evals\", method, collection, knowledgGraph, \"indriRunQuery.evals\") \n",
    "print(\"evalsFileName =\", evalsFileName)\n",
    "dumpindexStatementFilename = os.path.join(\"/\", \"tmp\", \"sbsb\", \"statement\" + \"_\" + collection + \"_\" + method + \"_\" + knowledgGraph )\n",
    "print(\"dumpindexStatementFilename =\", dumpindexStatementFilename)\n",
    "occuranceCountFilename = os.path.join(projectDir, \"occuranceCount\", \"occuranceCount\")\n",
    "print(\"occuranceCountFilename =\", occuranceCountFilename)\n",
    "countsResultsFile = os.path.join(projectDir,\"occuranceCount\",\"results\",collection+\".txt\")\n",
    "print(\"countsResultsFile =\", countsResultsFile)\n",
    "conceptnet5RelAllFilename =\"/scratch/saeid/data/conceptnet5_simp.csv\"\n",
    "print(\"conceptnet5RelAllFilename =\", conceptnet5RelAllFilename)\n",
    "cfgTmpOutFileName = cfgOutFileName + \".tmp\"\n",
    "print(\"cfgTmpOutFileName =\", cfgTmpOutFileName)\n",
    "docIdNameMapFileName = os.path.join(projectDir, \"occuranceCount\", \"results\", collection + \"_docIdNameMap.txt\")\n",
    "print(\"docIdNameMapFileName =\", docIdNameMapFileName)\n",
    "#dumpindexResFilename = os.path.join(projectDir,\"occuranceCount\",\"results\",collection +\"_dumpindexRes\"+\".txt\")\n",
    "#print(\"dumpindexResFilename =\", dumpindexResFilename)\n",
    "#expansionCount = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# trec_eval -q ../qrels/trec7n8/qrels.csv indriRunQuery.res | grep map | grep -v all | awk '{if ($3 < 0.1) printf $2 \",\"}'\n",
    "if collection == \"gov\":\n",
    "    diffTopics = [4, 6, 7, 8, 9, 10, 12, 18, 19, 21, 22, 24, 26, 27, 30, 31, 32, 33, 36, 37, 40, 45, 46, 47, 48, 50, 52, 53, 54, 57, 58, 59, 63, 65, 66, 67, 70, 73, 78, 79, 80, 82, 84, 86, 92, 93, 94, 95, 96, 98, 99, 101, 102, 103, 104, 105, 106, 107, 109, 110, 111, 112, 113, 115, 116, 117, 118, 120, 123, 125, 126, 127, 129, 132, 133, 134, 135, 136, 137, 140, 142, 143, 147, 148, 149, 152, 153, 154, 155, 156, 157, 158, 160, 161, 162, 163, 165, 166, 167, 168, 169, 171, 175, 176, 177, 178, 179, 180, 182, 183, 185, 186, 188, 189, 191, 192, 194, 195, 196, 197, 200, 201, 202, 203, 204, 207, 208, 211, 212, 214, 215, 218, 219, 221, 224]\n",
    "if collection == \"robust\":\n",
    "    diffTopics = [301, 305, 309, 314, 315, 318, 319, 320, 322, 325, 327, 332, 336, 340, 342, 343, 344, 345, 346, 347, 350, 352, 354, 355, 356, 359, 363, 367, 371, 372, 376, 378, 379, 380, 381, 383, 386, 388, 389, 390, 393, 394, 397, 398, 401, 405, 409, 412, 419, 421, 426, 432, 435, 436, 437, 439, 440, 442, 448, 449, 605, 608, 610, 620, 622, 625, 626, 627, 638, 650, 651, 655, 659, 666, 668, 680, 683, 684, 688, 689, 690]\n",
    "if collection == \"trec7n8\":\n",
    "    diffTopics = [352,354,356,359,363,367,370,371,372,373,376,378,379,381,383,386,388,389,390,393,394,397,398,401,405,408,419,421,422,426,432,435,436,437,439,440,442,443,445,447,448,449]\n",
    "diffTopic = [str(i) for i in diffTopics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generate docIdNameMapFileName\n",
    "if not os.path.isfile(occuranceCountFilename):\n",
    "    cmd = ' '.join([occuranceCountFilename, colIndexDir, \"dm\", \">\", docIdNameMapFileName])\n",
    "    res = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE).stdout.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', 'G45-42-0000000']\n",
      "G27-29-4040585 \t 1220122\n",
      "G06-40-3131996 \t 281807\n",
      "G32-16-0767517 \t 336566\n",
      "G38-20-1081477 \t 35031\n",
      "G09-37-0269135 \t 606264\n",
      "G35-90-1409165 \t 1000554\n",
      "G45-00-1993329 \t 6030\n",
      "G02-14-0310925 \t 409727\n",
      "G24-75-3379489 \t 482494\n",
      "G41-54-1115839 \t 613900\n",
      "G08-45-4056636 \t 443549\n",
      "G07-57-3916819 \t 649375\n",
      "G18-64-1809424 \t 382013\n",
      "G01-53-1599922 \t 56902\n",
      "G09-45-1860255 \t 607278\n",
      "G10-84-0819779 \t 126372\n",
      "G07-75-1332741 \t 640405\n",
      "G41-16-2022415 \t 612570\n",
      "G28-26-3010045 \t 735202\n",
      "G40-48-0701479 \t 932948\n",
      "G10-12-3054008 \t 136841\n",
      "G27-23-2273389 \t 1206944\n"
     ]
    }
   ],
   "source": [
    "with open(docIdNameMapFileName, 'r') as f:\n",
    "    reader = list(csv.reader(f, delimiter = ' '))\n",
    "    print(reader[0])\n",
    "    docIdNameMap = {i[1]:i[0] for i in reader}\n",
    "    docNameIdMap = {i[0]:i[1] for i in reader}\n",
    "for i, (k, v) in enumerate(docIdNameMap.iteritems()):\n",
    "    print (k, '\\t', v)\n",
    "    if (i>20):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count_history = dict()\n",
    "if os.path.isfile(countsResultsFile):\n",
    "    with open(countsResultsFile, 'r') as f:\n",
    "        reader = csv.reader(f, delimiter = \"\\t\")\n",
    "        count_history = {k:int(float(v)) for k,v in list(reader)}\n",
    "    print(\"size of count_history =\", len(count_history))\n",
    "    for i, (k, v) in enumerate(count_history.iteritems()):\n",
    "        print (k, '\\t', v)\n",
    "        if (i>20):\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of conceptnet5RelAll = 2965684\n",
      "tripolitan \t tripoline \t Synonym\n",
      "tripolitan \t tripoli \t RelatedTo\n",
      "age of nemesis \t band \t IsA\n",
      "age of nemesis \t organisation \t InstanceOf\n",
      "age of nemesis \t progressive rock \t dbpedia/genre\n",
      "age of nemesis \t progressive metal \t dbpedia/genre\n",
      "joseph john annabring \t person \t InstanceOf\n",
      "british rail class 438 \t mean of transportation \t InstanceOf\n",
      "british rail class 438 \t train \t InstanceOf\n"
     ]
    }
   ],
   "source": [
    "conceptnet5RelAll = defaultdict(dict)\n",
    "conceptnet5RelAllInv = defaultdict(dict)\n",
    "with open(conceptnet5RelAllFilename, 'r') as f:\n",
    "    reader = csv.reader(f, delimiter = \",\")\n",
    "    for line in list(reader):\n",
    "        if line[0].strip() != \"\" and line[1].strip() != \"\" and line[2].strip() != \"\":\n",
    "            if all(c in string.printable for c in line[0]) and all(c in string.printable for c in line[1]) and all(c in string.printable for c in line[2]):\n",
    "                conceptnet5RelAll[line[1].strip()][line[2].strip()] = line[0].strip()\n",
    "                conceptnet5RelAllInv[line[2].strip()][line[1].strip()] = line[0].strip()\n",
    "print(\"size of conceptnet5RelAll =\", len(conceptnet5RelAll))\n",
    "for i, (k1, k2v) in enumerate(conceptnet5RelAll.iteritems()):\n",
    "    for j, (k2, v) in enumerate(k2v.iteritems()):\n",
    "        if (i>3 or j>3):\n",
    "            break\n",
    "        print (k1, '\\t', k2, '\\t', v)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indriRunQuery_hist_= dict()\n",
    "dumpindex_hist_ = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing: ['go', 'read']\n"
     ]
    }
   ],
   "source": [
    "def splitStemText(text):\n",
    "    stemmedWords = []\n",
    "    text = re.sub('/|-|\\\"|_',' ', text) # replace - and slash with space\n",
    "    if method in {\"assocRestful\", \"lr\", \"wsdmImpr\"}:\n",
    "        for text_ in text.split(' '):\n",
    "            text = text_.replace(\" \", \"_\")\n",
    "            url = \"http://conceptnet5.media.mit.edu/data/5.4/uri?language=en&text=\" + text\n",
    "            response = urllib2.urlopen(url)\n",
    "            data = response.read()\n",
    "            data_j = json.loads(data)\n",
    "            text = os.path.basename(data_j[u'uri'])\n",
    "            url = \"http://conceptnet5.media.mit.edu/data/5.4/uri?language=en&text=\" + text\n",
    "            response = urllib2.urlopen(url)\n",
    "            data = response.read()\n",
    "            data_j = json.loads(data)\n",
    "            #print(\"data_j =\", data_j)\n",
    "            #print(\"data_j[u'uri'] =\", data_j[u'uri'])\n",
    "            stemmedWords += [str(os.path.basename(data_j[u'uri']))]\n",
    "        #print(\"stemmedWords =\", stemmedWords)\n",
    "        return (stemmedWords)\n",
    "    else:\n",
    "        words = text.split()\n",
    "        for w in words:\n",
    "            w = re.sub('\\(|\\)|\\'s|,','', w) # remove paranthesis, apostrophe s, comma\n",
    "            w = re.sub('\\'','', w) # remove apostrophe\n",
    "            cmd = \"dumpindex \" + knowledgGraphIndexDir + \" t \" + w + \" | head -n1\"\n",
    "            #print(\"cmd =\", cmd)\n",
    "            stemmedWords.append(subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE).stdout.read().split()[1])\n",
    "        return (stemmedWords)\n",
    "\n",
    "print (\"testing:\", splitStemText(\"going reading\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def analyseQueries():\n",
    "    #    #print(\"intCoeff =\", intCoeff)\n",
    "    with open(cfgInFileName, 'r') as inFile:\n",
    "        reader = inFile.read()\n",
    "        soup = BeautifulSoup(reader, 'lxml')\n",
    "        if collection in {\"aquaint\", \"gov\"}:\n",
    "            tags = soup.find_all(['doc'])\n",
    "        elif collection in {\"robust\", \"trec7n8\", \"gov2\"}:\n",
    "            tags = soup.find_all(['top'])\n",
    "            #print(tags)\n",
    "        \n",
    "        origWordsAll = dict()\n",
    "        for tag in tags:\n",
    "            \n",
    "            if collection in {\"aquaint\", \"gov\"}:\n",
    "                docno = (tag.find('docno').string).strip()\n",
    "            elif collection in {\"robust\", \"trec7n8\", \"gov2\"}:\n",
    "                result = re.search('<num> Number: (.*)\\n', str(tag))                \n",
    "                docno = result.group(1).strip()\n",
    "            \n",
    "            if collection in {\"aquaint\", \"gov\"}:\n",
    "                origWords = splitStemText(tag.find('text').string.strip())\n",
    "            elif collection in {\"robust\", \"trec7n8\", \"gov2\"}:\n",
    "                result = re.search('<title>(.*)\\n', str(tag))                \n",
    "                origWords = splitStemText(result.group(1).strip())\n",
    "            #print(\"origWords =\", origWords)\n",
    "            origWordsAll[docno] = origWords\n",
    "            \n",
    "    return (origWordsAll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "colDocumentCount = 1247753\n"
     ]
    }
   ],
   "source": [
    "cmd = ' '.join([\"dumpindex\", colIndexDir, \"s\", \"|\", \"awk\", \"\\'NR==2{print $2}\\'\"])\n",
    "res = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE).stdout.read()\n",
    "colDocumentCount = int(np.float(res))\n",
    "print(\"colDocumentCount =\", colDocumentCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def genMultSimpQuery(argss):\n",
    "    #    #print(\"intCoeff =\", intCoeff)\n",
    "    soupNew = BeautifulSoup(\"\", 'lxml')\n",
    "    parameters_tag = soupNew.new_tag(\"parameters\")\n",
    "    index_tag = soupNew.new_tag(\"index\")\n",
    "    index_tag.string = colIndexDir\n",
    "    parameters_tag.append(index_tag)\n",
    "        #print(tags)\n",
    "\n",
    "    for args in argss:\n",
    "        #print(\"args =\", args)\n",
    "        (dist, docno, origWords_, relW, intCoeff0, intCoeff1, intCoeff2, dirCoeff, featureWeights, workingset) = args\n",
    "        #print(\"(...) =\", docno, origWords_, intCoeff0, intCoeff1, intCoeff2, dirCoeff, featureWeights, workingset)\n",
    "        doc_tag = soupNew.new_tag(\"query\")\n",
    "\n",
    "        docno_tag = soupNew.new_tag(\"number\")\n",
    "        if relW != \"\":\n",
    "            docno_tag.string = '_'.join([dist, docno, ('_'.join(origWords_)).replace(' ', '_'), relW.replace(' ', '_')])\n",
    "        else:\n",
    "            docno_tag.string = '_'.join([dist, docno, ('_'.join(origWords_)).replace(' ', '_'), relW.replace(' ', '_')])\n",
    "\n",
    "        doc_tag.append(docno_tag)\n",
    "\n",
    "        text_tag = soupNew.new_tag(\"text\")\n",
    "        text_tag.string = \"#weight(\\n\" \n",
    "\n",
    "        text_tag.string += str(intCoeff0) + \" #combine(\" \n",
    "        for ow in set(origWords_):\n",
    "            ow = re.sub(ur\"[^\\w\\d#(),:\\-\\s]+\", ' ', ow)\n",
    "            text_tag.string += ow + \" \"\n",
    "        text_tag.string += \")\\n\"\n",
    "\n",
    "        if len(relW)>0: \n",
    "            relText_string1 = re.sub(ur\"[^\\w\\d#(),:\\-\\s]+\",' ',relW)\n",
    "            text_tag.string += str(intCoeff1) + \" #combine(\" \n",
    "            #print(\"relText_string1 =\", relText_string1)\n",
    "            text_tag.string += relText_string1.encode('utf-8')\n",
    "            text_tag.string += \")\\n\"\n",
    "\n",
    "        text_tag.string += \") \"\n",
    "\n",
    "        doc_tag.append(text_tag)\n",
    "        \n",
    "        if workingset[0] != \"all\":\n",
    "            for d in workingset:\n",
    "                workingSetDocno_tag = soupNew.new_tag(\"workingSetDocno\")\n",
    "                workingSetDocno_tag.string = d\n",
    "                doc_tag.append(workingSetDocno_tag)\n",
    "\n",
    "        parameters_tag.append(doc_tag)\n",
    "        #print(doc_tag)\n",
    "\n",
    "    rule_tag = soupNew.new_tag(\"rule\")\n",
    "    rule_tag.string = \"method:dir,mu:\" + str(dirCoeff)\n",
    "    #rule_tag.string = \"method:two\"\n",
    "    parameters_tag.append(rule_tag)\n",
    "\n",
    "    threads_tag = soupNew.new_tag(\"threads\")\n",
    "    threads_tag.string = \"32\"\n",
    "    parameters_tag.append(threads_tag)\n",
    "\n",
    "    count_tag = soupNew.new_tag(\"count\")\n",
    "    count_tag.string = \"10\"\n",
    "    parameters_tag.append(count_tag)\n",
    "\n",
    "    trecFormat_tag = soupNew.new_tag(\"trecFormat\")\n",
    "    trecFormat_tag.string = \"true\"\n",
    "    parameters_tag.append(trecFormat_tag)\n",
    "\n",
    "    soupNew.append(parameters_tag)\n",
    "    #print(soupNew.prettify())\n",
    "    #print(\"cfgTmpOutFileName =\", cfgTmpOutFileName)\n",
    "    with open( cfgTmpOutFileName, 'a+') as outFile:\n",
    "        soupNewStr = str(soupNew)\n",
    "        soupNewStr = soupNewStr.replace(\"</text>\", \"\\n</text>\\n\").replace(\"query>\", \"query>\\n\").replace(\"<text>\", \"\\n<text>\\n\").replace(\"</index>\", \"</index>\\n\").replace(\"\\n<index>\", \"<index>\")\n",
    "\n",
    "        outFile.write(soupNewStr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def genCommandsinHist_i(dist, docno, origWords_, relW, intCoeff0, intCoeff1, intCoeff2, dirCoeff, featureWeights):\n",
    "    \n",
    "    indriRunQuery_hist = [(dist, docno, origWords_, relW, intCoeff0, intCoeff1, intCoeff2, dirCoeff, cfgTmpOutFileName, ['all'])]\n",
    "    return indriRunQuery_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def runCommandsinHist_i(cfgTmpOutFileName, indriRunQuery_hist, indriRunQuery_hist_, featureWeights):\n",
    "    with open( cfgTmpOutFileName, 'w+') as outFile:\n",
    "        outFile.write(\"\")\n",
    "    args = []\n",
    "    #print(\"indriRunQuery_hist =\", indriRunQuery_hist, \"\\nlen(indriRunQuery_hist) =\",len(indriRunQuery_hist))\n",
    "    for (dist, docno, origWords_, relW, intCoeff0, intCoeff1, intCoeff2, dirCoeff, cfgTmpOutFileName, workingset) in indriRunQuery_hist:\n",
    "        #print(\"origWords_, relW (before if) =\", origWords_, \"---\", relW)\n",
    "        if '_'.join([dist, docno, ('_'.join(origWords_)).replace(' ', '_'), relW.replace(' ', '_')]) not in indriRunQuery_hist_:\n",
    "            #print(\"origWords_, relW (after if) =\", origWords_, \"---\", relW)\n",
    "            args += [(dist, docno, origWords_, relW, intCoeff0, intCoeff1, intCoeff2, dirCoeff, featureWeights, workingset)]\n",
    "    #print(\"args =\", args)\n",
    "    if len(args)>0:\n",
    "        genMultSimpQuery(args)\n",
    "    cmd = ' '.join([\"IndriRunQuery\", cfgTmpOutFileName])\n",
    "    #print(\"cmd =\", cmd)\n",
    "    indriRun = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE).stdout.read()\n",
    "    #print(\"indriRun =\", indriRun)\n",
    "    indriRunQuery_hist_tmp = [[j for c, j in enumerate(i.split(' ')) if c in {0, 2, 4}] for i in indriRun.split('\\n') if len(i)>0]\n",
    "    #indriRunQuery_hist_ = dict()\n",
    "    for i in indriRunQuery_hist_tmp:\n",
    "        if i[0] in indriRunQuery_hist_:\n",
    "            indriRunQuery_hist_[i[0]] += [[i[1], i[2]]]\n",
    "        else:\n",
    "            indriRunQuery_hist_[i[0]] = [[i[1], i[2]]]\n",
    "    #indriRunQuery_hist_ = list(set(indriRunQuery_hist_))\n",
    "    return indriRunQuery_hist_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def genCommandsinHist_d(indriRunQuery_hist_, dumpindex_hist, dist, docno, origWords_, ow, relW, intCoeff0, intCoeff1, intCoeff2, dirCoeff, featureWeights):\n",
    "\n",
    "    expr = '_'.join([dist, docno, '_'.join(origWords_), \"\"]).replace(' ', '_')\n",
    "    #print(\"expr =\", expr)\n",
    "    indriRunQuery_hist_1 = indriRunQuery_hist_[expr]\n",
    "    #print(\"indriRunQuery_hist_1 =\", indriRunQuery_hist_1)\n",
    "    topTDocs = [i[0] for i in indriRunQuery_hist_1]\n",
    "    \n",
    "    expr = \"#4( \" + regex.sub(' ', relW) + \" )\"\n",
    "    #expr = '_'.join([dist, docno, '_'.join(origWords_), \"\"]).replace(' ', '_')\n",
    "    #print(\"expr =\", expr)\n",
    "    #cmd2 = [(dist, docno, [expr], \"\", intCoeff0, intCoeff1, intCoeff2, dirCoeff, cfgTmpOutFileName, topTDocs)]\n",
    "    #if cmd2 not in indriRunQuery_phCounter_hist:\n",
    "    #    indriRunQuery_phCounter_hist += cmd2\n",
    "    cmd2 = (\"efb\", expr + \":\" + ','.join(topTDocs))\n",
    "    if cmd2 not in dumpindex_hist:\n",
    "        dumpindex_hist.add(cmd2)\n",
    "    \n",
    "    #print(\"relW =\", relW)\n",
    "    expr = \"#4( \" + regex.sub(' ', relW) + \" )\"\n",
    "    cmd2 = (\"fx\", expr)\n",
    "    if cmd2 not in dumpindex_hist:\n",
    "        dumpindex_hist.add(cmd2)\n",
    "    #print(\"dumpindex_hist =\", dumpindex_hist)\n",
    "    \n",
    "    for documentName in topTDocs:\n",
    "        cmd2 = (\"dcf\", docIdNameMap[documentName])\n",
    "        if cmd2 not in dumpindex_hist:\n",
    "            dumpindex_hist.add(cmd2)\n",
    "\n",
    "    for ow in origWords_:\n",
    "        phrase = [\"#uw(#4( \" + regex.sub(' ', ow) + \" ) #4( \" + regex.sub(' ', relW) + \" ))\"]\n",
    "        #cmd2 = (\"efb\", \"#uw(#4( \" + regex.sub(' ', ow) + \" ) #4( \" + regex.sub(' ', relW) + \" )):\" + ','.join(topTDocs))\n",
    "        #cmd2 = [(dist, docno, phrase, \"\", intCoeff0, intCoeff1, intCoeff2, dirCoeff, cfgTmpOutFileName, topTDocs)]\n",
    "        #if cmd2 not in indriRunQuery_phCounter_hist:\n",
    "        #    indriRunQuery_phCounter_hist += cmd2        \n",
    "        cmd2 = (\"fx\", phrase[0])\n",
    "        if cmd2 not in dumpindex_hist:\n",
    "            dumpindex_hist.add(cmd2)\n",
    "        \n",
    "        cmd2 = (\"efb\", phrase[0] + \":\" + ','.join(topTDocs))\n",
    "        if cmd2 not in dumpindex_hist:\n",
    "            dumpindex_hist.add(cmd2)\n",
    "            \n",
    "    for j1, ow1 in enumerate(origWords_):\n",
    "        for j2, ow2 in enumerate(origWords_):\n",
    "            if j1>j2:\n",
    "                phrase = [\"#uw(#4( \" + regex.sub(' ', ow1) + \" ) #4( \" + regex.sub(' ', ow2) + \" ) #4( \" + regex.sub(' ', relW) + \" ))\"]\n",
    "                #cmd2 = [(dist, docno, phrase, \"\", intCoeff0, intCoeff1, intCoeff2, dirCoeff, cfgTmpOutFileName, topTDocs)]\n",
    "                #if cmd2 not in indriRunQuery_phCounter_hist:\n",
    "                #    indriRunQuery_phCounter_hist += cmd2        \n",
    "                #cmd2 = (\"fx\", phrase[0])\n",
    "                cmd2 = (\"fx\", phrase[0])\n",
    "                if cmd2 not in dumpindex_hist:\n",
    "                    dumpindex_hist.add(cmd2)\n",
    "                cmd2 = (\"efb\", phrase[0] + \":\" + ','.join(topTDocs))\n",
    "                if cmd2 not in dumpindex_hist:\n",
    "                    dumpindex_hist.add(cmd2)\n",
    "    #print(\"len(dumpindex_hist) =\", len(dumpindex_hist))\n",
    "    return dumpindex_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "def runCommandsinHist_d(dist, docno, dumpindex_hist, dumpindex_hist_):\n",
    "    args = []\n",
    "    dumpindex_hist_dict = dict()\n",
    "    tmp = set()\n",
    "    #print(\"dumpindex_hist =\", dumpindex_hist)\n",
    "    for i in dumpindex_hist:\n",
    "        expr = '_'.join([dist, docno, '_'.join(i[0]), \"\"]).replace(' ', '_')\n",
    "        if i[1] not in dumpindex_hist_.get(expr, dict()):\n",
    "#        if i[1] not in dumpindex_hist_.get(dist + \"_\" + docno + \"_\" + i[0], dict()):\n",
    "            if i[1] in tmp:\n",
    "                continue\n",
    "            #print(\"i =\", i)\n",
    "            tmp.add(i[1])\n",
    "            if i[0] in dumpindex_hist_dict:\n",
    "                dumpindex_hist_dict[i[0]] += [[re.sub(ur\"[^\\w\\d#(),:\\-\\s]+\",' ',j) for j in i[1:]]]\n",
    "            else:\n",
    "                dumpindex_hist_dict[i[0]] =  [[re.sub(ur\"[^\\w\\d#(),:\\-\\s]+\",' ',j) for j in i[1:]]]\n",
    "                \n",
    "    for k, v in dumpindex_hist_dict.iteritems():\n",
    "        fileName = dumpindexStatementFilename + \"_\" + k + \".tmp\"\n",
    "        with open(fileName, 'w') as f:\n",
    "            for i in v:\n",
    "                f.write(' '.join(i) + '\\n')\n",
    "                #print(\"i =\", i)\n",
    "        cmd = ' '.join([occuranceCountFilename, colIndexDir, k, fileName])\n",
    "        #print(\"cmd =\", cmd)\n",
    "        dumpindex_hist_tmp = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE).stdout.read()\n",
    "        #print(\"dumpindex_hist_tmp =\", dumpindex_hist_tmp)\n",
    "        dumpindex_hist_tmp = dict([tuple(j for c2, j in enumerate(i.split(':')) if c2 in {0,1} ) for i in dumpindex_hist_tmp.split(\"\\n\") if len(i)>0])\n",
    "        #print(\"dumpindex_hist_tmp.get(\\\"#od4(travel)\\\",\\\"\\\") =\", dumpindex_hist_tmp.get(\"#od4(travel)\", dict()) )\n",
    "        dumpindex_hist_tmp = {k1:v1.split(',') for k1,v1 in dumpindex_hist_tmp.iteritems()}\n",
    "        dumpindex_hist_tmp = {dist + \"_\" + docno + \"_\" + k1:[i for i in v1 if len(i)>0] for k1,v1 in dumpindex_hist_tmp.iteritems()}\n",
    "        #dumpindex_hist_tmp = {re.sub(ur\"[^\\w\\d#()\\s]+\",' ',k1):[i for i in v1 if len(i)>0] for k1,v1 in dumpindex_hist_tmp.iteritems()}\n",
    "        #dumpindex_tmp = [i for i in dumpindex_tmp.split(\"\\n\")]\n",
    "        #print(\"dumpindex_hist_tmp =\", dumpindex_hist_tmp)\n",
    "        #dumpindex_hist_[k] = dumpindex_hist_tmp\n",
    "        #expr = '_'.join([dist, docno, '_'.join(k), \"\"]).replace(' ', '_')    \n",
    "        dumpindex_hist_[k] = dict(dumpindex_hist_.get(k, dict()).items() + dumpindex_hist_tmp.items())\n",
    "        #dumpindex_hist_[dist + \"_\" + docno + \"_\" + k] = dict(dumpindex_hist_.get(dist + \"_\" + docno + \"_\" + k, dict()).items() + dumpindex_hist_tmp.items())\n",
    "    #print(\"dumpindex_hist_['efb'].get('#od4(travel)', dict())]\", dumpindex_hist_['efb'].get(\"#od4(travel)\", dict()))\n",
    "        #print(\"dumpindex_hist_[k].keys() =\", dumpindex_hist_[k].keys())\n",
    "    #print(\"dumpindex_hist_ =\", dumpindex_hist_)\n",
    "        \n",
    "    return dumpindex_hist_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# keys of rWords are important\n",
    "# values of rWords1 are important\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "def getRelWords_train(args, indriRunQuery_hist_, dumpindex_hist_):\n",
    "    #print(\"origWords =\", origWords)\n",
    "    text_string = \"\"\n",
    "    \n",
    "    count_history, rWords, dist, docno, origWords, expansionCount, intCoeff0, intCoeff1, intCoeff2, dirCoeff, featureWeights = args\n",
    "        \n",
    "    relWords = dict()\n",
    "    #if method == \"relAll\":\n",
    "            \n",
    "    if method in {\"lr\", \"wsdmImpr\"}:\n",
    "        origWords_ = [w.replace(\" \", \"_\") for w in origWords]\n",
    "        #relWords_l = []\n",
    "        dumpindex_hist = set()\n",
    "\n",
    "        indriRunQuery_hist = []\n",
    "        indriRunQuery_hist += genCommandsinHist_i(dist, docno, origWords_, \"\", intCoeff0, intCoeff1, intCoeff2, dirCoeff, featureWeights)\n",
    "        #for relW in conceptnet5RelAll[ow].keys():\n",
    "        #    indriRunQuery_hist += genCommandsinHist_i(dist, docno, origWords_, ow, relW, intCoeff0, intCoeff1, intCoeff2, dirCoeff, featureWeights)\n",
    "\n",
    "        indriRunQuery_hist_ = runCommandsinHist_i(cfgTmpOutFileName, indriRunQuery_hist, indriRunQuery_hist_, featureWeights)\n",
    "        #print(\"indriRunQuery_hist =\", indriRunQuery_hist)\n",
    "        for ow in origWords_:\n",
    "            #print(\"ow =\", ow)\n",
    "            #relWords_l += list(conceptnet5RelAll[ow])\n",
    "            #print(ow, \"---\", conceptnet5RelAll[ow])\n",
    "\n",
    "            \n",
    "            \n",
    "            for relW in conceptnet5RelAll[ow].keys():\n",
    "                dumpindex_hist |= genCommandsinHist_d(indriRunQuery_hist_, dumpindex_hist, dist, docno, origWords_, ow, relW, intCoeff0, intCoeff1, intCoeff2, dirCoeff, featureWeights)\n",
    "                #dumpindex_hist |= genCommandsinHist_id(indriRunQuery_hist_, dumpindex_hist, indriRunQuery_phCounter_hist, docno, origWords_, ow, relW, intCoeff0, intCoeff1, intCoeff2, dirCoeff, featureWeights)\n",
    "                #print(\"indriRunQuery_phCounter_hist =\", indriRunQuery_phCounter_hist)\n",
    "                #indriRunQuery_phCounter_hist = [list(x) for x in set(tuple(x) for x in indriRunQuery_phCounter_hist)]\n",
    "            \n",
    "        dumpindex_hist_ = runCommandsinHist_d(dist, docno, dumpindex_hist, dumpindex_hist_)\n",
    "        \n",
    "        #indriRunQuery_hist_ = runCommandsinHist_i(cfgTmpOutFileName, indriRunQuery_phCounter_hist, indriRunQuery_hist_, featureWeights)\n",
    "\n",
    "    return(indriRunQuery_hist_, dumpindex_hist_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "def documentLength(dist, docno, documentName, dumpindex_hist_):\n",
    "    #documentLength_ = int(dumpindex_hist_['dcf'][documentName][0])\n",
    "    documentLength_ = int(dumpindex_hist_['dcf'][dist + \"_\" + docno + \"_\" + documentName][0])\n",
    "    return documentLength_\n",
    "\n",
    "def weightRelConcept(dist, docno, origWords_, ow, relW, intCoeff0, intCoeff1, intCoeff2, dirCoeff, count_history, featureWeights, indriRunQuery_hist_, dumpindex_hist_):\n",
    "    \n",
    "    features = dict()\n",
    "    \n",
    "    expr_i = '_'.join([dist, docno, '_'.join(origWords_), \"\"]).replace(' ', '_')\n",
    "    #print(\"expr_i =\", expr_i)\n",
    "    indriRunQuery_hist_1 = indriRunQuery_hist_[expr_i]\n",
    "    #print(\"indriRunQuery_hist_1 =\", indriRunQuery_hist_1)\n",
    "    topTDocs = [i[0] for i in indriRunQuery_hist_1]\n",
    "    \n",
    "    topTDocScores_d = {i[0]:np.float(i[1]) for i in indriRunQuery_hist_1}\n",
    "    \n",
    "    \n",
    "    expTDocScore = np.float(indriRunQuery_hist_1[0][1])\n",
    "    features[\"expTDocScore\"] = expTDocScore\n",
    "    \n",
    "    expr = \"#4( \" + regex.sub(' ', relW) + \" )\"\n",
    "    #expr = expr.replace(' ', '_')\n",
    "    #print(\"expr =\", expr)\n",
    "    #expr_i = '_'.join([dist, docno, '_'.join([expr.replace(' ', '_')]), \"\"]).replace(' ', '_')\n",
    "    #indriRunQuery_hist_1 = indriRunQuery_hist_[expr_i]\n",
    "    #print(\"indriRunQuery_hist_1 =\", indriRunQuery_hist_1)\n",
    "    relWTopDocuments = dumpindex_hist_['efb'][dist + \"_\" + docno + \"_\" + expr][1:]\n",
    "    #relWTopDocuments = list([i[0] for i in indriRunQuery_hist_1]) #deep copy\n",
    "    \n",
    "    numerator = len(relWTopDocuments) # relW term count in the collection\n",
    "    denumerator = np.sum([documentLength(dist, docno, i, dumpindex_hist_) for i in topTDocs]) # number of terms in topDocs\n",
    "    topTermFrac = numerator / np.float(denumerator)\n",
    "    features[\"topTermFrac\"] = topTermFrac\n",
    "    \n",
    "    numCanDocs = len(set([i[0] for i in relWTopDocuments]))\n",
    "    features[\"numCanDocs\"] = numCanDocs\n",
    "    \n",
    "    if numCanDocs > 0:\n",
    "        avgCDocScore = np.sum([ topTDocScores_d[i] for i in set([i for i in relWTopDocuments])])/np.float(numCanDocs)\n",
    "    else:\n",
    "        avgCDocScore = 0\n",
    "    features[\"avgCDocScore\"] = avgCDocScore\n",
    "\n",
    "    l = [ topTDocScores_d[i] for i in set(relWTopDocuments)]\n",
    "    if len(l)>0:\n",
    "        maxCDocScore = np.max(l)\n",
    "    else:\n",
    "        maxCDocScore = 0\n",
    "    features[\"maxCDocScore\"] = maxCDocScore\n",
    "    \n",
    "    colDocFreq = np.float(dumpindex_hist_['fx'][dist + \"_\" + docno + \"_\" + expr][0])\n",
    "    if colDocFreq>0:\n",
    "        conIdf = np.log(colDocumentCount / colDocFreq)\n",
    "    else:\n",
    "        conIdf = np.log(colDocumentCount / 1)\n",
    "    features[\"conIdf\"] = conIdf\n",
    "    \n",
    "    coocurDocumentsTop = dict()\n",
    "    coocurDocumentsCount = dict()\n",
    "    for ow in origWords_:\n",
    "        expr = \"#uw(#4( \" + regex.sub(' ', ow) + \" ) #4( \" + regex.sub(' ', relW) + \" ))\"\n",
    "        #expr = expr.replace(' ', '_')\n",
    "        #expr_i = '_'.join([dist, docno, '_'.join([expr.replace(' ', '_')]), \"\"]).replace(' ', '_')\n",
    "        #indriRunQuery_hist_1 = indriRunQuery_hist_[expr_i]\n",
    "        coocurDocumentsTop[ow] = dumpindex_hist_['efb'][dist + \"_\" + docno + \"_\" + expr][1:]\n",
    "        #coocurDocumentsTop[ow] = list([i[0] for i in indriRunQuery_hist_1])\n",
    "        coocurDocumentsCount[ow] = np.float(dumpindex_hist_['fx'][dist + \"_\" + docno + \"_\" + expr][0])\n",
    "\n",
    "    avgColCor = np.sum(coocurDocumentsCount.values())/np.float(len(coocurDocumentsCount.values())) \n",
    "    features[\"avgColCor\"] = avgColCor\n",
    "    \n",
    "    maxColCor = np.max(coocurDocumentsCount.values())\n",
    "    features[\"maxColCor\"] = maxColCor\n",
    "    \n",
    "    avgTopColCor = np.sum([len(set(i)) for i in coocurDocumentsTop.values()])/np.float(len(coocurDocumentsTop.values())) \n",
    "    features[\"avgTopColCor\"] = avgTopColCor\n",
    "    \n",
    "    l = [len(set(i)) for i in coocurDocumentsTop.values()]\n",
    "    if len(l)>0:\n",
    "        maxTopColCor = np.max(l)\n",
    "    else:\n",
    "        maxTopColCor = 0\n",
    "    features[\"maxTopColCor\"] = maxTopColCor\n",
    "\n",
    "    coocurDocumentsTop = dict()\n",
    "    coocurDocumentsCount = dict()\n",
    "    for j1, ow1 in enumerate(origWords_):\n",
    "        for j2, ow2 in enumerate(origWords_):\n",
    "            if j1>j2:\n",
    "                expr = \"#uw(#4( \" + regex.sub(' ', ow1) + \" ) #4( \" + regex.sub(' ', ow2) + \" ) #4( \" + regex.sub(' ', relW) + \" ))\"\n",
    "                #expr = expr.replace(' ', '_')\n",
    "                #expr_i = '_'.join([dist, docno, '_'.join([expr.replace(' ', '_')]), \"\"]).replace(' ', '_')\n",
    "                #indriRunQuery_hist_1 = indriRunQuery_hist_[expr_i]\n",
    "                #indriRunQuery_hist_1 = indriRunQuery_hist_['_'.join([dist, docno, expr]).replace(' ', '_')]\n",
    "                coocurDocumentsTop[ow1+ow2] = dumpindex_hist_['efb'][dist + \"_\" + docno + \"_\" + expr][1:]\n",
    "                #coocurDocumentsTop[ow1+ow2] = list([i[0] for i in indriRunQuery_hist_1])\n",
    "                coocurDocumentsCount[ow] = np.float(dumpindex_hist_['fx'][dist + \"_\" + docno + \"_\" + expr][0])\n",
    "\n",
    "    avgColPCor = np.sum(coocurDocumentsCount.values())/np.float(len(coocurDocumentsCount.values()))  \n",
    "    features[\"avgColPCor\"] = avgColPCor\n",
    "\n",
    "    if len(coocurDocumentsCount.values())>0:\n",
    "        maxColPCor = np.max(coocurDocumentsCount.values())\n",
    "    else:\n",
    "        maxColPCor = 0\n",
    "    features[\"maxColPCor\"] = maxColPCor\n",
    "    \n",
    "    avgTopColPCor = np.sum([len(set(i)) for i in coocurDocumentsTop.values()])/np.float(len(coocurDocumentsTop.values())) \n",
    "    features[\"avgTopColPCor\"] = avgTopColPCor\n",
    "\n",
    "    l = [len(set(i)) for i in coocurDocumentsTop.values()] \n",
    "    if len(l)>0:\n",
    "        maxTopColPCor = np.max(l)\n",
    "    else:\n",
    "        maxTopColPCor = 0\n",
    "    features[\"maxTopColPCor\"] = maxTopColPCor\n",
    "        \n",
    "    relWg = 0\n",
    "    for feature, weight in featureWeights.iteritems():\n",
    "        relWg += features[feature] * weight\n",
    "    \n",
    "    #print(\"origWords_ =\", origWords_)\n",
    "    #print(\"relW =\", relW)\n",
    "    #print(\"features =\", features)\n",
    "    #print(\"relWg =\", relWg)\n",
    "\n",
    "    return relWg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(dumpindex_hist_['efb'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# keys of rWords are important\n",
    "# values of rWords1 are important\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "def getRelWords(args, indriRunQuery_hist_, dumpindex_hist_):\n",
    "    #print(\"origWords =\", origWords)\n",
    "    text_string = \"\"\n",
    "    \n",
    "    count_history, rWords, dist, docno, origWords, expansionCount, intCoeff0, intCoeff1, intCoeff2, dirCoeff, featureWeights = args\n",
    "        \n",
    "    relWords = dict()\n",
    "    #if method == \"relAll\":\n",
    "        \n",
    "    if method == \"assocRestful\":\n",
    "        origWords_ = [w.replace(\" \", \"_\") for w in origWords]\n",
    "        url = \"http://conceptnet5.media.mit.edu/data/5.4/assoc/list/en/\"+','.join(origWords_)\n",
    "        response = urllib2.urlopen(url)\n",
    "        data = response.read()\n",
    "        data_j = json.loads(data)\n",
    "        relWords = {str(d[0].encode('utf-8')).replace('/c/en/', '').replace('-', ' '):np.float(d[1]) for d in data_j[u'similar'] if '/c/en/' in str(d[0].encode('utf-8'))}\n",
    "    \n",
    "    elif method in {\"lr\", \"wsdmImpr\"}:\n",
    "        origWords_ = [w.replace(\" \", \"_\") for w in origWords]\n",
    "        #relWords_l = []\n",
    "\n",
    "        for ow in origWords_:\n",
    "            #relWords_l += list(conceptnet5RelAll[ow])\n",
    "            #print(ow, \"---\", conceptnet5RelAll[ow])\n",
    "            for relW in conceptnet5RelAll[ow].keys():\n",
    "                relWg = weightRelConcept(dist, docno, origWords_, ow, relW, intCoeff0, intCoeff1, intCoeff2, dirCoeff, count_history, featureWeights, indriRunQuery_hist_, dumpindex_hist_)\n",
    "                if relW in relWords:\n",
    "                    relWords[relW] += relWg\n",
    "                else:\n",
    "                    relWords[relW] = relWg  \n",
    "    \n",
    "    else:\n",
    "        count_history = countExpr(count_history, origWords, rWords)\n",
    "        expressions = set()\n",
    "        for ow in origWords:\n",
    "            rWords_ow = rWords.get(ow, [])\n",
    "            expressions.add(\"#4( \" + ow.translate(None, string.punctuation) + \" )\")\n",
    "            if ow.strip() == \"\":\n",
    "                continue\n",
    "            if simMeasure == \"mi\":\n",
    "                N_w = countExpr_get_1(count_history, ow)\n",
    "            for rw_wv in rWords_ow:\n",
    "                (rw_w, rw_v) = rw_wv.items()[0]\n",
    "                if rw_w.strip() == \"\":\n",
    "                    continue           \n",
    "                if simMeasure == \"mi\":\n",
    "                    N_v = countExpr_get_1(count_history, rw_w)\n",
    "                    N_wv = countExpr_get_2(count_history, ow, rw_w)\n",
    "                    rw_v_ = mi_(N_w, N_v, N_wv)\n",
    "                elif simMeasure == \"cnet\":\n",
    "                    rw_v_ = rw_v\n",
    "                relWords[rw_w] = relWords.get(rw_w, 0) + rw_v_\n",
    "    \n",
    "    relWords_sorted = sorted(relWords.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    #print (\"origWords =\", origWords)\n",
    "    #print (\"relWords_sorted =\", relWords_sorted)\n",
    "    #counter = 0\n",
    "    relWords_sel = []\n",
    "    for counter, (rw_w, rw_v) in enumerate(relWords_sorted):\n",
    "        if (counter >= expansionCount):\n",
    "            break\n",
    "        if  all(c in string.printable for c in rw_w):\n",
    "            if rw_w not in origWords:\n",
    "                rw_w = regex.sub(' ', rw_w)\n",
    "                text_string += rw_w + \" \"\n",
    "                relWords_sel += [rw_w]\n",
    "        #counter += 1\n",
    "    #print(\"counter =\", counter)\n",
    "    #rint(\"relWords_sel =\", relWords_sel)\n",
    "#    return(relWords_sel)\n",
    "    return(count_history, text_string, relWords_sel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getAllRelWords_train(dist, count_history, rWords, origWordsAll, expansionCount1, N1, expansionCount2, intCoeff0, intCoeff1, intCoeff2, dirCoeff, featureWeights, indriRunQuery_hist_, dumpindex_hist_):\n",
    "    relText_stringAll = []\n",
    "    relText_string = dict()\n",
    "    relWords_sel = dict()\n",
    "    for c1, (docno, origWords) in enumerate(origWordsAll.iteritems()):\n",
    "        print(\"c1 =\", c1, \"docno =\", docno, \"--- origWords =\", origWords)\n",
    "        args = (count_history, rWords, dist, docno, origWords, expansionCount1, intCoeff0, intCoeff1, intCoeff2, dirCoeff, featureWeights)\n",
    "        indriRunQuery_hist_, dumpindex_hist_ = getRelWords_train(args, indriRunQuery_hist_, dumpindex_hist_)\n",
    "        #print(\"c1 =\", c1, \"- len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())) =\", len(indriRunQuery_hist_), len(dumpindex_hist_.get('efb', dict())), len(dumpindex_hist_.get('dcf', dict())))\n",
    "    \n",
    "    #print(\"relText_stringAll =\", relText_stringAll)\n",
    "    return (indriRunQuery_hist_, dumpindex_hist_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getAllRelWords(dist, count_history, rWords, origWordsAll, expansionCount1, N1, expansionCount2, intCoeff0, intCoeff1, intCoeff2, dirCoeff, featureWeights, indriRunQuery_hist_, dumpindex_hist_):\n",
    "    relText_stringAll = []\n",
    "    relText_string = dict()\n",
    "    relWords_sel = dict()\n",
    "    for c1, (docno, origWords) in enumerate(origWordsAll.iteritems()):\n",
    "        #print(str(c1), \"---\", docno, \"---\", origWords)\n",
    "        args = (count_history, rWords, dist, docno, origWords, expansionCount1, intCoeff0, intCoeff1, intCoeff2, dirCoeff, featureWeights)\n",
    "        count_history, relText_string_, relWords_sel_ = getRelWords(args, indriRunQuery_hist_, dumpindex_hist_)\n",
    "        #print(\"len(indriRunQuery_hist_), len(dumpindex_hist_['dcf']), len(dumpindex_hist_['ef']) =\", len(indriRunQuery_hist_), len(dumpindex_hist_['dcf']), len(dumpindex_hist_['ef']))\n",
    "        relText_string[docno] = relText_string_\n",
    "        relWords_sel[docno] = relWords_sel_[0:N1]\n",
    "        #print(docno, \"---\", origWords, \"---\", relWords_sel_)\n",
    "    relText_stringAll += [relText_string]\n",
    "    \"\"\"\n",
    "    relText_string = dict()\n",
    "    relWords_sel = dict()\n",
    "    for docno, origWords_new in relWords_sel.iteritems():\n",
    "        args = (count_history, rWords, docno, origWords, expansionCount1, intCoeff0, intCoeff1, intCoeff2, dirCoeff, featureWeights)\n",
    "        count_history, relText_string_, relWords_sel_ = getRelWords(args, indriRunQuery_hist_, dumpindex_hist_)\n",
    "        relText_string[docno] = relText_string_\n",
    "        relWords_sel[docno] = relWords_sel_\n",
    "    relText_stringAll += [relText_string]\n",
    "    \"\"\"\n",
    "    #print(\"relText_stringAll =\", relText_stringAll)\n",
    "    return (count_history, relText_stringAll, relWords_sel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "def genQueries(cfgOutFileName_, origWordsAll, relText_stringAll, intCoeff0, intCoeff1, intCoeff2, dirCoeff):\n",
    "    #    #print(\"intCoeff =\", intCoeff)\n",
    "    with open(cfgInFileName, 'r') as inFile:\n",
    "        reader = inFile.read()\n",
    "        soupNew = BeautifulSoup(\"\", 'lxml')\n",
    "        parameters_tag = soupNew.new_tag(\"parameters\")\n",
    "        index_tag = soupNew.new_tag(\"index\")\n",
    "        index_tag.string = colIndexDir\n",
    "        parameters_tag.append(index_tag)\n",
    "            #print(tags)\n",
    "\n",
    "        for docno, origWords in origWordsAll.iteritems():\n",
    "            doc_tag = soupNew.new_tag(\"query\")\n",
    "\n",
    "            docno_tag = soupNew.new_tag(\"number\")\n",
    "            docno_tag.string = docno\n",
    "            doc_tag.append(docno_tag)\n",
    "\n",
    "            text_tag = soupNew.new_tag(\"text\")\n",
    "            text_tag.string = \"#weight(\\n\" \n",
    "                    \n",
    "            text_tag.string += str(intCoeff0) + \" #combine(\" \n",
    "            for ow in set(origWords):\n",
    "                ow = regex.sub(' ', ow)\n",
    "                text_tag.string += ow + \" \"\n",
    "            text_tag.string += \")\\n\"\n",
    "\n",
    "            if len(relText_stringAll[0])>0:\n",
    "                if len(relText_stringAll[0][docno]) > 3: \n",
    "                    relText_string1 = regex.sub(' ', relText_stringAll[0][docno])\n",
    "                    text_tag.string += str(intCoeff1) + \" #combine(\" \n",
    "                    #print(\"relText_string1 =\", relText_string1)\n",
    "                    text_tag.string += relText_string1.encode('utf-8')\n",
    "                    text_tag.string += \")\\n\"\n",
    "            \n",
    "            \"\"\"\n",
    "            #print(len(relText_stringAll[1]))\n",
    "            if len(relText_stringAll[1])>0:\n",
    "                if len(relText_stringAll[1][docno]) > 3: \n",
    "                    relText_string2 = regex.sub(' ', relText_stringAll[1][docno])\n",
    "                    text_tag.string += str(intCoeff2) + \" #combine(\" \n",
    "                    #print(\"relText_string2 =\", relText_string2)\n",
    "                    text_tag.string += relText_string2.encode('utf-8')\n",
    "                    text_tag.string += \")\\n\"\n",
    "            \"\"\"\n",
    "            #print(\"origWords =\", origWords)\n",
    "            #print(\"relWords_sel1 =\", relWords_sel1)\n",
    "            #print(\"relWords_sel2 =\", relWords_sel2)\n",
    "            \n",
    "            \n",
    "            text_tag.string += \") \"\n",
    "\n",
    "            doc_tag.append(text_tag)\n",
    "\n",
    "            parameters_tag.append(doc_tag)\n",
    "            #print(doc_tag)\n",
    "\n",
    "        rule_tag = soupNew.new_tag(\"rule\")\n",
    "        rule_tag.string = \"method:dir,mu:\" + str(dirCoeff)\n",
    "        #rule_tag.string = \"method:two\"\n",
    "        parameters_tag.append(rule_tag)\n",
    "\n",
    "        #intCoeff_tag = soupNew.new_tag(\"intCoeff\")\n",
    "        #intCoeff_tag.string = \"0.8\"\n",
    "        #parameters_tag.append(intCoeff_tag)\n",
    "\n",
    "        threads_tag = soupNew.new_tag(\"threads\")\n",
    "        threads_tag.string = \"32\"\n",
    "        parameters_tag.append(threads_tag)\n",
    "\n",
    "        count_tag = soupNew.new_tag(\"count\")\n",
    "        count_tag.string = \"1000\"\n",
    "        parameters_tag.append(count_tag)\n",
    "\n",
    "        trecFormat_tag = soupNew.new_tag(\"trecFormat\")\n",
    "        trecFormat_tag.string = \"true\"\n",
    "        parameters_tag.append(trecFormat_tag)\n",
    "\n",
    "        soupNew.append(parameters_tag)\n",
    "        #print(soupNew.prettify())\n",
    "    #print(\"outFileName =\", outFileName)\n",
    "    with open( cfgOutFileName_, 'w') as outFile:\n",
    "        soupNewStr = str(soupNew)\n",
    "        soupNewStr = soupNewStr.replace(\"</text>\", \"\\n</text>\\n\").replace(\"query>\", \"query>\\n\").replace(\"<text>\", \"\\n<text>\\n\").replace(\"</index>\", \"</index>\\n\").replace(\"\\n<index>\", \"<index>\")\n",
    "\n",
    "        outFile.write(soupNewStr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def precisionCompute_train(dist, origWordsAll, count_history, intCoeffs0, intCoeffs1, intCoeffs2, expansionCounts1, expansionCounts2, dirCoeffs, N1s, featureWeights_, indriRunQuery_hist_, dumpindex_hist_):\n",
    "    #randNum = random.randint(1,1e9)\n",
    "    indriRunQuery_hist_, dumpindex_hist_ = getAllRelWords_train(dist, count_history, rWords, origWordsAll, expansionCounts1[0], N1s[0], expansionCounts2[0], intCoeffs0[0], intCoeffs1[0], intCoeffs2[0], dirCoeffs[0], featureWeights_[0], indriRunQuery_hist_, dumpindex_hist_)\n",
    "                                    \n",
    "    return count_history, indriRunQuery_hist_, dumpindex_hist_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c1 = 0 docno = 216 --- origWords = ['windandsea']\n",
      "c1 = 1 docno = 217 --- origWords = ['astronaut', 'biography']\n",
      "c1 = 2 docno = 214 --- origWords = ['local', 'area', 'unemployment', 'statistic', 'home']\n",
      "c1 = 3 docno = 215 --- origWords = ['nih', 'video', 'cast']\n",
      "c1 = 4 docno = 212 --- origWords = ['national', 'mediation', 'board']\n",
      "c1 = 5 docno = 213 --- origWords = ['school', 'bus', 'recall']\n",
      "c1 = 6 docno = 210 --- origWords = ['california', 'state', 'park', 'job']\n",
      "c1 = 7 docno = 211 --- origWords = ['national', 'conference', 'advance', 'cholesterol', 'management']\n",
      "c1 = 8 docno = 165 --- origWords = ['national', 'laboratory']\n",
      "c1 = 9 docno = 218 --- origWords = ['medwatch']\n",
      "c1 = 10 docno = 219 --- origWords = ['west', 'indian', 'manatee', 'information']\n",
      "c1 = 11 docno = 133 --- origWords = ['human', 'genome', 'research']\n",
      "c1 = 12 docno = 132 --- origWords = ['food', 'illness', 'report']\n",
      "c1 = 13 docno = 131 --- origWords = ['persian', 'gulf', 'war', 'veteran', 'lou', 'gehrig', 'disease']\n",
      "c1 = 14 docno = 130 --- origWords = ['bay', 'area', 'terrorism', 'hazard']\n",
      "c1 = 15 docno = 137 --- origWords = ['map', 'map', 'cartography']\n",
      "c1 = 16 docno = 136 --- origWords = ['career', 'information']\n",
      "c1 = 17 docno = 135 --- origWords = ['laura', 'welch', 'bush']\n",
      "c1 = 18 docno = 134 --- origWords = ['national', 'park']\n",
      "c1 = 19 docno = 139 --- origWords = ['poetry', 'read', 'poem', 'online']\n",
      "c1 = 20 docno = 138 --- origWords = ['risk', 'tattoo', 'permanent', 'makeup']\n",
      "c1 = 21 docno = 166 --- origWords = ['art', 'white', 'house']\n",
      "c1 = 22 docno = 24 --- origWords = ['child', 'care']\n",
      "c1 = 23 docno = 25 --- origWords = ['history', 'phoenix', 'symbol']\n",
      "c1 = 24 docno = 26 --- origWords = ['white', 'house', 'president', 'bush', 'cabinet']\n",
      "c1 = 25 docno = 27 --- origWords = ['opm', 'new', 'retiree']\n",
      "c1 = 26 docno = 20 --- origWords = ['tuskegee', 'airman', 'observance']\n",
      "c1 = 27 docno = 21 --- origWords = ['substance', 'abuse']\n",
      "c1 = 28 docno = 22 --- origWords = ['national', 'atlas', 'map']\n",
      "c1 = 29 docno = 23 --- origWords = ['iraq', 'kuwait', 'threat', 'history']\n",
      "c1 = 30 docno = 160 --- origWords = ['scholarship', 'program']\n",
      "c1 = 31 docno = 28 --- origWords = ['fda', '2002', 'press', 'release']\n",
      "c1 = 32 docno = 29 --- origWords = ['grand', 'canyon', 'monitor', 'research', 'center']\n",
      "c1 = 33 docno = 161 --- origWords = ['tornado', 'outbreak', 'record']\n",
      "c1 = 34 docno = 4 --- origWords = ['jpl', 'stardust', 'comet', 'wild']\n",
      "c1 = 35 docno = 8 --- origWords = ['philippine']\n",
      "c1 = 36 docno = 163 --- origWords = ['bioterrorism']\n",
      "c1 = 37 docno = 119 --- origWords = ['groundhog', 'day', 'punxsutawney']\n",
      "c1 = 38 docno = 120 --- origWords = ['st', 'paul', 'sidewalk', 'snow', 'shovel', 'ordnance']\n",
      "c1 = 39 docno = 121 --- origWords = ['identity', 'theft']\n",
      "c1 = 40 docno = 122 --- origWords = ['white', 'house', 'fellowship']\n",
      "c1 = 41 docno = 123 --- origWords = ['cave', 'explore', 'lesson', 'plan']\n",
      "c1 = 42 docno = 124 --- origWords = ['detail', 'information', 'humpback', 'whale']\n",
      "c1 = 43 docno = 125 --- origWords = ['english', 'second', 'language']\n",
      "c1 = 44 docno = 126 --- origWords = ['trademark', 'faq']\n",
      "c1 = 45 docno = 127 --- origWords = ['gps']\n",
      "c1 = 46 docno = 128 --- origWords = ['planetary', 'balloon', 'program']\n",
      "c1 = 47 docno = 129 --- origWords = ['nodc', 'coastal', 'water', 'temp']\n",
      "c1 = 48 docno = 167 --- origWords = ['school', 'program']\n",
      "c1 = 49 docno = 118 --- origWords = ['california', 'board', 'correction']\n",
      "c1 = 50 docno = 59 --- origWords = ['health', 'resource', 'service', 'administration']\n",
      "c1 = 51 docno = 58 --- origWords = ['automobile', 'emission', 'vehicle', 'pollution']\n",
      "c1 = 52 docno = 55 --- origWords = ['budget', 'process', 'us', 'department', 'education']\n",
      "c1 = 53 docno = 54 --- origWords = ['food', 'drug', 'administration']\n",
      "c1 = 54 docno = 57 --- origWords = ['religious', 'belief']\n",
      "c1 = 55 docno = 56 --- origWords = ['kudzu', 'invasive', 'specie']\n",
      "c1 = 56 docno = 51 --- origWords = ['consumer', 'food', 'advice']\n",
      "c1 = 57 docno = 50 --- origWords = ['money', 'launder']\n",
      "c1 = 58 docno = 53 --- origWords = ['telecommuting']\n",
      "c1 = 59 docno = 52 --- origWords = ['monterey', 'bay', 'national', 'marine', 'sanctuary']\n",
      "c1 = 60 docno = 164 --- origWords = ['winter', 'weather', 'safety']\n",
      "c1 = 61 docno = 201 --- origWords = ['social', 'security', 'inspector', 'general']\n",
      "c1 = 62 docno = 199 --- origWords = ['adhd', 'clinical', 'trial']\n",
      "c1 = 63 docno = 179 --- origWords = ['gpo', 'subscription', 'catalog']\n",
      "c1 = 64 docno = 200 --- origWords = ['maritime', 'trade']\n",
      "c1 = 65 docno = 195 --- origWords = ['firstgov', 'worker']\n",
      "c1 = 66 docno = 194 --- origWords = ['cigarette', 'nicotine', 'health']\n",
      "c1 = 67 docno = 197 --- origWords = ['vietnam', 'war']\n",
      "c1 = 68 docno = 178 --- origWords = ['recycle']\n",
      "c1 = 69 docno = 191 --- origWords = ['coral', 'reef', 'system']\n",
      "c1 = 70 docno = 190 --- origWords = ['child', 'butterfly', 'site', 'life', 'cycle', 'page']\n",
      "c1 = 71 docno = 193 --- origWords = ['dinosaur', 'fact', 'fiction']\n",
      "c1 = 72 docno = 192 --- origWords = ['drunk', 'drive']\n",
      "c1 = 73 docno = 115 --- origWords = ['information', 'security']\n",
      "c1 = 74 docno = 114 --- origWords = ['laguna', 'canyon', 'landslide']\n",
      "c1 = 75 docno = 88 --- origWords = ['frank', 'lloyd', 'wright']\n",
      "c1 = 76 docno = 89 --- origWords = ['cdc', 'rabies', 'homepage']\n",
      "c1 = 77 docno = 111 --- origWords = ['specie', 'identification']\n",
      "c1 = 78 docno = 110 --- origWords = ['fema', 'victim', 'disaster', 'information']\n",
      "c1 = 79 docno = 113 --- origWords = ['fermilab', 'flora', 'fauna']\n",
      "c1 = 80 docno = 112 --- origWords = ['oak', 'ridge', 'tennesse', 'doe']\n",
      "c1 = 81 docno = 82 --- origWords = ['1000', 'popular', 'give', 'name', 'birth', '2000']\n",
      "c1 = 82 docno = 83 --- origWords = ['nsa', 'home', 'kid']\n",
      "c1 = 83 docno = 80 --- origWords = ['department', 'labor', 'wage', 'hour', 'division']\n",
      "c1 = 84 docno = 81 --- origWords = ['mosquito', 'control']\n",
      "c1 = 85 docno = 86 --- origWords = ['california', 'state', 'auditor']\n",
      "c1 = 86 docno = 87 --- origWords = ['chronic', 'pain']\n",
      "c1 = 87 docno = 84 --- origWords = ['national', 'institute', 'alcohol', 'abuse', 'alcoholism', 'homepage']\n",
      "c1 = 88 docno = 85 --- origWords = ['international', 'parental', 'child', 'abduction']\n",
      "c1 = 89 docno = 198 --- origWords = ['acupuncture', 'effectiveness']\n",
      "c1 = 90 docno = 206 --- origWords = ['bird', 'monitor', 'north', 'america']\n",
      "c1 = 91 docno = 3 --- origWords = ['citizen', 'attitude', 'prairie', 'dog']\n",
      "c1 = 92 docno = 177 --- origWords = ['minority', 'business']\n",
      "c1 = 93 docno = 7 --- origWords = ['togo', 'embassy']\n",
      "c1 = 94 docno = 225 --- origWords = ['japanese', 'surrender', 'document']\n",
      "c1 = 95 docno = 108 --- origWords = ['panic', 'disorder', 'nimh']\n",
      "c1 = 96 docno = 109 --- origWords = ['mojave', 'desert', 'ecology']\n",
      "c1 = 97 docno = 102 --- origWords = ['diet', 'nutrition', 'weight', 'management']\n",
      "c1 = 98 docno = 103 --- origWords = ['vanity', 'license', 'plate']\n",
      "c1 = 99 docno = 100 --- origWords = ['stable', 'isotope', 'bird', 'migration']\n",
      "c1 = 100 docno = 101 --- origWords = ['migrant', 'farm', 'worker']\n",
      "c1 = 101 docno = 106 --- origWords = ['national', 'gamble', 'impact', 'study', 'commission']\n",
      "c1 = 102 docno = 107 --- origWords = ['small', 'business']\n",
      "c1 = 103 docno = 104 --- origWords = ['space', 'exploration']\n",
      "c1 = 104 docno = 105 --- origWords = ['presidential', 'rank', 'merit', 'award', 'federal', 'employee']\n",
      "c1 = 105 docno = 39 --- origWords = ['child', 'support', 'enforcement', 'cost', 'benefit']\n",
      "c1 = 106 docno = 38 --- origWords = ['ohio', 'dam', 'lock']\n",
      "c1 = 107 docno = 33 --- origWords = ['teen', 'pregnancy']\n",
      "c1 = 108 docno = 32 --- origWords = ['history', 'american', 'agriculture']\n",
      "c1 = 109 docno = 31 --- origWords = ['origin', 'universe']\n",
      "c1 = 110 docno = 30 --- origWords = ['hiv', 'aid']\n",
      "c1 = 111 docno = 37 --- origWords = ['magnetism']\n",
      "c1 = 112 docno = 36 --- origWords = ['art', 'education']\n",
      "c1 = 113 docno = 35 --- origWords = ['religious', 'freedom', 'amendment']\n",
      "c1 = 114 docno = 34 --- origWords = ['ntp', 'herbal', 'medicine', 'factsheet']\n",
      "c1 = 115 docno = 205 --- origWords = ['state', 'department', 'bureau', 'intelligence', 'research']\n",
      "c1 = 116 docno = 223 --- origWords = ['wright', 'brother', 'history']\n",
      "c1 = 117 docno = 176 --- origWords = ['hurricane']\n",
      "c1 = 118 docno = 60 --- origWords = ['plant', 'hardiness', 'zone']\n",
      "c1 = 119 docno = 61 --- origWords = ['cosmology', 'peebles', 'turner']\n",
      "c1 = 120 docno = 62 --- origWords = ['firstgov', 'kid', 'computer']\n",
      "c1 = 121 docno = 63 --- origWords = ['art', 'museum']\n",
      "c1 = 122 docno = 64 --- origWords = ['cell', 'phone', 'radiation']\n",
      "c1 = 123 docno = 65 --- origWords = ['california', 'art', 'council']\n",
      "c1 = 124 docno = 66 --- origWords = ['intelligence', 'analysis']\n",
      "c1 = 125 docno = 67 --- origWords = ['state', 'local', 'gateway']\n",
      "c1 = 126 docno = 68 --- origWords = ['woman', 'health', 'binge', 'eat', 'disorder']\n",
      "c1 = 127 docno = 69 --- origWords = ['mono', 'basin', 'national', 'forest', 'lake']\n",
      "c1 = 128 docno = 175 --- origWords = ['social', 'security', 'benefit', 'application']\n",
      "c1 = 129 docno = 174 --- origWords = ['keep', 'cholesterol', 'control']\n",
      "c1 = 130 docno = 173 --- origWords = ['internet', 'business', 'card']\n",
      "c1 = 131 docno = 172 --- origWords = ['fcc', 'freedom', 'information', 'act']\n",
      "c1 = 132 docno = 171 --- origWords = ['agricultural', 'research', 'service']\n",
      "c1 = 133 docno = 170 --- origWords = ['science', 'fair', 'competition']\n",
      "c1 = 134 docno = 203 --- origWords = ['art', 'embassy', 'exhibition']\n",
      "c1 = 135 docno = 222 --- origWords = ['home', 'equity', 'credit', 'line']\n",
      "c1 = 136 docno = 181 --- origWords = ['ahrq', 'clinical', 'practice', 'guideline']\n",
      "c1 = 137 docno = 182 --- origWords = ['federal', 'consumer', 'information', 'center', 'scam']\n",
      "c1 = 138 docno = 183 --- origWords = ['near', 'earth', 'asteroid']\n",
      "c1 = 139 docno = 180 --- origWords = ['greenwich', 'mean', 'time']\n",
      "c1 = 140 docno = 2 --- origWords = ['ireland', 'consular', 'information', 'sheet']\n",
      "c1 = 141 docno = 162 --- origWords = ['fda', 'clear', 'medical', 'device']\n",
      "c1 = 142 docno = 187 --- origWords = ['backyard', 'bird', 'habitat']\n",
      "c1 = 143 docno = 184 --- origWords = ['child', 'leave']\n",
      "c1 = 144 docno = 6 --- origWords = ['philadelphia', 'street']\n",
      "c1 = 145 docno = 220 --- origWords = ['wildfire', 'usgs', 'wildland', 'fire', 'research']\n",
      "c1 = 146 docno = 186 --- origWords = ['piglet', 'anti', 'inflammatory']\n",
      "c1 = 147 docno = 188 --- origWords = ['study', 'comet']\n",
      "c1 = 148 docno = 189 --- origWords = ['homeopathic', 'medicine']\n",
      "c1 = 149 docno = 202 --- origWords = ['farm', 'service', 'agency', 'online']\n",
      "c1 = 150 docno = 196 --- origWords = ['new', 'york', 'city', 'employee', 'pension']\n",
      "c1 = 151 docno = 221 --- origWords = ['homeland', 'security']\n",
      "c1 = 152 docno = 185 --- origWords = ['estuary', 'conservation']\n",
      "c1 = 153 docno = 99 --- origWords = ['salmon']\n",
      "c1 = 154 docno = 98 --- origWords = ['mine', 'safety', 'health', 'administration']\n",
      "c1 = 155 docno = 168 --- origWords = ['maryland', 'rural', 'development']\n",
      "c1 = 156 docno = 169 --- origWords = ['save', 'energy']\n",
      "c1 = 157 docno = 91 --- origWords = ['opm', 'dual', 'employment']\n",
      "c1 = 158 docno = 90 --- origWords = ['healthy', 'home', 'lead', 'hazard', 'control']\n",
      "c1 = 159 docno = 93 --- origWords = ['head', 'start', 'home']\n",
      "c1 = 160 docno = 92 --- origWords = ['doe', 'illness', 'compensation']\n",
      "c1 = 161 docno = 95 --- origWords = ['flu', 'unite', 'state']\n",
      "c1 = 162 docno = 94 --- origWords = ['florida', 'key', 'marine', 'sanctuary']\n",
      "c1 = 163 docno = 97 --- origWords = ['ufo']\n",
      "c1 = 164 docno = 96 --- origWords = ['planet', 'mar']\n",
      "c1 = 165 docno = 11 --- origWords = ['pileat', 'woodpecker']\n",
      "c1 = 166 docno = 10 --- origWords = ['well', 'water', 'contamination']\n",
      "c1 = 167 docno = 13 --- origWords = ['eruption', 'mount', 'st', 'helen']\n",
      "c1 = 168 docno = 12 --- origWords = ['oil', 'petroleum', 'resource']\n",
      "c1 = 169 docno = 15 --- origWords = ['welfare', 'reform']\n",
      "c1 = 170 docno = 14 --- origWords = ['club', 'drug']\n",
      "c1 = 171 docno = 17 --- origWords = ['secure', 'linux']\n",
      "c1 = 172 docno = 16 --- origWords = ['sandhil', 'crane', 'platte', 'river']\n",
      "c1 = 173 docno = 19 --- origWords = ['toxic', 'waste']\n",
      "c1 = 174 docno = 18 --- origWords = ['copyright', 'basic']\n",
      "c1 = 175 docno = 117 --- origWords = ['san', 'diego', 'water', 'supply']\n",
      "c1 = 176 docno = 116 --- origWords = ['fafsa']\n",
      "c1 = 177 docno = 204 --- origWords = ['endanger', 'specie']\n",
      "c1 = 178 docno = 151 --- origWords = ['institution', 'university', 'fuel', 'cell', 'research']\n",
      "c1 = 179 docno = 150 --- origWords = ['snake', 'bite', 'aid']\n",
      "c1 = 180 docno = 153 --- origWords = ['technology', 'transfer']\n",
      "c1 = 181 docno = 152 --- origWords = ['bioengineer', 'food', 'crop']\n",
      "c1 = 182 docno = 155 --- origWords = ['federal', 'government', 'outsource']\n",
      "c1 = 183 docno = 154 --- origWords = ['kid', 'state']\n",
      "c1 = 184 docno = 157 --- origWords = ['federal', 'grant', 'program']\n",
      "c1 = 185 docno = 156 --- origWords = ['cpsc', 'recall']\n",
      "c1 = 186 docno = 159 --- origWords = ['tax', 'cut', 'deja', 'voo', 'doo']\n",
      "c1 = 187 docno = 158 --- origWords = ['ethic', 'congress', 'representative']\n",
      "c1 = 188 docno = 207 --- origWords = ['annual', 'parade']\n",
      "c1 = 189 docno = 224 --- origWords = ['york', 'county']\n",
      "c1 = 190 docno = 48 --- origWords = ['federal', 'state', 'statistic']\n",
      "c1 = 191 docno = 49 --- origWords = ['fcc', 'consumer', 'bureau']\n",
      "c1 = 192 docno = 46 --- origWords = ['local', 'drink', 'water']\n",
      "c1 = 193 docno = 47 --- origWords = ['medical', 'residency']\n",
      "c1 = 194 docno = 44 --- origWords = ['dash', 'combination', 'diet']\n",
      "c1 = 195 docno = 45 --- origWords = ['faith', 'base', 'initiative']\n",
      "c1 = 196 docno = 42 --- origWords = ['sibir', 'air', 'anatomy', 'disaster']\n",
      "c1 = 197 docno = 43 --- origWords = ['vehicle', 'registration', 'california']\n",
      "c1 = 198 docno = 40 --- origWords = ['public', 'school', 'standard', 'performance']\n",
      "c1 = 199 docno = 41 --- origWords = ['historic', 'preservation']\n",
      "c1 = 200 docno = 1 --- origWords = ['electoral', 'college']\n",
      "c1 = 201 docno = 5 --- origWords = ['american', 'music']\n",
      "c1 = 202 docno = 9 --- origWords = ['baltimore']\n",
      "c1 = 203 docno = 146 --- origWords = ['astrobiology']\n",
      "c1 = 204 docno = 147 --- origWords = ['hear', 'balance', 'loud']\n",
      "c1 = 205 docno = 144 --- origWords = ['california', 'tahoe', 'conservancy']\n",
      "c1 = 206 docno = 145 --- origWords = ['national', 'fall', 'firefighter', 'memorial']\n",
      "c1 = 207 docno = 142 --- origWords = ['hev', 'program', 'homepage']\n",
      "c1 = 208 docno = 143 --- origWords = ['fema', 'kid', 'hurricane', 'fact']\n",
      "c1 = 209 docno = 140 --- origWords = ['natural', 'resource', 'conservation', 'service']\n",
      "c1 = 210 docno = 141 --- origWords = ['international', 'monetary', 'fund']\n",
      "c1 = 211 docno = 209 --- origWords = ['radon']\n",
      "c1 = 212 docno = 208 --- origWords = ['bill', 'right', 'amendment']\n",
      "c1 = 213 docno = 148 --- origWords = ['low', 'income', 'house']\n",
      "c1 = 214 docno = 149 --- origWords = ['national', 'science', 'bowl']\n",
      "c1 = 215 docno = 77 --- origWords = ['hcfa', 'manual']\n",
      "c1 = 216 docno = 76 --- origWords = ['streamline', 'fha', 'mortgage']\n",
      "c1 = 217 docno = 75 --- origWords = ['noaa', 'fishery', 'northwest', 'region']\n",
      "c1 = 218 docno = 74 --- origWords = ['skin', 'cancer']\n",
      "c1 = 219 docno = 73 --- origWords = ['solar', 'flare']\n",
      "c1 = 220 docno = 72 --- origWords = ['edy', 'ice', 'cream', 'recall']\n",
      "c1 = 221 docno = 71 --- origWords = ['duck', 'identification', 'guide']\n",
      "c1 = 222 docno = 70 --- origWords = ['social', 'security', 'office', 'locator']\n",
      "c1 = 223 docno = 79 --- origWords = ['international', 'trade']\n",
      "c1 = 224 docno = 78 --- origWords = ['us', 'antarctic', 'program', 'science', 'section']\n"
     ]
    }
   ],
   "source": [
    "#training\n",
    "dist = str(1)\n",
    "intCoeffs0 = [ 0.7 ]\n",
    "intCoeffs1 = [ 0.3 ]\n",
    "#intCoeffs2 = [ 0.18 ]\n",
    "intCoeffs2 = [ 0.5 ]\n",
    "#intCoeffs1 = np.arange(0.1, 1, 0.1)\n",
    "expansionCounts1 = [ 85 ]\n",
    "#expansionCounts2 = [ 145 ]\n",
    "expansionCounts2 = [ 0 ]\n",
    "#expansionCounts2 = range(15, 220, 5)\n",
    "dirCoeffs = [ 1600 ]\n",
    "#dirCoeffs = range(200, 4000, 200)\n",
    "N1s = [0]\n",
    "rWords = dict()\n",
    "featureWeights_1 = {'maxTopColCor': 0.1, 'expTDocScore': 0.1, 'maxColPCor': 0.1, 'avgCDocScore': 0.1, 'topTermFrac': 0.1, 'avgColCor': 0.1, 'avgTopColCor': 0.1, 'numCanDocs': 0.1, 'maxTopColPCor': 0.1, 'maxCDocScore': 0.1, 'avgColPCor': 0.1, 'conIdf': 0.1, 'avgTopColPCor': 0.1, 'maxColCor': 0.1}\n",
    "featureWeights_l=[featureWeights_1]\n",
    "origWordsAll = analyseQueries()\n",
    "\n",
    "count_history, indriRunQuery_hist_, dumpindex_hist_ = precisionCompute_train(dist, origWordsAll, count_history, intCoeffs0, intCoeffs1, intCoeffs2, expansionCounts1, expansionCounts2, dirCoeffs, N1s, featureWeights_l, indriRunQuery_hist_, dumpindex_hist_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def precisionCompute(dist, origWordsAll, count_history, intCoeffs0, intCoeffs1, intCoeffs2, expansionCounts1, expansionCounts2, dirCoeffs, N1s, featureWeights_, indriRunQuery_hist_, dumpindex_hist_):\n",
    "    mapPrecs = []\n",
    "    for featureWeights in featureWeights_:\n",
    "        for N1 in N1s:\n",
    "            for intCoeff0 in intCoeffs0:\n",
    "                for intCoeff1 in intCoeffs1:\n",
    "                    for intCoeff2 in intCoeffs2:\n",
    "                        for expansionCount1 in expansionCounts1:\n",
    "                            for expansionCount2 in expansionCounts2:\n",
    "                                for dirCoeff in dirCoeffs:\n",
    "                                    \n",
    "                                    \n",
    "                                    #res = getAllRelWords_p(count_history, rWords, origWordsAll, expansionCount1, N1, expansionCount2, intCoeff0, intCoeff1, intCoeff2, dirCoeff, featureWeights)\n",
    "                                    \n",
    "                                    count_history, relText_stringAll, relWords_sel = getAllRelWords(dist, count_history, rWords, origWordsAll, expansionCount1, N1, expansionCount2, intCoeff0, intCoeff1, intCoeff2, dirCoeff, featureWeights, indriRunQuery_hist_, dumpindex_hist_)\n",
    "                                    \n",
    "                                    #print(\"len(relText_stringAll[1]) =\", len(relText_stringAll[1]))\n",
    "                                    ##########\n",
    "                                    #cfgOutFileName = os.path.join(\"tmp\",\"sbsb\",str(randNum)+\".cfg\")\n",
    "                                    #runsFileName = os.path.join(\"tmp\",\"sbsb\",str(randNum)+\".run\")\n",
    "                                    #evalsFileName = os.path.join(\"tmp\",\"sbsb\",str(randNum)+\".eval\")\n",
    "                                    ##########\n",
    "                                    genQueries(cfgOutFileName, origWordsAll, relText_stringAll, intCoeff0, intCoeff1, intCoeff2, dirCoeff)\n",
    "                                    subprocess.Popen(\"IndriRunQuery \" + cfgOutFileName + \" > \" + runsFileName, shell=True, stdout=subprocess.PIPE).stdout.read()\n",
    "                                    cmd = \"trec_eval -q \" + colQrelsFileName + \" \" + runsFileName + \" > \" + evalsFileName\n",
    "                                    subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE).stdout.read()\n",
    "                                    cmd = \"cat \" + evalsFileName + \" | grep map | grep all | grep -v gm | awk '{print $3}' \"\n",
    "                                    #print (\"cmd = \", cmd)\n",
    "                                    mapPrec = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE).stdout.read()\n",
    "                                    mapPrecs += [mapPrec]\n",
    "                                    print(\"featureWeights =\", featureWeights)\n",
    "                                    print(\"intCoeff0, intCoeff1, intCoeff2, expansionCount1, expansionCount2, dirCoeff, map precision, N1 =\", intCoeff0, intCoeff1, intCoeff2, expansionCount1, expansionCount2, dirCoeff, N1, mapPrec)\n",
    "                                    #mapPrecs[intCoeff] = mapPrec\n",
    "    return relText_stringAll, relWords_sel, mapPrecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "featureWeights = {'maxTopColCor': 500, 'maxColPCor': 500, 'conIdf': -1.0, 'expTDocScore': 500, 'topTermFrac': 0, 'avgColCor': -0.05, 'avgTopColCor': 0.1, 'numCanDocs': 0.1, 'maxTopColPCor': 0.1, 'avgCDocScore': 500, 'avgColPCor': 0.1, 'maxCDocScore': -500, 'avgTopColPCor': 0.1, 'maxColCor': 0.1}\n",
      "intCoeff0, intCoeff1, intCoeff2, expansionCount1, expansionCount2, dirCoeff, map precision, N1 = 0.7 0.5 0.5 10 0 1600 100 0.1921\n",
      "\n",
      "featureWeights = {'maxTopColCor': 500, 'maxColPCor': 500, 'conIdf': -0.80000000000000004, 'expTDocScore': 500, 'topTermFrac': 0, 'avgColCor': -0.05, 'avgTopColCor': 0.1, 'numCanDocs': 0.1, 'maxTopColPCor': 0.1, 'avgCDocScore': 500, 'avgColPCor': 0.1, 'maxCDocScore': -500, 'avgTopColPCor': 0.1, 'maxColCor': 0.1}\n",
      "intCoeff0, intCoeff1, intCoeff2, expansionCount1, expansionCount2, dirCoeff, map precision, N1 = 0.7 0.5 0.5 10 0 1600 100 0.1921\n",
      "\n",
      "featureWeights = {'maxTopColCor': 500, 'maxColPCor': 500, 'conIdf': -0.60000000000000009, 'expTDocScore': 500, 'topTermFrac': 0, 'avgColCor': -0.05, 'avgTopColCor': 0.1, 'numCanDocs': 0.1, 'maxTopColPCor': 0.1, 'avgCDocScore': 500, 'avgColPCor': 0.1, 'maxCDocScore': -500, 'avgTopColPCor': 0.1, 'maxColCor': 0.1}\n",
      "intCoeff0, intCoeff1, intCoeff2, expansionCount1, expansionCount2, dirCoeff, map precision, N1 = 0.7 0.5 0.5 10 0 1600 100 0.1921\n",
      "\n",
      "featureWeights = {'maxTopColCor': 500, 'maxColPCor': 500, 'conIdf': -0.40000000000000013, 'expTDocScore': 500, 'topTermFrac': 0, 'avgColCor': -0.05, 'avgTopColCor': 0.1, 'numCanDocs': 0.1, 'maxTopColPCor': 0.1, 'avgCDocScore': 500, 'avgColPCor': 0.1, 'maxCDocScore': -500, 'avgTopColPCor': 0.1, 'maxColCor': 0.1}\n",
      "intCoeff0, intCoeff1, intCoeff2, expansionCount1, expansionCount2, dirCoeff, map precision, N1 = 0.7 0.5 0.5 10 0 1600 100 0.1921\n",
      "\n",
      "featureWeights = {'maxTopColCor': 500, 'maxColPCor': 500, 'conIdf': -0.20000000000000018, 'expTDocScore': 500, 'topTermFrac': 0, 'avgColCor': -0.05, 'avgTopColCor': 0.1, 'numCanDocs': 0.1, 'maxTopColPCor': 0.1, 'avgCDocScore': 500, 'avgColPCor': 0.1, 'maxCDocScore': -500, 'avgTopColPCor': 0.1, 'maxColCor': 0.1}\n",
      "intCoeff0, intCoeff1, intCoeff2, expansionCount1, expansionCount2, dirCoeff, map precision, N1 = 0.7 0.5 0.5 10 0 1600 100 0.1921\n",
      "\n",
      "featureWeights = {'maxTopColCor': 500, 'maxColPCor': 500, 'conIdf': -2.2204460492503131e-16, 'expTDocScore': 500, 'topTermFrac': 0, 'avgColCor': -0.05, 'avgTopColCor': 0.1, 'numCanDocs': 0.1, 'maxTopColPCor': 0.1, 'avgCDocScore': 500, 'avgColPCor': 0.1, 'maxCDocScore': -500, 'avgTopColPCor': 0.1, 'maxColCor': 0.1}\n",
      "intCoeff0, intCoeff1, intCoeff2, expansionCount1, expansionCount2, dirCoeff, map precision, N1 = 0.7 0.5 0.5 10 0 1600 100 0.1921\n",
      "\n",
      "featureWeights = {'maxTopColCor': 500, 'maxColPCor': 500, 'conIdf': 0.19999999999999973, 'expTDocScore': 500, 'topTermFrac': 0, 'avgColCor': -0.05, 'avgTopColCor': 0.1, 'numCanDocs': 0.1, 'maxTopColPCor': 0.1, 'avgCDocScore': 500, 'avgColPCor': 0.1, 'maxCDocScore': -500, 'avgTopColPCor': 0.1, 'maxColCor': 0.1}\n",
      "intCoeff0, intCoeff1, intCoeff2, expansionCount1, expansionCount2, dirCoeff, map precision, N1 = 0.7 0.5 0.5 10 0 1600 100 0.1921\n",
      "\n",
      "featureWeights = {'maxTopColCor': 500, 'maxColPCor': 500, 'conIdf': 0.39999999999999969, 'expTDocScore': 500, 'topTermFrac': 0, 'avgColCor': -0.05, 'avgTopColCor': 0.1, 'numCanDocs': 0.1, 'maxTopColPCor': 0.1, 'avgCDocScore': 500, 'avgColPCor': 0.1, 'maxCDocScore': -500, 'avgTopColPCor': 0.1, 'maxColCor': 0.1}\n",
      "intCoeff0, intCoeff1, intCoeff2, expansionCount1, expansionCount2, dirCoeff, map precision, N1 = 0.7 0.5 0.5 10 0 1600 100 0.1921\n",
      "\n",
      "featureWeights = {'maxTopColCor': 500, 'maxColPCor': 500, 'conIdf': 0.59999999999999964, 'expTDocScore': 500, 'topTermFrac': 0, 'avgColCor': -0.05, 'avgTopColCor': 0.1, 'numCanDocs': 0.1, 'maxTopColPCor': 0.1, 'avgCDocScore': 500, 'avgColPCor': 0.1, 'maxCDocScore': -500, 'avgTopColPCor': 0.1, 'maxColCor': 0.1}\n",
      "intCoeff0, intCoeff1, intCoeff2, expansionCount1, expansionCount2, dirCoeff, map precision, N1 = 0.7 0.5 0.5 10 0 1600 100 0.1921\n",
      "\n",
      "featureWeights = {'maxTopColCor': 500, 'maxColPCor': 500, 'conIdf': 0.7999999999999996, 'expTDocScore': 500, 'topTermFrac': 0, 'avgColCor': -0.05, 'avgTopColCor': 0.1, 'numCanDocs': 0.1, 'maxTopColPCor': 0.1, 'avgCDocScore': 500, 'avgColPCor': 0.1, 'maxCDocScore': -500, 'avgTopColPCor': 0.1, 'maxColCor': 0.1}\n",
      "intCoeff0, intCoeff1, intCoeff2, expansionCount1, expansionCount2, dirCoeff, map precision, N1 = 0.7 0.5 0.5 10 0 1600 100 0.1921\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fj9124/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:101: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/home/fj9124/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:110: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "dist = str(1)\n",
    "intCoeffs0 = [ 0.7 ]\n",
    "intCoeffs1 = [ 0.5 ]\n",
    "#intCoeffs2 = [ 0.18 ]\n",
    "intCoeffs2 = [ 0.5 ]\n",
    "#intCoeffs1 = np.arange(0.1, 1, 0.1)\n",
    "expansionCounts1 = [ 10 ]\n",
    "#expansionCounts2 = [ 145 ]\n",
    "expansionCounts2 = [ 0 ]\n",
    "#expansionCounts1 = range(0, 220, 5)\n",
    "dirCoeffs = [ 1600 ]\n",
    "#dirCoeffs = range(200, 4000, 200)\n",
    "N1s_max = [100]\n",
    "#N1s = range(1, 30, 1)\n",
    "#mapPrecs = \n",
    "rWords = dict()\n",
    "featureWeights_l=[]\n",
    "featureWeights_1 = {'maxTopColCor': 500, 'expTDocScore': 500, 'maxColPCor': 500, 'avgCDocScore': 500, 'topTermFrac': 0, 'avgColCor': -0.05, 'avgTopColCor': 0.1, 'numCanDocs': 0.1, 'maxTopColPCor': 0.1, 'maxCDocScore': -500, 'avgColPCor': 0.1, 'conIdf': -0.05, 'avgTopColPCor': 0.1, 'maxColCor': 0.1}\n",
    "#for conIdf in [-0.05] :\n",
    "#for conIdf in [-5000, -500, -50, -5, -0.5, -0.05, 0, 0.05, 0.5, 5, 50, 500, 5000] :\n",
    "for conIdf in np.arange(-1,1,0.2):\n",
    "    featureWeights_1[\"conIdf\"] = conIdf\n",
    "    featureWeights_l += [dict(featureWeights_1)]\n",
    "relText_stringAll, relWords_sel1, mapPrecs = precisionCompute(dist, origWordsAll, count_history, intCoeffs0, intCoeffs1, intCoeffs2, expansionCounts1, expansionCounts2, dirCoeffs, N1s_max, featureWeights_l, indriRunQuery_hist_, dumpindex_hist_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c1 = 0 docno = 216 --- origWords = ['windandsea']\n",
      "c1 = 1 docno = 217 --- origWords = ['astronaut', 'biography']\n",
      "c1 ="
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "dist = str(2)\n",
    "intCoeffs0 = [ 0.7 ]\n",
    "intCoeffs1 = [ 0.5 ]\n",
    "#intCoeffs2 = [ 0.18 ]\n",
    "intCoeffs2 = [ 0.5 ]\n",
    "#intCoeffs1 = np.arange(0.1, 1, 0.1)\n",
    "expansionCounts1 = [ 10 ]\n",
    "#expansionCounts2 = [ 145 ]\n",
    "expansionCounts2 = [ 0 ]\n",
    "#expansionCounts2 = range(15, 220, 5)\n",
    "dirCoeffs = [ 1600 ]\n",
    "#dirCoeffs = range(200, 4000, 200)\n",
    "N1s = 2\n",
    "N2s_max = [100]\n",
    "rWords = dict()\n",
    "featureWeights_1 = {'maxTopColCor': 0.1, 'expTDocScore': 0.1, 'maxColPCor': 0.1, 'avgCDocScore': 0.1, 'topTermFrac': 0.1, 'avgColCor': 0.1, 'avgTopColCor': 0.1, 'numCanDocs': 0.1, 'maxTopColPCor': 0.1, 'maxCDocScore': 0.1, 'avgColPCor': 0.1, 'conIdf': 0.1, 'avgTopColPCor': 0.1, 'maxColCor': 0.1}\n",
    "featureWeights_l=[featureWeights_1]\n",
    "relWords_sel1 = {k:v[0:N1s] for k,v in relWords_sel.iteritems()}\n",
    "indriRunQuery_hist_, dumpindex_hist_ = precisionCompute_train(dist, relWords_sel1, count_history, intCoeffs0, intCoeffs1, intCoeffs2, expansionCounts1, expansionCounts2, dirCoeffs, N2s_max, featureWeights_l, indriRunQuery_hist_, dumpindex_hist_)\n",
    "\"\"\"\n",
    "#training\n",
    "dist = str(1)\n",
    "intCoeffs0 = [ 0.7 ]\n",
    "intCoeffs1 = [ 0.3 ]\n",
    "#intCoeffs2 = [ 0.18 ]\n",
    "intCoeffs2 = [ 0.5 ]\n",
    "#intCoeffs1 = np.arange(0.1, 1, 0.1)\n",
    "expansionCounts1 = [ 85 ]\n",
    "#expansionCounts2 = [ 145 ]\n",
    "expansionCounts2 = [ 0 ]\n",
    "#expansionCounts2 = range(15, 220, 5)\n",
    "dirCoeffs = [ 1600 ]\n",
    "#dirCoeffs = range(200, 4000, 200)\n",
    "N2s_max = [0]\n",
    "N1s = 2\n",
    "rWords = dict()\n",
    "featureWeights_1 = {'maxTopColCor': 0.1, 'expTDocScore': 0.1, 'maxColPCor': 0.1, 'avgCDocScore': 0.1, 'topTermFrac': 0.1, 'avgColCor': 0.1, 'avgTopColCor': 0.1, 'numCanDocs': 0.1, 'maxTopColPCor': 0.1, 'maxCDocScore': 0.1, 'avgColPCor': 0.1, 'conIdf': 0.1, 'avgTopColPCor': 0.1, 'maxColCor': 0.1}\n",
    "featureWeights_l=[featureWeights_1]\n",
    "relWords_sel1 = {k:v[0:N1s] for k,v in relWords_sel1.iteritems()}\n",
    "count_history, indriRunQuery_hist_, dumpindex_hist_ = precisionCompute_train(dist, origWordsAll, count_history, intCoeffs0, intCoeffs1, intCoeffs2, expansionCounts1, expansionCounts2, dirCoeffs, N2s_max, featureWeights_l, indriRunQuery_hist_, dumpindex_hist_)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
